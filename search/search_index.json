{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Tracelet","text":"<p> Intelligent experiment tracking for PyTorch and PyTorch Lightning Automagic hyperparameter detection and multi-backend logging </p> <p> </p>"},{"location":"#what-is-tracelet","title":"What is Tracelet?","text":"<p>Tracelet is a powerful Python library that automatically captures and logs your machine learning experiments without requiring code modifications. Simply add one line to start tracking, and Tracelet will:</p> <ul> <li>\ud83d\udd2e Automagic instrumentation - Zero-config hyperparameter detection and logging</li> <li>\ud83d\udd0d Automatically capture TensorBoard metrics, PyTorch Lightning logs, and system metrics</li> <li>\ud83d\udd04 Route to multiple backends simultaneously (MLflow, ClearML, W&amp;B, AIM)</li> <li>\ud83d\udcca Track everything - scalars, histograms, images, audio, text, and artifacts</li> <li>\u26a1 Zero code changes required for existing TensorBoard workflows</li> <li>\ud83c\udfaf Plugin architecture for extensible functionality</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":""},{"location":"#installation","title":"Installation","text":"pipuvconda <pre><code>pip install tracelet\n</code></pre> <pre><code>uv add tracelet\n</code></pre> <pre><code>conda install -c conda-forge tracelet\n</code></pre>"},{"location":"#demo","title":"Demo","text":"Your browser does not support the video tag. Download the demo video <p>See Tracelet in action! The video above shows how easy it is to get started with automatic experiment tracking.</p>"},{"location":"#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# 1. Start tracking (one line!)\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"  # or \"clearml\", \"wandb\", \"aim\"\n)\n\n# 2. Use TensorBoard as normal - metrics automatically captured\nwriter = SummaryWriter()\nfor epoch in range(100):\n    loss = train_one_epoch()  # Your existing training code\n    writer.add_scalar('Loss/train', loss, epoch)\n    # \u2728 Metrics automatically sent to MLflow!\n\n# 3. Stop tracking\ntracelet.stop_logging()\n</code></pre> <p>That's it!</p> <p>Your existing TensorBoard code now logs to MLflow, ClearML, W&amp;B, or AIM with zero changes!</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#multi-backend-support","title":"\ud83d\udd0c Multi-Backend Support","text":"<p>Choose from 4 popular experiment tracking backends:</p> <ul> <li>MLflow - Open source ML lifecycle management</li> <li>ClearML - Enterprise-grade MLOps platform</li> <li>Weights &amp; Biases - Collaborative ML platform</li> <li>AIM - Open source experiment tracking</li> </ul>"},{"location":"#automatic-instrumentation","title":"\ud83c\udfaf Automatic Instrumentation","text":"<p>Tracelet automatically captures:</p> <ul> <li>TensorBoard metrics - Scalars, histograms, images, audio, text</li> <li>PyTorch Lightning - Training/validation metrics, hyperparameters</li> <li>System metrics - CPU, memory, GPU usage</li> <li>Git information - Repository state, commit hash, branch</li> <li>Environment - Python version, package versions, hardware info</li> </ul>"},{"location":"#rich-data-types","title":"\ud83d\udcca Rich Data Types","text":"<p>Log and visualize various data types:</p> <ul> <li>Scalars - Loss curves, accuracy, learning rates</li> <li>Histograms - Weight distributions, gradients</li> <li>Images - Sample predictions, confusion matrices</li> <li>Audio - Speech samples, music generation</li> <li>Text - Training summaries, generated text</li> <li>Artifacts - Models, datasets, configuration files</li> </ul>"},{"location":"#performance-optimized","title":"\u26a1 Performance Optimized","text":"<ul> <li>Thread-safe orchestrator for concurrent logging</li> <li>Batched operations to minimize overhead</li> <li>Smart buffering for high-throughput scenarios</li> <li>Configurable routing for different metric types</li> </ul>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    A[Your PyTorch Code] --&gt; B[TensorBoard SummaryWriter]\n    A --&gt; C[PyTorch Lightning Trainer]\n    A --&gt; D[Direct Tracelet API]\n\n    B --&gt; E[Tracelet Orchestrator]\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F[Plugin System]\n\n    F --&gt; G[MLflow Backend]\n    F --&gt; H[ClearML Backend]\n    F --&gt; I[W&amp;B Backend]\n    F --&gt; J[AIM Backend]\n\n    G --&gt; K[MLflow Server]\n    H --&gt; L[ClearML Platform]\n    I --&gt; M[W&amp;B Platform]\n    J --&gt; N[AIM Repository]\n</code></pre>"},{"location":"#why-tracelet","title":"Why Tracelet?","text":""},{"location":"#before-tracelet","title":"Before Tracelet \ud83d\ude24","text":"<pre><code># Different APIs for each backend\nimport mlflow\nimport wandb\nfrom clearml import Task\n\n# Separate logging calls\nmlflow.log_metric(\"loss\", loss)\nwandb.log({\"loss\": loss})\nTask.current_task().logger.report_scalar(\"loss\", loss)\n\n# Manual setup for each backend\nmlflow.start_run()\nwandb.init(project=\"my-project\")\ntask = Task.init(project_name=\"my-project\")\n</code></pre>"},{"location":"#with-tracelet","title":"With Tracelet \ud83c\udf89","text":"<pre><code># One API, any backend\nimport tracelet\n\ntracelet.start_logging(backend=\"mlflow\")  # or any backend\nwriter.add_scalar(\"loss\", loss)  # Works everywhere!\n</code></pre>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li>:material-rocket-launch: Quick Start Guide</li> </ul> <p>Get up and running in under 5 minutes with your first experiment</p> <ul> <li>:material-cog: Installation Guide</li> </ul> <p>Detailed installation instructions for all backends and environments</p> <ul> <li>:material-api: API Reference</li> </ul> <p>Complete API documentation with examples and type hints</p> <ul> <li>:material-book-open: Examples</li> </ul> <p>Real-world examples and Jupyter notebooks to learn from</p>"},{"location":"#community-support","title":"Community &amp; Support","text":"<ul> <li>\ud83d\udcda Documentation - Comprehensive guides and API docs</li> <li>\ud83d\udc1b Issues - Bug reports and feature requests</li> <li>\ud83d\udcac Discussions - Questions and community support</li> <li>\ud83d\udce7 Email - Direct contact with maintainers</li> </ul>"},{"location":"#license","title":"License","text":"<p>Tracelet is released under the MIT License.</p> Ready to supercharge your ML experiments? Get Started View on GitHub"},{"location":"automagic/","title":"\ud83d\udd2e Automagic Instrumentation","text":"<p>Tracelet's most powerful feature is automagic instrumentation - automatic detection and logging of machine learning hyperparameters with zero configuration. Just enable automagic mode and Tracelet intelligently captures your experiment parameters using advanced heuristics.</p>"},{"location":"automagic/#overview","title":"Overview","text":"<p>Traditional experiment tracking requires manual logging of every hyperparameter:</p> <pre><code># Traditional approach - tedious and error-prone\nexperiment.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 64,\n    \"epochs\": 100,\n    \"dropout\": 0.3,\n    \"hidden_layers\": [256, 128, 64],\n    \"optimizer\": \"adam\",\n    # ... 20+ more parameters\n})\n</code></pre> <p>With automagic instrumentation, this becomes:</p> <pre><code># Automagic approach - zero configuration\nlearning_rate = 0.001\nbatch_size = 64\nepochs = 100\ndropout = 0.3\nhidden_layers = [256, 128, 64]\noptimizer = \"adam\"\n# All parameters automatically captured! \u2728\n</code></pre>"},{"location":"automagic/#quick-start","title":"Quick Start","text":""},{"location":"automagic/#basic-automagic-usage","title":"Basic Automagic Usage","text":"<pre><code>from tracelet import Experiment\n\n# Enable automagic mode\nexperiment = Experiment(\n    name=\"automagic_experiment\",\n    backend=[\"mlflow\"],\n    automagic=True  # \u2728 Enable automagic instrumentation\n)\nexperiment.start()\n\n# Define hyperparameters normally - they're captured automatically\nlearning_rate = 3e-4\nbatch_size = 128\nepochs = 50\ndropout_rate = 0.1\nnum_layers = 6\n\n# Your training code here...\n# Automagic captures all relevant variables!\n\nexperiment.end()\n</code></pre>"},{"location":"automagic/#framework-integration","title":"Framework Integration","text":"<p>Automagic automatically hooks into popular ML frameworks:</p> <pre><code>import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom tracelet import Experiment\n\n# Enable automagic\nexperiment = Experiment(\"pytorch_training\", automagic=True)\nexperiment.start()\n\n# Model hyperparameters (automatically captured)\nlearning_rate = 0.001\nweight_decay = 1e-4\nmomentum = 0.9\nbatch_size = 32\nepochs = 10\ninput_size = 784\nhidden_size = 128\nnum_classes = 10\n\n# Create a simple model (automatically instrumented)\nmodel = nn.Sequential(\n    nn.Linear(input_size, hidden_size),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(hidden_size, num_classes)\n)\n\noptimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\ncriterion = nn.CrossEntropyLoss()\n\n# Create dummy training data for demonstration\ntrain_data = torch.randn(1000, input_size)\ntrain_targets = torch.randint(0, num_classes, (1000,))\ntrain_dataset = TensorDataset(train_data, train_targets)\ndataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Training loop - metrics captured via framework hooks\nfor epoch in range(epochs):\n    for batch_idx, (inputs, targets) in enumerate(dataloader):\n        optimizer.zero_grad()\n        loss = criterion(model(inputs), targets)\n        loss.backward()\n        optimizer.step()  # Learning rate automatically logged\n        # Loss and gradient norms captured automatically (if track_model_gradients=True)\n\n        # Optional: Break after a few batches for demonstration\n        if batch_idx &gt;= 2:\n            break\n\nexperiment.end()\n</code></pre>"},{"location":"automagic/#how-automagic-works","title":"How Automagic Works","text":""},{"location":"automagic/#intelligent-parameter-detection","title":"Intelligent Parameter Detection","text":"<p>Automagic uses sophisticated heuristics to identify ML-relevant parameters:</p>"},{"location":"automagic/#1-name-pattern-recognition","title":"1. Name Pattern Recognition","text":"<pre><code># These are automatically detected by name patterns\nlearning_rate = 0.001      # Contains \"rate\"\nbatch_size = 64           # Contains \"size\"\nnum_layers = 5            # Starts with \"num_\"\nhidden_dim = 256          # Contains \"dim\"\nmax_epochs = 100          # Contains \"epoch\"\n</code></pre>"},{"location":"automagic/#2-value-range-analysis","title":"2. Value Range Analysis","text":"<pre><code># Detected by typical ML value ranges\nlearning_rate = 3e-4      # 0.00001 - 0.1 range\ndropout = 0.3             # 0 - 1 range for rates\nbatch_size = 128          # 1 - 1024 range (power of 2)\ntemperature = 2.0         # Scientific notation\n</code></pre>"},{"location":"automagic/#3-data-type-classification","title":"3. Data Type Classification","text":"<pre><code># Boolean hyperparameters\nuse_layer_norm = True     # Boolean flags\nenable_dropout = False    # use_*, enable_*, has_*\n\n# String configurations\noptimizer = \"adamw\"       # Optimizer names\nactivation = \"gelu\"       # Activation functions\nlr_scheduler = \"cosine\"   # Scheduler types\n</code></pre>"},{"location":"automagic/#4-keyword-detection","title":"4. Keyword Detection","text":"<pre><code># ML-specific keywords automatically recognized\nalpha = 0.7              # Regularization parameter\nbeta1 = 0.9              # Optimizer beta\nepsilon = 1e-8           # Numerical stability\npatience = 10            # Early stopping\nthreshold = 1e-4         # Convergence threshold\n</code></pre>"},{"location":"automagic/#framework-hooks","title":"Framework Hooks","text":"<p>Automagic automatically instruments popular ML frameworks:</p>"},{"location":"automagic/#pytorch-integration","title":"PyTorch Integration","text":"<ul> <li>Optimizer hooks: Automatically log learning rates and gradient norms</li> <li>Loss function hooks: Capture loss values during forward passes (Note: Only works with nn.Module-based losses like nn.CrossEntropyLoss(), not functional equivalents like torch.nn.functional.cross_entropy)</li> <li>Model hooks: Track model architecture and parameter counts</li> <li>Checkpoint hooks: Monitor model saving and loading</li> </ul>"},{"location":"automagic/#scikit-learn-integration","title":"Scikit-learn Integration","text":"<ul> <li>Estimator hooks: Capture model hyperparameters during <code>.fit()</code> calls</li> <li>Dataset hooks: Log training set size and feature dimensions</li> <li>Prediction hooks: Track inference statistics</li> </ul>"},{"location":"automagic/#xgboost-integration","title":"XGBoost Integration","text":"<ul> <li>Training hooks: Capture boosting parameters and evaluation metrics</li> <li>Parameter extraction: Automatic detection of tree-specific settings</li> </ul>"},{"location":"automagic/#smart-filtering","title":"Smart Filtering","text":"<p>Automagic intelligently excludes non-relevant variables:</p> <pre><code># \u274c Automatically excluded\ni = 0                     # Loop variables\nmodel = nn.Sequential()   # Complex objects\ndevice = \"cuda\"           # System variables\ntmp_value = 123          # Temporary variables\n_private_var = \"test\"    # Private variables\n\n# \u2705 Automatically included\nlearning_rate = 0.001    # ML hyperparameter\nbatch_size = 64          # Training parameter\nuse_attention = True     # Boolean configuration\n</code></pre>"},{"location":"automagic/#configuration","title":"Configuration","text":""},{"location":"automagic/#automagic-settings","title":"Automagic Settings","text":"<p>Control automagic behavior through configuration:</p> <pre><code>from tracelet.automagic import AutomagicConfig\n\nconfig = AutomagicConfig(\n    # Hyperparameter detection\n    detect_function_args=True,      # Function argument scanning\n    detect_class_attributes=True,   # Class attribute detection\n    detect_argparse=True,          # Command-line argument parsing\n    detect_config_files=True,      # Configuration file parsing\n\n    # Model tracking\n    track_model_architecture=True, # Model structure capture\n    track_model_checkpoints=True,  # Checkpoint monitoring\n    track_model_gradients=False,   # Gradient tracking (expensive)\n\n    # Dataset tracking\n    track_dataset_info=True,       # Dataset statistics\n    track_data_samples=False,      # Data sample logging (privacy)\n\n    # Training monitoring\n    monitor_training_loop=True,    # Training progress detection\n    monitor_loss_curves=True,      # Loss trend analysis\n    monitor_learning_rate=True,    # LR schedule tracking\n\n    # Resource monitoring\n    monitor_gpu_memory=True,       # GPU usage tracking\n    monitor_cpu_usage=True,        # CPU utilization\n\n    # Framework selection\n    frameworks={\"pytorch\", \"sklearn\", \"xgboost\"}\n)\n\nexperiment = Experiment(\n    \"configured_experiment\",\n    automagic=True,\n    automagic_config=config\n)\n</code></pre>"},{"location":"automagic/#environment-variables","title":"Environment Variables","text":"<p>Configure automagic via environment variables:</p> <pre><code># Enable/disable automagic\nexport TRACELET_ENABLE_AUTOMAGIC=true\n\n# Select frameworks to instrument\nexport TRACELET_AUTOMAGIC_FRAMEWORKS=\"pytorch,sklearn\"\n\n# Control detection scope\nexport TRACELET_DETECT_FUNCTION_ARGS=true\nexport TRACELET_DETECT_CLASS_ATTRIBUTES=true\nexport TRACELET_TRACK_MODEL_ARCHITECTURE=true\n</code></pre>"},{"location":"automagic/#advanced-usage","title":"Advanced Usage","text":""},{"location":"automagic/#manual-hyperparameter-capture","title":"Manual Hyperparameter Capture","text":"<p>Force capture of specific variables:</p> <pre><code>from tracelet.automagic import capture_hyperparams\n\n# Capture current scope variables and log them automatically\n# Note: This function automatically logs to the experiment, no need to call log_params\nhyperparams = capture_hyperparams(experiment)\n\n# Alternative: Use the experiment's built-in method\nhyperparams = experiment.capture_hyperparams()\n</code></pre>"},{"location":"automagic/#custom-detection-rules","title":"Custom Detection Rules","text":"<p>Automagic uses built-in detection patterns that can be customized through configuration:</p> <pre><code>from tracelet.automagic import AutomagicConfig\n\n# Configure detection patterns through AutomagicConfig\nconfig = AutomagicConfig(\n    # Enable/disable detection methods\n    detect_function_args=True,\n    detect_argparse=True,\n    detect_config_files=True,\n\n    # Framework-specific detection\n    frameworks={\"pytorch\", \"sklearn\", \"xgboost\"}\n)\n\nexperiment = Experiment(\n    \"custom_detection\",\n    automagic=True,\n    automagic_config=config\n)\n</code></pre>"},{"location":"automagic/#integration-with-existing-code","title":"Integration with Existing Code","text":"<p>Automagic works seamlessly with existing tracking:</p> <pre><code>experiment = Experiment(\"mixed_tracking\", automagic=True)\nexperiment.start()\n\n# Automagic captures these automatically\nlearning_rate = 0.001\nbatch_size = 64\n\n# Manual logging still works\nexperiment.log_params({\n    \"model_name\": \"custom_transformer\",\n    \"dataset_version\": \"v2.1\"\n})\n\n# Both automatic and manual tracking combined!\n</code></pre>"},{"location":"automagic/#performance-considerations","title":"Performance Considerations","text":""},{"location":"automagic/#overhead","title":"Overhead","text":"<p>Automagic is designed for minimal performance impact:</p> <ul> <li>Frame inspection: ~0.1ms per variable check</li> <li>Hook installation: One-time cost at experiment start</li> <li>Metric capture: Asynchronous, non-blocking</li> <li>Memory usage: &lt;10MB for typical experiments</li> </ul>"},{"location":"automagic/#best-practices","title":"Best Practices","text":"<ol> <li>Scope management: Define hyperparameters at function/class level</li> <li>Naming conventions: Use descriptive, ML-specific variable names</li> <li>Framework integration: Let automagic handle metric capture</li> <li>Selective enabling: Disable expensive features if not needed</li> </ol> <pre><code># \u2705 Good practice\ndef train_model():\n    learning_rate = 0.001  # Clear scope\n    batch_size = 64        # Descriptive name\n\n    experiment = Experiment(\"training\", automagic=True)\n    # Automagic captures hyperparameters from this scope\n\n# \u274c Avoid\nlearning_rate = 0.001      # Global scope (harder to detect)\nlr = 0.001                 # Ambiguous name\n</code></pre>"},{"location":"automagic/#troubleshooting","title":"Troubleshooting","text":""},{"location":"automagic/#common-issues","title":"Common Issues","text":"<p>Hyperparameters not detected:</p> <pre><code># Check variable scope and naming\ndef train():\n    learning_rate = 0.001  # \u2705 Function scope\n    lr = 0.001            # \u274c Ambiguous name\n\n# Ensure automagic is enabled\nexperiment = Experiment(\"test\", automagic=True)  # \u2705\n</code></pre> <p>Framework hooks not working:</p> <pre><code># Import frameworks after starting experiment\nexperiment.start()\nimport torch  # \u2705 Hooks installed after this import\n\n# Or restart Python session if hooks conflict\n</code></pre> <p>Performance concerns:</p> <pre><code># Disable expensive features\nconfig = AutomagicConfig(\n    track_model_gradients=False,  # Expensive\n    track_data_samples=False,     # Privacy risk\n    monitor_cpu_usage=False       # High frequency\n)\n</code></pre>"},{"location":"automagic/#debug-mode","title":"Debug Mode","text":"<p>Enable detailed logging to understand automagic behavior:</p> <pre><code>import logging\nlogging.getLogger(\"tracelet.automagic\").setLevel(logging.DEBUG)\n\nexperiment = Experiment(\"debug\", automagic=True)\n# Detailed logs show detection process\n</code></pre>"},{"location":"automagic/#examples","title":"Examples","text":"<p>See comprehensive examples in the examples documentation and the examples directory for automagic usage patterns and best practices.</p>"},{"location":"automagic/#api-reference","title":"API Reference","text":"<p>For detailed API documentation, refer to the automagic module source code and docstrings.</p>"},{"location":"configuration/","title":"Configuration","text":"<p>Tracelet can be configured in multiple ways to suit your workflow.</p>"},{"location":"configuration/#configuration-methods","title":"Configuration Methods","text":""},{"location":"configuration/#1-code-configuration","title":"1. Code Configuration","text":"<pre><code>import tracelet\n\n# Basic configuration\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\",\n    config={\n        \"tracking_uri\": \"http://localhost:5000\",\n        \"experiment_name\": \"Deep Learning Experiments\"\n    }\n)\n</code></pre>"},{"location":"configuration/#2-environment-variables","title":"2. Environment Variables","text":"<p>Set environment variables to configure defaults:</p> <pre><code>export TRACELET_BACKEND=mlflow\nexport TRACELET_PROJECT=my_default_project\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre>"},{"location":"configuration/#3-configuration-file","title":"3. Configuration File","text":"<p>Create a <code>tracelet.yaml</code> file in your project root:</p> <pre><code># tracelet.yaml\nbackend: mlflow\nproject: my_project\nexperiment_name: my_experiment\n\n# Backend-specific configuration\nmlflow:\n  tracking_uri: http://localhost:5000\n  experiment_name: \"ML Experiments\"\n\nclearml:\n  project_name: \"Tracelet Experiments\"\n  task_name: \"Training Run\"\n\nwandb:\n  project: \"tracelet-experiments\"\n  entity: \"your-username\"\n\naim:\n  repo_path: \"./aim_logs\"\n  experiment_name: \"Tracelet Experiments\"\n</code></pre>"},{"location":"configuration/#backend-specific-configuration","title":"Backend-Specific Configuration","text":""},{"location":"configuration/#mlflow","title":"MLflow","text":"<pre><code>tracelet.start_logging(\n    backend=\"mlflow\",\n    config={\n        \"tracking_uri\": \"http://localhost:5000\",\n        \"experiment_name\": \"My Experiments\",\n        \"run_name\": \"run_001\",\n        \"tags\": {\"team\": \"ml\", \"version\": \"v1.0\"}\n    }\n)\n</code></pre> <p>Environment variables:</p> <pre><code>export MLFLOW_TRACKING_URI=http://localhost:5000\nexport MLFLOW_EXPERIMENT_NAME=\"My Experiments\"\n</code></pre>"},{"location":"configuration/#clearml","title":"ClearML","text":"<pre><code>tracelet.start_logging(\n    backend=\"clearml\",\n    config={\n        \"project_name\": \"Tracelet Experiments\",\n        \"task_name\": \"Training Session\",\n        \"tags\": [\"pytorch\", \"experiment\"],\n        \"output_uri\": \"s3://my-bucket/clearml-output\"\n    }\n)\n</code></pre> <p>Environment variables:</p> <pre><code>export CLEARML_WEB_HOST=https://app.clear.ml\nexport CLEARML_API_HOST=https://api.clear.ml\nexport CLEARML_FILES_HOST=https://files.clear.ml\nexport CLEARML_API_ACCESS_KEY=your_access_key\nexport CLEARML_API_SECRET_KEY=your_secret_key\n</code></pre>"},{"location":"configuration/#weights-biases","title":"Weights &amp; Biases","text":"<pre><code>tracelet.start_logging(\n    backend=\"wandb\",\n    config={\n        \"project\": \"tracelet-experiments\",\n        \"entity\": \"your-username\",\n        \"name\": \"experiment_001\",\n        \"tags\": [\"pytorch\", \"baseline\"],\n        \"mode\": \"online\"  # or \"offline\"\n    }\n)\n</code></pre> <p>Environment variables:</p> <pre><code>export WANDB_API_KEY=your_api_key\nexport WANDB_PROJECT=tracelet-experiments\nexport WANDB_ENTITY=your-username\n</code></pre>"},{"location":"configuration/#aim","title":"AIM","text":"<pre><code>tracelet.start_logging(\n    backend=\"aim\",\n    config={\n        \"repo_path\": \"./aim_repo\",\n        \"experiment_name\": \"Tracelet Experiments\",\n        \"run_name\": \"baseline_run\",\n        \"tags\": {\"model\": \"resnet\", \"dataset\": \"cifar10\"}\n    }\n)\n</code></pre> <p>For remote AIM:</p> <pre><code>tracelet.start_logging(\n    backend=\"aim\",\n    config={\n        \"remote_uri\": \"http://aim-server:53800\",\n        \"experiment_name\": \"Remote Experiments\"\n    }\n)\n</code></pre>"},{"location":"configuration/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"configuration/#system-metrics-collection","title":"System Metrics Collection","text":"<pre><code>tracelet.start_logging(\n    backend=\"mlflow\",\n    config={\n        \"collect_system_metrics\": True,\n        \"system_metrics_interval\": 30,  # seconds\n        \"collect_gpu_metrics\": True\n    }\n)\n</code></pre>"},{"location":"configuration/#git-integration","title":"Git Integration","text":"<pre><code>tracelet.start_logging(\n    backend=\"mlflow\",\n    config={\n        \"track_git_info\": True,\n        \"git_repo_path\": \".\",\n        \"include_uncommitted_changes\": True\n    }\n)\n</code></pre>"},{"location":"configuration/#multi-backend-configuration","title":"Multi-Backend Configuration","text":"<pre><code>tracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\"],\n    config={\n        \"mlflow\": {\n            \"tracking_uri\": \"http://localhost:5000\"\n        },\n        \"wandb\": {\n            \"project\": \"multi-backend-experiment\"\n        }\n    }\n)\n</code></pre>"},{"location":"configuration/#configuration-priority","title":"Configuration Priority","text":"<p>Configuration values are resolved in this order (highest to lowest priority):</p> <ol> <li>Direct function arguments</li> <li>Environment variables</li> <li>Configuration file (<code>tracelet.yaml</code>)</li> <li>Default values</li> </ol>"},{"location":"configuration/#validation","title":"Validation","text":"<p>Tracelet validates your configuration at startup:</p> <pre><code># Invalid configuration will raise an error\ntry:\n    tracelet.start_logging(backend=\"nonexistent\")\nexcept ValueError as e:\n    print(f\"Configuration error: {e}\")\n</code></pre>"},{"location":"configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Environment-specific configs: Use different config files for dev/staging/prod</li> <li>Secrets: Store API keys and credentials in environment variables, not config files</li> <li>Project organization: Use consistent project and experiment naming conventions</li> <li>Defaults: Set sensible defaults in your config file to reduce boilerplate</li> </ol>"},{"location":"configuration/#configuration-reference","title":"Configuration Reference","text":"<p>Complete configuration options for each backend:</p> <p>MLflow Configuration \u2192 ClearML Configuration \u2192 W&amp;B Configuration \u2192 AIM Configuration \u2192</p>"},{"location":"examples/","title":"Examples","text":"<p>This page contains examples of how to use Tracelet.</p>"},{"location":"examples/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start experiment tracking with your preferred backend\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"  # or \"clearml\", \"wandb\", \"aim\"\n)\n\n# Use TensorBoard as usual - metrics are automatically captured\nwriter = SummaryWriter()\nfor epoch in range(100):\n    loss = 0.9**epoch\n    writer.add_scalar('Loss/train', loss, epoch)\n    # Metrics are automatically sent to MLflow!\n\n# Stop tracking when done\ntracelet.stop_logging()\n</code></pre>"},{"location":"examples/#multi-backend-comparison","title":"Multi-Backend Comparison","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nExample: Multi-Backend Comparison with Tracelet\n\nThis example demonstrates how to run the same experiment across multiple backends\n(MLflow, ClearML, and W&amp;B) to compare their capabilities and performance.\n\nPrerequisites:\n- MLflow: pip install mlflow (works out of the box)\n- ClearML: pip install clearml (works in offline mode)\n- W&amp;B: pip install wandb (works in offline mode if no API key)\n\nUsage:\n    python examples/multi_backend_comparison.py\n\"\"\"\n\nimport importlib.util\nimport os\nimport time\nfrom pathlib import Path\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torch.utils.tensorboard import SummaryWriter\n\nimport tracelet\n\n\nclass SimpleNN(nn.Module):\n    \"\"\"Simple neural network for demonstration.\"\"\"\n\n    def __init__(self, input_size=20, hidden_size=64, output_size=3):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Linear(input_size, hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(hidden_size // 2, output_size),\n        )\n\n    def forward(self, x):\n        return self.network(x)\n\n\ndef generate_synthetic_data(num_samples=2000, input_size=20, num_classes=3):\n    \"\"\"Generate synthetic classification data.\"\"\"\n    torch.manual_seed(42)\n    data, labels = [], []\n    for class_idx in range(num_classes):\n        samples_per_class = num_samples // num_classes\n        base_values = torch.randn(input_size) * 0.5 + (class_idx - 1) * 2\n        class_data = (\n            torch.randn(samples_per_class, input_size) * 0.8 + base_values\n        )\n        class_labels = torch.full(\n            (samples_per_class,), class_idx, dtype=torch.long\n        )\n        data.append(class_data)\n        labels.append(class_labels)\n\n    X, y = torch.cat(data, dim=0), torch.cat(labels, dim=0)\n    indices = torch.randperm(len(X))\n    return X[indices], y[indices]\n\n\ndef _setup_backend_environment(backend_name: str):\n    \"\"\"Configure environment for a specific backend (e.g., offline mode).\"\"\"\n    if backend_name == \"wandb\" and not os.environ.get(\"WANDB_API_KEY\"):\n        os.environ[\"WANDB_MODE\"] = \"offline\"\n        print(\"\ud83d\udcdd W&amp;B: Running in offline mode (no API key found)\")\n    elif backend_name == \"clearml\":\n        os.environ.update(\n            {\n                \"CLEARML_WEB_HOST\": \"\",\n                \"CLEARML_API_HOST\": \"\",\n                \"CLEARML_FILES_HOST\": \"\",\n                \"CLEARML_OFFLINE_MODE\": \"1\",\n            }\n        )\n        print(\"\ud83d\udcdd ClearML: Running in offline mode\")\n\n\ndef _get_dataloaders(batch_size=64):\n    \"\"\"Prepare and return train/validation data loaders.\"\"\"\n    X, y = generate_synthetic_data()\n    split_idx = int(0.8 * len(X))\n    train_X, val_X = X[:split_idx], X[split_idx:]\n    train_y, val_y = y[:split_idx], y[split_idx:]\n\n    train_dataset = TensorDataset(train_X, train_y)\n    val_dataset = TensorDataset(val_X, val_y)\n    train_loader = DataLoader(\n        train_dataset, batch_size=batch_size, shuffle=True\n    )\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    return train_loader, val_loader, len(train_dataset), len(val_dataset)\n\n\ndef _train_epoch(model, loader, criterion, optimizer, exp, writer, epoch):\n    \"\"\"Run a single training epoch.\"\"\"\n    model.train()\n    total_loss, correct, total = 0.0, 0, 0\n    for i, (data, target) in enumerate(loader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n        pred = output.argmax(dim=1)\n        correct += pred.eq(target).sum().item()\n        total += target.size(0)\n\n        if i % 10 == 0:\n            step = epoch * len(loader) + i\n            exp.log_metric(\"train/batch_loss\", loss.item(), step)\n            writer.add_scalar(\"train/batch_loss\", loss.item(), step)\n\n    return total_loss / len(loader), 100.0 * correct / total\n\n\ndef _validate_epoch(model, loader, criterion):\n    \"\"\"Run a single validation epoch.\"\"\"\n    model.eval()\n    total_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for data, target in loader:\n            output = model(data)\n            total_loss += criterion(output, target).item()\n            pred = output.argmax(dim=1)\n            correct += pred.eq(target).sum().item()\n            total += target.size(0)\n    return total_loss / len(loader), 100.0 * correct / total\n\n\ndef _log_epoch_metrics(\n    exp, writer, metrics: dict, epoch: int, model: nn.Module\n):\n    \"\"\"Log all metrics for a given epoch.\"\"\"\n    for key, value in metrics.items():\n        exp.log_metric(key, value, epoch)\n        writer.add_scalar(key, value, epoch)\n\n    if epoch % 3 == 0:\n        for name, param in model.named_parameters():\n            writer.add_histogram(f\"params/{name}\", param, epoch)\n            if param.grad is not None:\n                writer.add_histogram(f\"gradients/{name}\", param.grad, epoch)\n\n\ndef run_experiment_with_backend(\n    backend_name: str, project_name: str = \"Multi-Backend Comparison\"\n):\n    \"\"\"Run the same experiment with a specific backend.\"\"\"\n    print(\n        f\"\\n{'='*60}\\n\ud83d\ude80 Running experiment with {backend_name.upper()} backend\\n{'='*60}\"\n    )\n    _setup_backend_environment(backend_name)\n\n    train_loader, val_loader, num_train, num_val = _get_dataloaders()\n    model = SimpleNN()\n    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n    criterion = nn.CrossEntropyLoss()\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, patience=3, factor=0.5\n    )\n\n    exp = tracelet.start_logging(\n        exp_name=f\"{backend_name}_classification_experiment\",\n        project=project_name,\n        backend=backend_name,\n    )\n    print(f\"\u2705 Started {backend_name} experiment: {exp.name}\")\n\n    hyperparams = {\n        \"learning_rate\": 0.001, \"batch_size\": 64, \"epochs\": 12\n    }\n    exp.log_params(hyperparams)\n\n    temp_dir = Path(f\"./demo_results/{backend_name}_tensorboard\")\n    temp_dir.mkdir(parents=True, exist_ok=True)\n    writer = SummaryWriter(str(temp_dir))\n\n    start_time = time.time()\n    best_val_accuracy = 0.0\n    for epoch in range(12):\n        train_loss, train_acc = _train_epoch(\n            model, train_loader, criterion, optimizer, exp, writer, epoch\n        )\n        val_loss, val_acc = _validate_epoch(model, val_loader, criterion)\n        scheduler.step(val_loss)\n\n        if val_acc &gt; best_val_accuracy:\n            best_val_accuracy = val_acc\n\n        metrics = {\n            \"train/epoch_loss\": train_loss,\n            \"train/epoch_accuracy\": train_acc,\n            \"val/epoch_loss\": val_loss,\n            \"val/epoch_accuracy\": val_acc,\n            \"val/best_accuracy\": best_val_accuracy,\n            \"train/learning_rate\": optimizer.param_groups[0][\"lr\"],\n        }\n        _log_epoch_metrics(exp, writer, metrics, epoch, model)\n\n        print(\n            f\"Epoch {epoch:2d}/11: Train Loss={train_loss:.4f}, Acc={train_acc:.2f}% | \"\n            f\"Val Loss={val_loss:.4f}, Acc={val_acc:.2f}%\"\n        )\n\n    execution_time = time.time() - start_time\n    exp.log_metric(\"final/best_val_accuracy\", best_val_accuracy, epoch)\n    exp.log_metric(\"final/execution_time\", execution_time, epoch)\n\n    writer.close()\n    tracelet.stop_logging()\n    return {\n        \"backend\": backend_name,\n        \"best_val_accuracy\": best_val_accuracy,\n        \"execution_time\": execution_time,\n    }\n\n\ndef _is_backend_available(backend_name: str) -&gt; bool:\n    \"\"\"Check if a backend's library is installed.\"\"\"\n    return importlib.util.find_spec(backend_name) is not None\n\n\ndef _get_available_backends(backends_to_check: list[str]) -&gt; list[str]:\n    \"\"\"Get a list of available backends from a given list.\"\"\"\n    available = []\n    for backend in backends_to_check:\n        if _is_backend_available(backend):\n            print(f\"\u2705 {backend.upper()}: Available\")\n            available.append(backend)\n        else:\n            print(f\"\u274c {backend.upper()}: Not installed\")\n    return available\n\n\ndef _print_comparison_report(results: list[dict], total_time: float):\n    \"\"\"Print a formatted report comparing backend performance.\"\"\"\n    if not results or len(results) &lt;= 1:\n        return\n\n    print(f\"\\n{'='*80}\\n\ud83d\udcca MULTI-BACKEND COMPARISON RESULTS\\n{'='*80}\")\n    print(f\"{'Backend':&lt;12} {'Best Val Acc':&lt;15} {'Time (s)':&lt;10}\")\n    print(\"-\" * 80)\n\n    for res in results:\n        print(\n            f\"{res['backend'].upper():&lt;12} {res['best_val_accuracy']:&lt;15.2f}% \"\n            f\"{res['execution_time']:&lt;10.2f}\"\n        )\n\n    best_accuracy = max(results, key=lambda x: x[\"best_val_accuracy\"])\n    fastest_time = min(results, key=lambda x: x[\"execution_time\"])\n    accuracies = [r[\"best_val_accuracy\"] for r in results]\n    accuracy_std = np.std(accuracies)\n\n    print(\"\\n\ud83c\udfc6 PERFORMANCE HIGHLIGHTS:\")\n    print(\n        f\"   \ud83c\udfaf Best Accuracy: {best_accuracy['backend'].upper()} \"\n        f\"({best_accuracy['best_val_accuracy']:.2f}%)\"\n    )\n    print(\n        f\"   \u26a1 Fastest Training: {fastest_time['backend'].upper()} \"\n        f\"({fastest_time['execution_time']:.2f}s)\"\n    )\n    print(f\"   \ud83d\udcca Total Execution Time: {total_time:.2f}s\")\n    print(\n        f\"   \ud83d\udcc8 Accuracy Consistency: \u00b1{accuracy_std:.2f}% (lower is more consistent)\"\n    )\n\n\ndef main():\n    \"\"\"Run multi-backend comparison experiment.\"\"\"\n    print(\"\ud83c\udf1f Tracelet Multi-Backend Comparison Example\")\n    backends_to_test = [\"mlflow\", \"clearml\", \"wandb\"]\n    available_backends = _get_available_backends(backends_to_test)\n\n    if not available_backends:\n        print(\"\\n\u274c No backends available! Please install at least one.\")\n        return\n\n    print(f\"\\n\ud83d\ude80 Running experiments with: {', '.join(available_backends)}\")\n    results = []\n    overall_start_time = time.time()\n\n    for backend in available_backends:\n        try:\n            result = run_experiment_with_backend(backend)\n            results.append(result)\n        except Exception as e:\n            print(f\"\u274c Failed to run experiment with {backend}: {e}\")\n\n    overall_execution_time = time.time() - overall_start_time\n    _print_comparison_report(results, overall_execution_time)\n\n    print(\"\\n\ud83c\udf89 Multi-backend comparison completed successfully!\")\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/#aim-backend-example","title":"AIM Backend Example","text":"<pre><code>import tracelet\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start experiment tracking with your preferred backend\ntracelet.start_logging(\n    exp_name=\"my_aim_experiment\",\n    project=\"my_aim_project\",\n    backend=\"aim\"  # Use the AIM backend\n)\n\n# Use TensorBoard as usual - metrics are automatically captured\nwriter = SummaryWriter()\nfor epoch in range(10):\n    loss = 0.9**epoch  # Example loss\n    writer.add_scalar('Loss/train', loss, epoch)\n    # Metrics are automatically sent to AIM!\n\n# Log some parameters\ntracelet.get_active_experiment().log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"epochs\": 10\n})\n\n# Log an artifact (e.g., a dummy model file)\nwith open(\"dummy_model.txt\", \"w\") as f:\n    f.write(\"This is a dummy model file.\")\ntracelet.get_active_experiment().log_artifact(\"dummy_model.txt\")\n\n# Stop tracking when done\ntracelet.stop_logging()\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#quick-installation","title":"Quick Installation","text":"pip <p><code>bash     pip install tracelet</code></p> uv <p><code>bash     uv add tracelet</code></p> conda <p><code>bash     conda install -c conda-forge tracelet</code></p>"},{"location":"installation/#backend-dependencies","title":"Backend Dependencies","text":"<p>Tracelet supports multiple experiment tracking backends. Install the backend you want to use:</p> MLflow <p><code>bash     pip install tracelet[mlflow]     # or     pip install mlflow</code></p> ClearML <p><code>bash     pip install tracelet[clearml]     # or     pip install clearml</code></p> Weights &amp; Biases <p><code>bash     pip install tracelet[wandb]     # or     pip install wandb</code></p> AIM <p><code>bash     pip install tracelet[aim]     # or     pip install aim</code></p>"},{"location":"installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Python 3.8+</li> <li>PyTorch (optional, for enhanced integrations)</li> <li>PyTorch Lightning (optional, for Lightning integration)</li> </ul>"},{"location":"installation/#development-installation","title":"Development Installation","text":"<p>For development or to get the latest features:</p> <pre><code>git clone https://github.com/prassanna-ravishankar/tracelet.git\ncd tracelet\nuv pip install -e \".[dev,all]\"\n</code></pre>"},{"location":"installation/#verification","title":"Verification","text":"<p>Verify your installation:</p> <pre><code>import tracelet\nprint(tracelet.__version__)\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<p>Choose your backend and follow the configuration guide:</p> <ul> <li>MLflow Setup</li> <li>ClearML Setup</li> <li>W&amp;B Setup</li> <li>AIM Setup</li> </ul>"},{"location":"quick-start/","title":"Quick Start","text":"<p>Get up and running with Tracelet in under 5 minutes!</p>"},{"location":"quick-start/#1-installation","title":"1. Installation","text":"pip <p><code>bash     pip install tracelet mlflow</code></p> uv <p><code>bash     uv add tracelet mlflow</code></p>"},{"location":"quick-start/#2-basic-usage","title":"2. Basic Usage","text":"<p>Here's a complete example using PyTorch with TensorBoard:</p> <pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# 1. Start experiment tracking\ntracelet.start_logging(\n    exp_name=\"my_first_experiment\",\n    project=\"tracelet_demo\",\n    backend=\"mlflow\"  # or \"clearml\", \"wandb\", \"aim\"\n)\n\n# 2. Create a simple model and data\nmodel = nn.Linear(10, 1)\noptimizer = optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\n# Synthetic data\nX = torch.randn(100, 10)\ny = torch.randn(100, 1)\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=16)\n\n# 3. Use TensorBoard as normal - metrics are automatically captured!\nwriter = SummaryWriter()\n\nfor epoch in range(50):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n        # Log batch metrics - automatically sent to MLflow!\n        writer.add_scalar('Loss/batch', loss.item(), epoch * len(dataloader) + batch_idx)\n\n    # Log epoch metrics\n    avg_loss = total_loss / len(dataloader)\n    writer.add_scalar('Loss/epoch', avg_loss, epoch)\n\n    print(f\"Epoch {epoch:2d}: Loss = {avg_loss:.4f}\")\n\n# 4. Log additional experiment info\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 16,\n    \"epochs\": 50,\n    \"model\": \"linear\"\n})\n\n# 5. Clean up\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Experiment completed! Check your MLflow UI to see the results.\")\n</code></pre>"},{"location":"quick-start/#3-view-results","title":"3. View Results","text":""},{"location":"quick-start/#mlflow","title":"MLflow","text":"<pre><code>mlflow ui\n# Visit http://localhost:5000\n</code></pre>"},{"location":"quick-start/#clearml","title":"ClearML","text":"<p>Visit app.clear.ml or your ClearML server</p>"},{"location":"quick-start/#weights-biases","title":"Weights &amp; Biases","text":"<p>Visit wandb.ai</p>"},{"location":"quick-start/#aim","title":"AIM","text":"<pre><code>aim up\n# Visit http://localhost:43800\n</code></pre>"},{"location":"quick-start/#what-just-happened","title":"What Just Happened?","text":"<ol> <li>Automatic Capture: Your existing <code>SummaryWriter.add_scalar()</code> calls were automatically intercepted</li> <li>Zero Code Changes: No modifications to your existing TensorBoard code</li> <li>Multi-Backend: Same code works with MLflow, ClearML, W&amp;B, or AIM</li> <li>Rich Logging: Scalars, parameters, and system info automatically tracked</li> </ol>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration Guide - Customize your setup</li> <li>Backend Guides - Deep dive into each backend</li> <li>PyTorch Integration - Advanced PyTorch features</li> <li>Examples - More comprehensive examples</li> </ul> <p>Pro Tip</p> <p>Try the multi-backend example to compare different tracking platforms with the same experiment!</p>"},{"location":"api/","title":"API Reference","text":"<p>Complete API documentation for Tracelet's public interfaces.</p>"},{"location":"api/#quick-navigation","title":"Quick Navigation","text":""},{"location":"api/#core-api","title":"Core API","text":"<ul> <li>Main Interface - Primary entry points (<code>start_logging</code>, <code>get_active_experiment</code>, <code>stop_logging</code>)</li> <li>Experiment - Main experiment management class</li> <li>Settings - Configuration and settings management</li> </ul>"},{"location":"api/#backends","title":"Backends","text":"<ul> <li>MLflow - MLflow integration</li> <li>ClearML - ClearML integration</li> <li>Weights &amp; Biases - W&amp;B integration</li> <li>AIM - AIM integration</li> </ul>"},{"location":"api/#frameworks","title":"Frameworks","text":"<ul> <li>PyTorch - PyTorch and TensorBoard integration</li> <li>Lightning - PyTorch Lightning integration</li> </ul>"},{"location":"api/#data-collection","title":"Data Collection","text":"<ul> <li>Git Collector - Git repository information</li> <li>System Metrics - System performance metrics</li> </ul>"},{"location":"api/#plugin-system","title":"Plugin System","text":"<ul> <li>Core Plugins - Plugin architecture and interfaces</li> <li>Data Flow Management - Orchestrator and data flow</li> </ul>"},{"location":"api/#usage-examples","title":"Usage Examples","text":""},{"location":"api/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\n\n# Start experiment tracking\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Get active experiment and log metrics\nexp = tracelet.get_active_experiment()\nexp.log_metric(\"accuracy\", 0.95, iteration=100)\nexp.log_params({\"lr\": 0.001, \"batch_size\": 32})\n\n# Stop tracking\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/#multi-backend-usage","title":"Multi-Backend Usage","text":"<pre><code>import tracelet\n\n# Track to multiple backends simultaneously\ntracelet.start_logging(\n    exp_name=\"multi_backend_experiment\",\n    project=\"comparison_study\",\n    backend=[\"mlflow\", \"wandb\"]  # List of backends\n)\n</code></pre>"},{"location":"api/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>from tracelet.settings import TraceletSettings\n\nsettings = TraceletSettings(\n    project=\"advanced_project\",\n    backend=[\"mlflow\"],\n    track_system=True,\n    metrics_interval=5.0\n)\n\ntracelet.start_logging(\n    exp_name=\"advanced_experiment\",\n    settings=settings\n)\n</code></pre>"},{"location":"api/core/","title":"Core API Reference","text":"<p>This section covers the core APIs and classes that form the foundation of Tracelet.</p>"},{"location":"api/core/#main-interface","title":"Main Interface","text":""},{"location":"api/core/#traceletstart_logging","title":"<code>tracelet.start_logging()</code>","text":"<p>Start experiment tracking with the specified configuration.</p> <p>Parameters:</p> <ul> <li><code>exp_name: str</code> - Name of the experiment</li> <li><code>project: str</code> - Project name (optional)</li> <li><code>backend: str | List[str]</code> - Backend(s) to use</li> <li><code>config: dict</code> - Additional configuration (optional)</li> </ul> <p>Returns: <code>Experiment</code> - The created experiment instance</p>"},{"location":"api/core/#traceletstop_logging","title":"<code>tracelet.stop_logging()</code>","text":"<p>Stop the current experiment tracking session.</p>"},{"location":"api/core/#traceletget_active_experiment","title":"<code>tracelet.get_active_experiment()</code>","text":"<p>Get the currently active experiment instance.</p> <p>Returns: <code>Experiment | None</code> - The active experiment, or None if no experiment is active</p>"},{"location":"api/core/#core-classes","title":"Core Classes","text":""},{"location":"api/core/#experiment","title":"Experiment","text":"<p>The main experiment tracking interface.</p> <p>Key Methods:</p> <ul> <li><code>log_metric(name, value, step)</code> - Log a scalar metric</li> <li><code>log_params(params)</code> - Log experiment parameters</li> <li><code>log_artifact(path)</code> - Log an artifact file</li> <li><code>log_image(name, image, step)</code> - Log an image</li> <li><code>log_text(name, text, step)</code> - Log text data</li> </ul> <p>Full API documentation coming soon.</p>"},{"location":"api/core/#orchestrator","title":"Orchestrator","text":"<p>The Orchestrator class manages metric routing and backend coordination.</p> <p>Key Methods:</p> <ul> <li><code>start()</code> - Start the orchestrator and worker threads</li> <li><code>stop()</code> - Stop all operations gracefully</li> <li><code>route_metric(metric)</code> - Route a metric to all configured backends</li> <li><code>add_backend(backend)</code> - Add a new backend to the orchestrator</li> <li><code>remove_backend(backend)</code> - Remove a backend from the orchestrator</li> </ul> <p>Full API documentation coming soon.</p>"},{"location":"api/core/#plugin-system","title":"Plugin System","text":""},{"location":"api/core/#plugin-interfaces","title":"Plugin Interfaces","text":"<p>Tracelet uses a plugin-based architecture for extensibility.</p> <p>PluginInterface - Base interface for all plugins BackendPlugin - Interface for experiment tracking backends FrameworkPlugin - Interface for ML framework integrations</p>"},{"location":"api/core/#plugin-metadata","title":"Plugin Metadata","text":"<p>PluginMetadata - Contains plugin information (name, version, description) PluginType - Enum defining plugin types (BACKEND, FRAMEWORK, COLLECTOR)</p> <p>Full plugin API documentation coming soon.</p>"},{"location":"api/core/#configuration","title":"Configuration","text":""},{"location":"api/core/#settings","title":"Settings","text":"<p>TraceletSettings - Main configuration class with these key settings:</p> <ul> <li><code>project: str</code> - Default project name</li> <li><code>backend: List[str]</code> - Default backends to use</li> <li><code>track_system: bool</code> - Enable system metrics tracking</li> <li><code>track_git: bool</code> - Enable git repository tracking</li> <li><code>track_env: bool</code> - Enable environment tracking</li> <li><code>metrics_interval: float</code> - System metrics collection interval</li> </ul>"},{"location":"api/core/#exceptions","title":"Exceptions","text":""},{"location":"api/core/#base-exceptions","title":"Base Exceptions","text":"<p>TraceletException - Base exception for all Tracelet errors BackendError - Errors related to backend operations ConfigurationError - Errors in configuration or setup</p> <p>Full exception API documentation coming soon.</p>"},{"location":"api/core/#usage-examples","title":"Usage Examples","text":""},{"location":"api/core/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start experiment tracking\nexperiment = tracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Use TensorBoard normally - metrics automatically captured\nwriter = SummaryWriter()\nwriter.add_scalar('loss', 0.5, 0)\nwriter.add_scalar('accuracy', 0.95, 0)\n\n# Direct API usage\nexperiment.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32\n})\n\nexperiment.log_artifact(\"model.pth\")\n\n# Stop tracking\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/core/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code>import tracelet\n\n# Multi-backend logging with custom configuration\nexperiment = tracelet.start_logging(\n    exp_name=\"advanced_experiment\",\n    project=\"research_project\",\n    backend=[\"mlflow\", \"wandb\"],\n    config={\n        \"track_system\": True,\n        \"track_git\": True,\n        \"track_env\": True,\n        \"metrics_interval\": 10.0,\n        \"mlflow_tracking_uri\": \"http://localhost:5000\",\n        \"wandb_project\": \"my-wandb-project\"\n    }\n)\n\n# Get current experiment for direct manipulation\ncurrent_exp = tracelet.get_active_experiment()\nprint(f\"Experiment ID: {current_exp.experiment_id}\")\nprint(f\"Active backends: {[b.name for b in current_exp.backends]}\")\n</code></pre>"},{"location":"api/core/#context-manager-usage","title":"Context Manager Usage","text":"<pre><code>import tracelet\n\n# Use as context manager for automatic cleanup\nwith tracelet.start_logging(\"context_experiment\", backend=\"mlflow\") as exp:\n    # Training code here\n    for epoch in range(10):\n        loss = train_epoch()\n        exp.log_metric(\"loss\", loss, epoch)\n\n    # Experiment automatically closed when exiting context\n</code></pre>"},{"location":"api/core/#type-hints","title":"Type Hints","text":"<p>Tracelet provides comprehensive type hints for better IDE support:</p> <pre><code>from typing import Dict, List, Optional, Union\nfrom tracelet.core.experiment import Experiment\nfrom tracelet.core.plugins import BackendPlugin\n\ndef my_training_function(\n    experiment: Experiment,\n    hyperparams: Dict[str, Union[str, int, float]],\n    backends: Optional[List[str]] = None\n) -&gt; None:\n    \"\"\"Example function with proper type hints\"\"\"\n    experiment.log_params(hyperparams)\n    # Training logic here\n</code></pre> <p>For more detailed information about specific modules, see the dedicated API reference pages for each component.</p>"},{"location":"api/interface/","title":"Main Interface","text":"<p>options: show_source: true show_bases: true heading_level: 2</p>"},{"location":"api/interface/#tracelet.interface.get_active_experiment","title":"<code>get_active_experiment()</code>","text":"<p>Get the currently active experiment</p> Source code in <code>tracelet/interface.py</code> <pre><code>def get_active_experiment() -&gt; Optional[Experiment]:\n    \"\"\"Get the currently active experiment\"\"\"\n    return _active_experiment\n</code></pre>"},{"location":"api/interface/#tracelet.interface.start_logging","title":"<code>start_logging(exp_name=None, project=None, backend=None, api_key=None, backend_url=None, config=None)</code>","text":"<p>Start logging metrics and metadata for your ML experiment.</p> <p>Args:     exp_name: Name of the experiment. If not provided, uses TRACELET_EXPERIMENT_NAME env var     project: Project name. If not provided, uses TRACELET_PROJECT env var     backend: Backend to use (\"mlflow\", \"wandb\", \"aim\"). If not provided, uses TRACELET_BACKEND env var     api_key: API key for the backend. If not provided, uses TRACELET_API_KEY env var     backend_url: Backend URL. If not provided, uses TRACELET_BACKEND_URL env var     config: Additional configuration to override defaults and env vars</p> <p>Returns:     Experiment: The active experiment instance</p> <p>Example:     <pre><code>import tracelet\n\n# Start logging with minimal config\ntracelet.start_logging(\"my_experiment\")\n\n# Or with more configuration\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"wandb\",\n    api_key=\"...\",\n)\n</code></pre></p> Source code in <code>tracelet/interface.py</code> <pre><code>def start_logging(\n    exp_name: Optional[str] = None,\n    project: Optional[str] = None,\n    backend: Optional[str] = None,\n    api_key: Optional[str] = None,\n    backend_url: Optional[str] = None,\n    config: Optional[dict[str, Any]] = None,\n) -&gt; Experiment:\n    \"\"\"Start logging metrics and metadata for your ML experiment.\n\n    Args:\n        exp_name: Name of the experiment. If not provided, uses TRACELET_EXPERIMENT_NAME env var\n        project: Project name. If not provided, uses TRACELET_PROJECT env var\n        backend: Backend to use (\"mlflow\", \"wandb\", \"aim\"). If not provided, uses TRACELET_BACKEND env var\n        api_key: API key for the backend. If not provided, uses TRACELET_API_KEY env var\n        backend_url: Backend URL. If not provided, uses TRACELET_BACKEND_URL env var\n        config: Additional configuration to override defaults and env vars\n\n    Returns:\n        Experiment: The active experiment instance\n\n    Example:\n        ```python\n        import tracelet\n\n        # Start logging with minimal config\n        tracelet.start_logging(\"my_experiment\")\n\n        # Or with more configuration\n        tracelet.start_logging(\n            exp_name=\"my_experiment\",\n            project=\"my_project\",\n            backend=\"wandb\",\n            api_key=\"...\",\n        )\n        ```\n    \"\"\"\n    global _active_experiment, _settings\n\n    # Stop any existing experiment first\n    if _active_experiment:\n        _active_experiment.stop()\n        _active_experiment = None\n\n    # Initialize settings - let TraceletSettings handle env vars\n    settings_dict = {}\n    if exp_name is not None:\n        settings_dict[\"experiment_name\"] = exp_name\n    if project is not None:\n        settings_dict[\"project_name\"] = project\n    if backend is not None:\n        settings_dict[\"backend\"] = backend\n    if api_key is not None:\n        settings_dict[\"api_key\"] = api_key\n    if backend_url is not None:\n        settings_dict[\"backend_url\"] = backend_url\n    if config:\n        settings_dict.update(config)\n\n    _settings = TraceletSettings(**settings_dict)\n\n    # Create experiment config\n    exp_config = ExperimentConfig(\n        track_metrics=True,\n        track_environment=_settings.track_env,\n        track_args=True,\n        track_stdout=True,\n        track_checkpoints=True,\n        track_system_metrics=_settings.track_system_metrics,\n        track_git=_settings.track_git,\n    )\n\n    # Create experiment\n    _active_experiment = Experiment(\n        name=_settings.experiment_name or \"default_experiment\",\n        config=exp_config,\n        backend=_settings.backend,\n        tags=[f\"project:{_settings.project_name}\"],\n    )\n\n    # Initialize frameworks based on settings\n    if _settings.track_tensorboard:\n        pytorch = PyTorchFramework(patch_tensorboard=True)\n        _active_experiment._framework = pytorch\n        pytorch.initialize(_active_experiment)\n\n    if _settings.track_lightning:\n        lightning = LightningFramework()\n        lightning.initialize(_active_experiment)\n\n    # Start tracking\n    _active_experiment.start()\n\n    return _active_experiment\n</code></pre>"},{"location":"api/interface/#tracelet.interface.stop_logging","title":"<code>stop_logging()</code>","text":"<p>Stop the active experiment and cleanup</p> Source code in <code>tracelet/interface.py</code> <pre><code>def stop_logging():\n    \"\"\"Stop the active experiment and cleanup\"\"\"\n    global _active_experiment\n    if _active_experiment:\n        _active_experiment.stop()\n        _active_experiment = None\n</code></pre>"},{"location":"api/interface/#main-public-functions","title":"Main Public Functions","text":"<p>The main interface provides three primary functions for experiment tracking:</p>"},{"location":"api/interface/#start_logging","title":"start_logging","text":"<p>Start logging metrics and metadata for your ML experiment.</p> <p>Args:     exp_name: Name of the experiment. If not provided, uses TRACELET_EXPERIMENT_NAME env var     project: Project name. If not provided, uses TRACELET_PROJECT env var     backend: Backend to use (\"mlflow\", \"wandb\", \"aim\"). If not provided, uses TRACELET_BACKEND env var     api_key: API key for the backend. If not provided, uses TRACELET_API_KEY env var     backend_url: Backend URL. If not provided, uses TRACELET_BACKEND_URL env var     config: Additional configuration to override defaults and env vars</p> <p>Returns:     Experiment: The active experiment instance</p> <p>Example:     <pre><code>import tracelet\n\n# Start logging with minimal config\ntracelet.start_logging(\"my_experiment\")\n\n# Or with more configuration\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"wandb\",\n    api_key=\"...\",\n)\n</code></pre></p> Source code in <code>tracelet/interface.py</code> <pre><code>def start_logging(\n    exp_name: Optional[str] = None,\n    project: Optional[str] = None,\n    backend: Optional[str] = None,\n    api_key: Optional[str] = None,\n    backend_url: Optional[str] = None,\n    config: Optional[dict[str, Any]] = None,\n) -&gt; Experiment:\n    \"\"\"Start logging metrics and metadata for your ML experiment.\n\n    Args:\n        exp_name: Name of the experiment. If not provided, uses TRACELET_EXPERIMENT_NAME env var\n        project: Project name. If not provided, uses TRACELET_PROJECT env var\n        backend: Backend to use (\"mlflow\", \"wandb\", \"aim\"). If not provided, uses TRACELET_BACKEND env var\n        api_key: API key for the backend. If not provided, uses TRACELET_API_KEY env var\n        backend_url: Backend URL. If not provided, uses TRACELET_BACKEND_URL env var\n        config: Additional configuration to override defaults and env vars\n\n    Returns:\n        Experiment: The active experiment instance\n\n    Example:\n        ```python\n        import tracelet\n\n        # Start logging with minimal config\n        tracelet.start_logging(\"my_experiment\")\n\n        # Or with more configuration\n        tracelet.start_logging(\n            exp_name=\"my_experiment\",\n            project=\"my_project\",\n            backend=\"wandb\",\n            api_key=\"...\",\n        )\n        ```\n    \"\"\"\n    global _active_experiment, _settings\n\n    # Stop any existing experiment first\n    if _active_experiment:\n        _active_experiment.stop()\n        _active_experiment = None\n\n    # Initialize settings - let TraceletSettings handle env vars\n    settings_dict = {}\n    if exp_name is not None:\n        settings_dict[\"experiment_name\"] = exp_name\n    if project is not None:\n        settings_dict[\"project_name\"] = project\n    if backend is not None:\n        settings_dict[\"backend\"] = backend\n    if api_key is not None:\n        settings_dict[\"api_key\"] = api_key\n    if backend_url is not None:\n        settings_dict[\"backend_url\"] = backend_url\n    if config:\n        settings_dict.update(config)\n\n    _settings = TraceletSettings(**settings_dict)\n\n    # Create experiment config\n    exp_config = ExperimentConfig(\n        track_metrics=True,\n        track_environment=_settings.track_env,\n        track_args=True,\n        track_stdout=True,\n        track_checkpoints=True,\n        track_system_metrics=_settings.track_system_metrics,\n        track_git=_settings.track_git,\n    )\n\n    # Create experiment\n    _active_experiment = Experiment(\n        name=_settings.experiment_name or \"default_experiment\",\n        config=exp_config,\n        backend=_settings.backend,\n        tags=[f\"project:{_settings.project_name}\"],\n    )\n\n    # Initialize frameworks based on settings\n    if _settings.track_tensorboard:\n        pytorch = PyTorchFramework(patch_tensorboard=True)\n        _active_experiment._framework = pytorch\n        pytorch.initialize(_active_experiment)\n\n    if _settings.track_lightning:\n        lightning = LightningFramework()\n        lightning.initialize(_active_experiment)\n\n    # Start tracking\n    _active_experiment.start()\n\n    return _active_experiment\n</code></pre> <p>options: show_source: true heading_level: 3</p> <p>Example Usage:</p> <pre><code>import tracelet\n\n# Basic usage with MLflow\nexperiment = tracelet.start_logging(\n    exp_name=\"image_classification\",\n    project=\"computer_vision\",\n    backend=\"mlflow\"\n)\n\n# With custom configuration\nexperiment = tracelet.start_logging(\n    exp_name=\"hyperparameter_tuning\",\n    project=\"optimization\",\n    backend=\"wandb\",\n    config={\n        \"entity\": \"my_team\",\n        \"tags\": [\"pytorch\", \"resnet\"]\n    }\n)\n\n# Multi-backend tracking\nexperiment = tracelet.start_logging(\n    exp_name=\"model_comparison\",\n    project=\"research\",\n    backend=[\"mlflow\", \"wandb\", \"clearml\"]\n)\n</code></pre>"},{"location":"api/interface/#get_active_experiment","title":"get_active_experiment","text":"<p>Get the currently active experiment</p> Source code in <code>tracelet/interface.py</code> <pre><code>def get_active_experiment() -&gt; Optional[Experiment]:\n    \"\"\"Get the currently active experiment\"\"\"\n    return _active_experiment\n</code></pre> <p>options: show_source: true heading_level: 3</p> <p>Example Usage:</p> <pre><code># Start tracking\ntracelet.start_logging(exp_name=\"my_exp\", project=\"my_project\", backend=\"mlflow\")\n\n# Get the active experiment from anywhere in your code\nexperiment = tracelet.get_active_experiment()\n\nif experiment:\n    experiment.log_metric(\"loss\", 0.1, iteration=50)\n    experiment.log_params({\"learning_rate\": 0.001})\nelse:\n    print(\"No active experiment found\")\n</code></pre>"},{"location":"api/interface/#stop_logging","title":"stop_logging","text":"<p>Stop the active experiment and cleanup</p> Source code in <code>tracelet/interface.py</code> <pre><code>def stop_logging():\n    \"\"\"Stop the active experiment and cleanup\"\"\"\n    global _active_experiment\n    if _active_experiment:\n        _active_experiment.stop()\n        _active_experiment = None\n</code></pre> <p>options: show_source: true heading_level: 3</p> <p>Example Usage:</p> <pre><code># Stop the current experiment\ntracelet.stop_logging()\n\n# Verify no active experiment\nassert tracelet.get_active_experiment() is None\n</code></pre>"},{"location":"api/interface/#integration-patterns","title":"Integration Patterns","text":""},{"location":"api/interface/#context-manager-pattern","title":"Context Manager Pattern","text":"<pre><code>import tracelet\n\n# Using with automatic cleanup\nwith tracelet.start_logging(exp_name=\"context_exp\", project=\"test\", backend=\"mlflow\") as exp:\n    exp.log_metric(\"accuracy\", 0.95)\n    exp.log_params({\"epochs\": 10})\n# Automatically calls stop_logging() when exiting context\n</code></pre>"},{"location":"api/interface/#error-handling","title":"Error Handling","text":"<pre><code>import tracelet\n\ntry:\n    tracelet.start_logging(\n        exp_name=\"my_experiment\",\n        project=\"my_project\",\n        backend=\"invalid_backend\"\n    )\nexcept ValueError as e:\n    print(f\"Backend error: {e}\")\n    # Fall back to default backend\n    tracelet.start_logging(\n        exp_name=\"my_experiment\",\n        project=\"my_project\",\n        backend=\"mlflow\"  # Default fallback\n    )\n</code></pre>"},{"location":"api/interface/#configuration-based-setup","title":"Configuration-Based Setup","text":"<pre><code>from tracelet.settings import TraceletSettings\nimport tracelet\n\n# Load configuration\nsettings = TraceletSettings(\n    project=\"ml_pipeline\",\n    backend=[\"mlflow\", \"wandb\"],\n    track_system=True,\n    metrics_interval=10.0\n)\n\n# Use settings\nexperiment = tracelet.start_logging(\n    exp_name=\"pipeline_run_001\",\n    settings=settings\n)\n</code></pre>"},{"location":"api/settings/","title":"Settings and Configuration","text":"<p>               Bases: <code>BaseSettings</code></p> <p>Global settings for Tracelet, configurable via environment variables</p> <p>options: show_source: true show_bases: true heading_level: 2</p>"},{"location":"api/settings/#tracelet.settings.TraceletSettings.project_name","title":"<code>project_name: str</code>  <code>property</code>","text":"<p>Alias for project field for backwards compatibility</p>"},{"location":"api/settings/#tracelet.settings.TraceletSettings.track_system_metrics","title":"<code>track_system_metrics: bool</code>  <code>property</code>","text":"<p>Alias for track_system field for backwards compatibility</p>"},{"location":"api/settings/#tracelet.settings.TraceletSettings.settings_customise_sources","title":"<code>settings_customise_sources(settings_cls, init_settings, env_settings, dotenv_settings, file_secret_settings)</code>  <code>classmethod</code>","text":"<p>Customize settings sources to handle backend field properly</p> Source code in <code>tracelet/settings.py</code> <pre><code>@classmethod\ndef settings_customise_sources(\n    cls,\n    settings_cls,\n    init_settings,\n    env_settings,\n    dotenv_settings,\n    file_secret_settings,\n):\n    \"\"\"Customize settings sources to handle backend field properly\"\"\"\n    return (\n        init_settings,\n        CustomEnvSettings(settings_cls),\n        dotenv_settings,\n        file_secret_settings,\n    )\n</code></pre>"},{"location":"api/settings/#tracelet.settings.TraceletSettings.validate_backend","title":"<code>validate_backend(v)</code>  <code>classmethod</code>","text":"<p>Convert string backend to list if needed</p> Source code in <code>tracelet/settings.py</code> <pre><code>@field_validator(\"backend\", mode=\"before\")\n@classmethod\ndef validate_backend(cls, v: Any) -&gt; list:\n    \"\"\"Convert string backend to list if needed\"\"\"\n    if isinstance(v, str):\n        # Handle both comma-separated and single values\n        backends = [b.strip() for b in v.split(\",\") if b.strip()]\n        return backends if backends else [\"mlflow\"]\n    elif isinstance(v, list):\n        return v\n    else:\n        return [\"mlflow\"]\n</code></pre>"},{"location":"api/settings/#configuration-fields","title":"Configuration Fields","text":"<p>The <code>TraceletSettings</code> class uses Pydantic for configuration management with validation and environment variable support.</p>"},{"location":"api/settings/#core-settings","title":"Core Settings","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Basic configuration\nsettings = TraceletSettings(\n    project=\"my_ml_project\",\n    experiment_name=\"baseline_model\",\n    backend=[\"mlflow\"]  # Single backend as list\n)\n\n# Multi-backend configuration\nsettings = TraceletSettings(\n    project=\"comparison_study\",\n    backend=[\"mlflow\", \"wandb\", \"clearml\"]  # Multiple backends\n)\n</code></pre>"},{"location":"api/settings/#backend-configuration","title":"Backend Configuration","text":"<pre><code># MLflow-specific settings\nmlflow_settings = TraceletSettings(\n    project=\"mlflow_project\",\n    backend=[\"mlflow\"],\n    mlflow_tracking_uri=\"http://localhost:5000\",\n    mlflow_experiment_name=\"Deep Learning Experiments\"\n)\n\n# Weights &amp; Biases settings\nwandb_settings = TraceletSettings(\n    project=\"wandb_project\",\n    backend=[\"wandb\"],\n    wandb_project=\"ml-experiments\",\n    wandb_entity=\"my_team\",\n    wandb_api_key=\"your_api_key_here\"  # Better to use env var\n)\n\n# ClearML settings\nclearml_settings = TraceletSettings(\n    project=\"clearml_project\",\n    backend=[\"clearml\"],\n    clearml_project_name=\"Research Experiments\",\n    clearml_task_name=\"Model Training\"\n)\n</code></pre>"},{"location":"api/settings/#system-metrics-configuration","title":"System Metrics Configuration","text":"<pre><code># Enable system monitoring\nsettings = TraceletSettings(\n    project=\"monitored_training\",\n    backend=[\"mlflow\"],\n    track_system=True,              # Enable system metrics\n    metrics_interval=5.0,           # Collect every 5 seconds\n    track_gpu=True,                 # Include GPU metrics\n    track_disk=True,                # Include disk I/O\n    track_network=True              # Include network I/O\n)\n</code></pre>"},{"location":"api/settings/#environment-variables","title":"Environment Variables","text":"<p>All settings can be configured via environment variables using the <code>TRACELET_</code> prefix:</p>"},{"location":"api/settings/#basic-environment-setup","title":"Basic Environment Setup","text":"<pre><code># Core settings\nexport TRACELET_PROJECT=\"production_models\"\nexport TRACELET_EXPERIMENT_NAME=\"model_v2_training\"\nexport TRACELET_BACKEND=\"mlflow,wandb\"  # Comma-separated for multiple\n\n# System monitoring\nexport TRACELET_TRACK_SYSTEM=\"true\"\nexport TRACELET_METRICS_INTERVAL=\"10.0\"\n</code></pre>"},{"location":"api/settings/#backend-specific-environment-variables","title":"Backend-Specific Environment Variables","text":"<pre><code># MLflow\nexport TRACELET_MLFLOW_TRACKING_URI=\"http://mlflow-server:5000\"\nexport TRACELET_MLFLOW_EXPERIMENT_NAME=\"Production Experiments\"\n\n# Weights &amp; Biases\nexport TRACELET_WANDB_PROJECT=\"production-ml\"\nexport TRACELET_WANDB_ENTITY=\"company-ml-team\"\nexport WANDB_API_KEY=\"your_wandb_api_key\"\n\n# ClearML\nexport TRACELET_CLEARML_PROJECT_NAME=\"Production Models\"\nexport TRACELET_CLEARML_TASK_NAME=\"Training Session\"\nexport CLEARML_API_ACCESS_KEY=\"your_access_key\"\nexport CLEARML_API_SECRET_KEY=\"your_secret_key\"\n\n# AIM\nexport TRACELET_AIM_REPO_PATH=\"./aim_logs\"\nexport TRACELET_AIM_EXPERIMENT_NAME=\"Local Experiments\"\n</code></pre>"},{"location":"api/settings/#configuration-files","title":"Configuration Files","text":""},{"location":"api/settings/#yaml-configuration","title":"YAML Configuration","text":"<p>Create a <code>tracelet.yaml</code> file in your project root:</p> <pre><code># tracelet.yaml\nproject: \"ml_pipeline\"\nexperiment_name: \"automated_training\"\nbackend:\n  - \"mlflow\"\n  - \"wandb\"\n\n# System monitoring\ntrack_system: true\nmetrics_interval: 15.0\ntrack_gpu: true\n\n# Backend configurations\nmlflow_tracking_uri: \"http://localhost:5000\"\nmlflow_experiment_name: \"Pipeline Experiments\"\n\nwandb_project: \"ml-pipeline\"\nwandb_entity: \"research-team\"\n</code></pre>"},{"location":"api/settings/#loading-configuration","title":"Loading Configuration","text":"<pre><code>from tracelet.settings import TraceletSettings\nimport tracelet\n\n# Load from file (automatically detected)\nsettings = TraceletSettings()  # Loads tracelet.yaml if present\n\n# Explicit file loading\nsettings = TraceletSettings(_env_file=\".env.production\")\n\n# Override specific fields\nsettings = TraceletSettings(\n    backend=[\"mlflow\"],  # Override YAML setting\n    track_system=False   # Override YAML setting\n)\n\n# Use with tracelet\ntracelet.start_logging(\n    exp_name=\"config_example\",\n    settings=settings\n)\n</code></pre>"},{"location":"api/settings/#advanced-configuration-patterns","title":"Advanced Configuration Patterns","text":""},{"location":"api/settings/#environment-specific-configurations","title":"Environment-Specific Configurations","text":"<pre><code>import os\nfrom tracelet.settings import TraceletSettings\n\n# Different configs for different environments\nif os.getenv(\"ENVIRONMENT\") == \"production\":\n    settings = TraceletSettings(\n        project=\"production_models\",\n        backend=[\"mlflow\", \"clearml\"],  # Multiple backends for production\n        track_system=True,\n        mlflow_tracking_uri=\"http://prod-mlflow:5000\"\n    )\nelif os.getenv(\"ENVIRONMENT\") == \"development\":\n    settings = TraceletSettings(\n        project=\"dev_experiments\",\n        backend=[\"mlflow\"],  # Single backend for dev\n        track_system=False,  # No system tracking in dev\n        mlflow_tracking_uri=\"http://localhost:5000\"\n    )\nelse:\n    settings = TraceletSettings(project=\"local_tests\", backend=[\"mlflow\"])\n</code></pre>"},{"location":"api/settings/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>from tracelet.settings import TraceletSettings\n\ndef create_experiment_settings(experiment_type: str) -&gt; TraceletSettings:\n    \"\"\"Create settings based on experiment type.\"\"\"\n\n    base_settings = {\n        \"project\": \"adaptive_experiments\",\n        \"track_system\": True\n    }\n\n    if experiment_type == \"hyperparameter_search\":\n        return TraceletSettings(\n            **base_settings,\n            backend=[\"wandb\"],  # W&amp;B for hyperparameter tracking\n            wandb_project=\"hyperparameter-optimization\"\n        )\n    elif experiment_type == \"model_comparison\":\n        return TraceletSettings(\n            **base_settings,\n            backend=[\"mlflow\", \"clearml\"],  # Multiple backends for comparison\n            metrics_interval=5.0  # More frequent monitoring\n        )\n    else:\n        return TraceletSettings(**base_settings, backend=[\"mlflow\"])\n\n# Usage\nsettings = create_experiment_settings(\"hyperparameter_search\")\ntracelet.start_logging(exp_name=\"hp_search_001\", settings=settings)\n</code></pre>"},{"location":"api/settings/#validation-and-error-handling","title":"Validation and Error Handling","text":"<pre><code>from tracelet.settings import TraceletSettings\nfrom pydantic import ValidationError\n\ntry:\n    # This will raise ValidationError for invalid backend\n    settings = TraceletSettings(\n        project=\"test\",\n        backend=[\"invalid_backend\"]\n    )\nexcept ValidationError as e:\n    print(f\"Configuration error: {e}\")\n    # Use default settings\n    settings = TraceletSettings(project=\"test\")\n\n# Check available backends\nvalid_backends = TraceletSettings.__annotations__[\"backend\"].__args__[0].__args__\nprint(f\"Valid backends: {valid_backends}\")\n</code></pre>"},{"location":"api/settings/#best-practices","title":"Best Practices","text":""},{"location":"api/settings/#security","title":"Security","text":"<pre><code># \u274c Don't store API keys in code\nsettings = TraceletSettings(\n    wandb_api_key=\"sk-1234567890\"  # Bad!\n)\n\n# \u2705 Use environment variables\nimport os\nsettings = TraceletSettings(\n    wandb_api_key=os.getenv(\"WANDB_API_KEY\")  # Good!\n)\n\n# \u2705 Better: Let Pydantic handle it automatically\nsettings = TraceletSettings()  # Reads TRACELET_WANDB_API_KEY from env\n</code></pre>"},{"location":"api/settings/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<pre><code># Configuration priority (highest to lowest):\n# 1. Direct parameter override\n# 2. Environment variables\n# 3. Configuration file\n# 4. Default values\n\nsettings = TraceletSettings(\n    project=\"direct_override\",  # 1. Highest priority\n    # TRACELET_BACKEND env var   # 2. Middle priority\n    # tracelet.yaml file         # 3. Lower priority\n    # Default: [\"mlflow\"]        # 4. Lowest priority\n)\n</code></pre>"},{"location":"api/backends/mlflow/","title":"MLflow Backend","text":"<p>               Bases: <code>BackendInterface</code></p> <p>MLflow backend integration</p> Source code in <code>tracelet/backends/mlflow.py</code> <pre><code>def __init__(self, tracking_uri: Optional[str] = None, experiment_name: Optional[str] = None):\n    self.tracking_uri = tracking_uri\n    self.experiment_name = experiment_name\n    self._run = None\n</code></pre> <p>options: show_source: true show_bases: true merge_init_into_class: true heading_level: 2</p>"},{"location":"api/backends/mlflow/#overview","title":"Overview","text":"<p>The MLflow backend provides integration with MLflow tracking server for experiment logging and management.</p>"},{"location":"api/backends/mlflow/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\n\n# Basic MLflow usage\ntracelet.start_logging(\n    exp_name=\"mlflow_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Custom MLflow configuration\ntracelet.start_logging(\n    exp_name=\"custom_mlflow\",\n    project=\"my_project\",\n    backend=\"mlflow\",\n    config={\n        \"tracking_uri\": \"http://mlflow-server:5000\",\n        \"experiment_name\": \"Deep Learning Experiments\"\n    }\n)\n</code></pre>"},{"location":"api/backends/mlflow/#configuration-options","title":"Configuration Options","text":""},{"location":"api/backends/mlflow/#via-settings","title":"Via Settings","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Configure via settings\nsettings = TraceletSettings(\n    project=\"mlflow_project\",\n    backend=[\"mlflow\"],\n    mlflow_tracking_uri=\"http://localhost:5000\",\n    mlflow_experiment_name=\"ML Experiments\"\n)\n\ntracelet.start_logging(exp_name=\"configured_exp\", settings=settings)\n</code></pre>"},{"location":"api/backends/mlflow/#via-environment-variables","title":"Via Environment Variables","text":"<pre><code># MLflow-specific environment variables\nexport MLFLOW_TRACKING_URI=\"http://mlflow-server:5000\"\nexport MLFLOW_EXPERIMENT_NAME=\"Production Experiments\"\n\n# Tracelet MLflow settings\nexport TRACELET_MLFLOW_TRACKING_URI=\"http://localhost:5000\"\nexport TRACELET_MLFLOW_EXPERIMENT_NAME=\"Development Experiments\"\n</code></pre>"},{"location":"api/backends/mlflow/#via-configuration-object","title":"Via Configuration Object","text":"<pre><code># Direct configuration\nmlflow_config = {\n    \"tracking_uri\": \"http://localhost:5000\",\n    \"experiment_name\": \"My Experiments\",\n    \"run_name\": \"baseline_run_001\",\n    \"tags\": {\n        \"team\": \"ml_research\",\n        \"version\": \"v1.0\",\n        \"model_type\": \"transformer\"\n    }\n}\n\ntracelet.start_logging(\n    exp_name=\"tagged_experiment\",\n    project=\"research\",\n    backend=\"mlflow\",\n    config=mlflow_config\n)\n</code></pre>"},{"location":"api/backends/mlflow/#mlflow-server-setup","title":"MLflow Server Setup","text":""},{"location":"api/backends/mlflow/#local-mlflow-server","title":"Local MLflow Server","text":"<pre><code># Start local MLflow server\nmlflow server --host 0.0.0.0 --port 5000\n\n# With backend store (SQLite)\nmlflow server \\\n    --backend-store-uri sqlite:///mlflow.db \\\n    --default-artifact-root ./mlflow-artifacts \\\n    --host 0.0.0.0 \\\n    --port 5000\n</code></pre>"},{"location":"api/backends/mlflow/#remote-mlflow-server","title":"Remote MLflow Server","text":"<pre><code># Connect to remote MLflow server\ntracelet.start_logging(\n    exp_name=\"remote_experiment\",\n    project=\"distributed_training\",\n    backend=\"mlflow\",\n    config={\n        \"tracking_uri\": \"http://mlflow.company.com:5000\",\n        \"experiment_name\": \"Production Models\"\n    }\n)\n</code></pre>"},{"location":"api/backends/mlflow/#docker-mlflow-setup","title":"Docker MLflow Setup","text":"<pre><code># docker-compose.yml\nversion: \"3.8\"\nservices:\n  mlflow:\n    image: mlflow/mlflow\n    ports:\n      - \"5000:5000\"\n    command: &gt;\n      mlflow server\n      --backend-store-uri sqlite:///mlflow.db\n      --default-artifact-root /mlflow/artifacts\n      --host 0.0.0.0\n      --port 5000\n    volumes:\n      - mlflow_data:/mlflow\n\nvolumes:\n  mlflow_data:\n</code></pre>"},{"location":"api/backends/mlflow/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/backends/mlflow/#experiment-organization","title":"Experiment Organization","text":"<pre><code># Organize experiments by project/team\ntracelet.start_logging(\n    exp_name=\"resnet_baseline\",\n    project=\"computer_vision\",\n    backend=\"mlflow\",\n    config={\n        \"experiment_name\": \"CV Team - Image Classification\",\n        \"tags\": {\n            \"project\": \"computer_vision\",\n            \"team\": \"cv_team\",\n            \"model_family\": \"resnet\",\n            \"dataset\": \"imagenet\"\n        }\n    }\n)\n</code></pre>"},{"location":"api/backends/mlflow/#artifact-management","title":"Artifact Management","text":"<pre><code>import tracelet\nimport torch\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"artifact_demo\",\n    project=\"model_artifacts\",\n    backend=\"mlflow\"\n)\n\n# Log model artifacts\ntorch.save(model.state_dict(), \"model.pth\")\nexp.log_artifact(\"model.pth\", \"models/trained_model.pth\")\n\n# Log configuration files\nexp.log_artifact(\"config.yaml\", \"configs/training_config.yaml\")\n\n# Log processed datasets\nexp.log_artifact(\"train_data.csv\", \"datasets/processed_train.csv\")\n</code></pre>"},{"location":"api/backends/mlflow/#model-registry-integration","title":"Model Registry Integration","text":"<pre><code>import mlflow\nimport tracelet\n\n# Register model after training\nexp = tracelet.start_logging(\n    exp_name=\"model_registry_demo\",\n    project=\"production_models\",\n    backend=\"mlflow\"\n)\n\n# Train model and log metrics\n# ... training code ...\n\n# Register model in MLflow Model Registry\nmlflow.pytorch.log_model(\n    pytorch_model=model,\n    artifact_path=\"model\",\n    registered_model_name=\"ProductionImageClassifier\"\n)\n</code></pre>"},{"location":"api/backends/mlflow/#auto-logging-integration","title":"Auto-logging Integration","text":"<pre><code>import mlflow\nimport mlflow.pytorch\nimport tracelet\n\n# Enable MLflow auto-logging\nmlflow.pytorch.autolog()\n\n# Start Tracelet (will work alongside auto-logging)\ntracelet.start_logging(\n    exp_name=\"autolog_experiment\",\n    project=\"auto_tracking\",\n    backend=\"mlflow\"\n)\n\n# Training code - metrics logged automatically by both systems\nmodel = torch.nn.Linear(10, 1)\noptimizer = torch.optim.Adam(model.parameters())\n# ... training loop ...\n</code></pre>"},{"location":"api/backends/mlflow/#error-handling","title":"Error Handling","text":""},{"location":"api/backends/mlflow/#connection-issues","title":"Connection Issues","text":"<pre><code>import tracelet\n\ntry:\n    tracelet.start_logging(\n        exp_name=\"connection_test\",\n        project=\"reliability\",\n        backend=\"mlflow\",\n        config={\"tracking_uri\": \"http://unreachable-server:5000\"}\n    )\nexcept Exception as e:\n    print(f\"MLflow connection failed: {e}\")\n\n    # Fallback to local tracking\n    tracelet.start_logging(\n        exp_name=\"connection_test_local\",\n        project=\"reliability\",\n        backend=\"mlflow\"  # Uses default local tracking\n    )\n</code></pre>"},{"location":"api/backends/mlflow/#experiment-creation","title":"Experiment Creation","text":"<pre><code># Handle experiment name conflicts\nimport mlflow\nimport tracelet\n\nexperiment_name = \"Shared Experiment\"\n\ntry:\n    tracelet.start_logging(\n        exp_name=\"shared_run\",\n        project=\"collaborative\",\n        backend=\"mlflow\",\n        config={\"experiment_name\": experiment_name}\n    )\nexcept mlflow.exceptions.MlflowException as e:\n    if \"already exists\" in str(e):\n        print(f\"Experiment '{experiment_name}' already exists, joining it\")\n        # MLflow will automatically use existing experiment\n    else:\n        raise e\n</code></pre>"},{"location":"api/backends/mlflow/#integration-examples","title":"Integration Examples","text":""},{"location":"api/backends/mlflow/#pytorch-training-loop","title":"PyTorch Training Loop","text":"<pre><code>import torch\nimport torch.nn as nn\nimport tracelet\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"pytorch_training\",\n    project=\"deep_learning\",\n    backend=\"mlflow\"\n)\n\n# Log hyperparameters\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"epochs\": 100,\n    \"model\": \"resnet18\"\n})\n\n# Training loop\nmodel = torch.nn.Linear(784, 10)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(100):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    # Log epoch metrics\n    avg_loss = total_loss / len(dataloader)\n    exp.log_metric(\"train_loss\", avg_loss, iteration=epoch)\n\n    # Save checkpoint every 10 epochs\n    if epoch % 10 == 0:\n        torch.save(model.state_dict(), f\"checkpoint_epoch_{epoch}.pth\")\n        exp.log_artifact(\n            f\"checkpoint_epoch_{epoch}.pth\",\n            f\"checkpoints/epoch_{epoch}.pth\"\n        )\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/backends/mlflow/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<pre><code>import optuna\nimport tracelet\n\ndef objective(trial):\n    # Suggest hyperparameters\n    lr = trial.suggest_loguniform(\"lr\", 1e-5, 1e-1)\n    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n\n    # Start experiment for this trial\n    exp = tracelet.start_logging(\n        exp_name=f\"optuna_trial_{trial.number}\",\n        project=\"hyperparameter_optimization\",\n        backend=\"mlflow\",\n        config={\n            \"tags\": {\n                \"optimization\": \"optuna\",\n                \"trial_number\": str(trial.number)\n            }\n        }\n    )\n\n    # Log trial parameters\n    exp.log_params({\n        \"lr\": lr,\n        \"batch_size\": batch_size,\n        \"trial_number\": trial.number\n    })\n\n    # Train model with suggested parameters\n    accuracy = train_model(lr=lr, batch_size=batch_size, experiment=exp)\n\n    # Log final result\n    exp.log_metric(\"final_accuracy\", accuracy)\n    tracelet.stop_logging()\n\n    return accuracy\n\n# Run optimization\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n</code></pre>"},{"location":"api/backends/mlflow/#best-practices","title":"Best Practices","text":""},{"location":"api/backends/mlflow/#experiment-naming","title":"Experiment Naming","text":"<pre><code># Use descriptive experiment names\ntracelet.start_logging(\n    exp_name=f\"resnet50_imagenet_{datetime.now().strftime('%Y%m%d_%H%M')}\",\n    project=\"computer_vision\",\n    backend=\"mlflow\",\n    config={\n        \"experiment_name\": \"CV Team - ImageNet Classification\",\n        \"tags\": {\n            \"architecture\": \"resnet50\",\n            \"dataset\": \"imagenet\",\n            \"date\": datetime.now().isoformat()\n        }\n    }\n)\n</code></pre>"},{"location":"api/backends/mlflow/#metric-organization","title":"Metric Organization","text":"<pre><code># Use hierarchical metric names\nexp.log_metric(\"train/loss\", train_loss, iteration=epoch)\nexp.log_metric(\"train/accuracy\", train_acc, iteration=epoch)\nexp.log_metric(\"val/loss\", val_loss, iteration=epoch)\nexp.log_metric(\"val/accuracy\", val_acc, iteration=epoch)\nexp.log_metric(\"lr_schedule/learning_rate\", current_lr, iteration=epoch)\n</code></pre>"},{"location":"api/backends/mlflow/#resource-management","title":"Resource Management","text":"<pre><code># Ensure proper cleanup\nimport atexit\nimport tracelet\n\nexp = tracelet.start_logging(\n    exp_name=\"robust_experiment\",\n    project=\"production\",\n    backend=\"mlflow\"\n)\n\n# Register cleanup function\natexit.register(tracelet.stop_logging)\n\ntry:\n    # Training code here\n    pass\nfinally:\n    # Always stop logging\n    tracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/git/","title":"Git Collector","text":"<p>               Bases: <code>CollectorInterface</code></p> <p>Collector for Git repository information</p> Source code in <code>tracelet/collectors/git.py</code> <pre><code>def __init__(self, repo_path: Optional[str] = None):\n    self.repo_path = repo_path or os.getcwd()\n    self._repo = None\n</code></pre> <p>options: show_source: true show_bases: true merge_init_into_class: true heading_level: 2</p>"},{"location":"api/collectors/git/#overview","title":"Overview","text":"<p>The Git Collector automatically captures git repository information for experiment reproducibility and version tracking.</p>"},{"location":"api/collectors/git/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\n\n# Git information is collected automatically when starting experiments\nexp = tracelet.start_logging(\n    exp_name=\"git_tracking_demo\",\n    project=\"version_control\",\n    backend=\"mlflow\"\n)\n\n# Git info is collected and logged automatically\n# No manual intervention needed\n</code></pre>"},{"location":"api/collectors/git/#manual-git-collection","title":"Manual Git Collection","text":"<pre><code>from tracelet.collectors.git import GitCollector\n\n# Create git collector\ngit_collector = GitCollector()\n\n# Initialize and collect git information\ngit_collector.initialize()\ngit_info = git_collector.collect()\n\nprint(\"Git Information:\")\nfor key, value in git_info.items():\n    print(f\"  {key}: {value}\")\n</code></pre>"},{"location":"api/collectors/git/#collected-information","title":"Collected Information","text":"<p>The Git Collector captures the following repository information:</p>"},{"location":"api/collectors/git/#repository-state","title":"Repository State","text":"<ul> <li>commit_hash: Current commit SHA</li> <li>branch: Current branch name</li> <li>remote_url: Remote repository URL</li> <li>is_dirty: Whether working directory has uncommitted changes</li> </ul>"},{"location":"api/collectors/git/#commit-details","title":"Commit Details","text":"<ul> <li>commit_message: Latest commit message</li> <li>commit_author: Commit author name and email</li> <li>commit_date: Commit timestamp</li> </ul>"},{"location":"api/collectors/git/#working-directory-status","title":"Working Directory Status","text":"<ul> <li>uncommitted_files: List of modified files (if any)</li> <li>untracked_files: List of untracked files (if any)</li> </ul>"},{"location":"api/collectors/git/#configuration-options","title":"Configuration Options","text":""},{"location":"api/collectors/git/#custom-repository-path","title":"Custom Repository Path","text":"<pre><code>from tracelet.collectors.git import GitCollector\n\n# Specify custom repository path\ngit_collector = GitCollector(repo_path=\"/path/to/custom/repo\")\ngit_collector.initialize()\ngit_info = git_collector.collect()\n</code></pre>"},{"location":"api/collectors/git/#integration-with-settings","title":"Integration with Settings","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Configure git tracking behavior\nsettings = TraceletSettings(\n    project=\"git_configured\",\n    backend=[\"mlflow\"],\n    # Git-specific settings would be added here if supported\n)\n\ntracelet.start_logging(exp_name=\"git_exp\", settings=settings)\n</code></pre>"},{"location":"api/collectors/git/#practical-examples","title":"Practical Examples","text":""},{"location":"api/collectors/git/#experiment-reproducibility","title":"Experiment Reproducibility","text":"<pre><code>import tracelet\nimport json\n\n# Start experiment with automatic git tracking\nexp = tracelet.start_logging(\n    exp_name=\"reproducible_experiment\",\n    project=\"research\",\n    backend=\"mlflow\"\n)\n\n# Get git information for logging\nfrom tracelet.collectors.git import GitCollector\ngit_collector = GitCollector()\ngit_collector.initialize()\ngit_info = git_collector.collect()\n\n# Log git info as parameters for reproducibility\nexp.log_params({\n    \"git_commit\": git_info.get(\"commit_hash\"),\n    \"git_branch\": git_info.get(\"branch\"),\n    \"git_is_dirty\": git_info.get(\"is_dirty\", False)\n})\n\n# Save detailed git info as artifact\nwith open(\"git_info.json\", \"w\") as f:\n    json.dump(git_info, f, indent=2, default=str)\n\nexp.log_artifact(\"git_info.json\", \"metadata/git_info.json\")\n\n# Your training code here...\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/git/#pre-commit-validation","title":"Pre-commit Validation","text":"<pre><code>from tracelet.collectors.git import GitCollector\nimport sys\n\ndef validate_git_state():\n    \"\"\"Validate git state before starting experiment.\"\"\"\n    git_collector = GitCollector()\n    git_collector.initialize()\n    git_info = git_collector.collect()\n\n    # Check for uncommitted changes\n    if git_info.get(\"is_dirty\", False):\n        print(\"Warning: Working directory has uncommitted changes!\")\n        print(f\"Modified files: {git_info.get('uncommitted_files', [])}\")\n\n        response = input(\"Continue anyway? (y/N): \")\n        if response.lower() != 'y':\n            print(\"Experiment cancelled.\")\n            sys.exit(1)\n\n    # Check if on main/master branch\n    current_branch = git_info.get(\"branch\", \"\")\n    if current_branch in [\"main\", \"master\"]:\n        print(f\"Warning: Running experiment on {current_branch} branch!\")\n        response = input(\"Are you sure? (y/N): \")\n        if response.lower() != 'y':\n            print(\"Experiment cancelled.\")\n            sys.exit(1)\n\n    return git_info\n\n# Usage\ngit_info = validate_git_state()\nprint(f\"Starting experiment on branch: {git_info['branch']}\")\nprint(f\"Commit: {git_info['commit_hash'][:8]}\")\n\ntracelet.start_logging(\n    exp_name=\"validated_experiment\",\n    project=\"safe_experiments\",\n    backend=\"mlflow\"\n)\n</code></pre>"},{"location":"api/collectors/git/#multi-repository-projects","title":"Multi-Repository Projects","text":"<pre><code>from tracelet.collectors.git import GitCollector\nimport os\n\ndef collect_multi_repo_info(repo_paths):\n    \"\"\"Collect git info from multiple repositories.\"\"\"\n    multi_repo_info = {}\n\n    for name, path in repo_paths.items():\n        if os.path.exists(path):\n            git_collector = GitCollector(repo_path=path)\n            git_collector.initialize()\n            repo_info = git_collector.collect()\n            multi_repo_info[name] = repo_info\n        else:\n            multi_repo_info[name] = {\"error\": \"Repository not found\"}\n\n    return multi_repo_info\n\n# Example usage for projects with multiple repositories\nrepo_paths = {\n    \"main_code\": \".\",\n    \"data_processing\": \"../data-pipeline\",\n    \"model_configs\": \"../model-configs\"\n}\n\nmulti_git_info = collect_multi_repo_info(repo_paths)\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"multi_repo_experiment\",\n    project=\"complex_project\",\n    backend=\"mlflow\"\n)\n\n# Log git info for each repository\nfor repo_name, git_info in multi_git_info.items():\n    if \"error\" not in git_info:\n        exp.log_params({\n            f\"{repo_name}_commit\": git_info.get(\"commit_hash\"),\n            f\"{repo_name}_branch\": git_info.get(\"branch\"),\n            f\"{repo_name}_is_dirty\": git_info.get(\"is_dirty\", False)\n        })\n</code></pre>"},{"location":"api/collectors/git/#error-handling","title":"Error Handling","text":""},{"location":"api/collectors/git/#repository-detection","title":"Repository Detection","text":"<pre><code>from tracelet.collectors.git import GitCollector\nimport logging\n\ndef safe_git_collection(repo_path=None):\n    \"\"\"Safely collect git information with error handling.\"\"\"\n    try:\n        git_collector = GitCollector(repo_path=repo_path)\n        git_collector.initialize()\n        return git_collector.collect()\n\n    except Exception as e:\n        logging.warning(f\"Failed to collect git information: {e}\")\n        return {\n            \"error\": str(e),\n            \"git_available\": False,\n            \"fallback_info\": {\n                \"timestamp\": str(datetime.now()),\n                \"hostname\": os.uname().nodename if hasattr(os, 'uname') else \"unknown\"\n            }\n        }\n\n# Usage\ngit_info = safe_git_collection()\n\nif git_info.get(\"git_available\", True):\n    print(f\"Git commit: {git_info.get('commit_hash')}\")\nelse:\n    print(f\"Git not available: {git_info.get('error')}\")\n</code></pre>"},{"location":"api/collectors/git/#non-git-directories","title":"Non-Git Directories","text":"<pre><code>import os\nfrom tracelet.collectors.git import GitCollector\n\ndef check_git_repository(path=\".\"):\n    \"\"\"Check if directory is a git repository.\"\"\"\n    git_dir = os.path.join(path, \".git\")\n    return os.path.exists(git_dir)\n\n# Check before initializing git collector\nif check_git_repository():\n    git_collector = GitCollector()\n    git_collector.initialize()\n    git_info = git_collector.collect()\n    print(\"Git repository detected\")\nelse:\n    print(\"No git repository found\")\n    git_info = {\"git_available\": False}\n</code></pre>"},{"location":"api/collectors/git/#integration-patterns","title":"Integration Patterns","text":""},{"location":"api/collectors/git/#automatic-git-tagging","title":"Automatic Git Tagging","text":"<pre><code>import subprocess\nfrom tracelet.collectors.git import GitCollector\nimport tracelet\n\ndef create_experiment_tag(exp_name):\n    \"\"\"Create git tag for experiment.\"\"\"\n    tag_name = f\"experiment-{exp_name}-{int(time.time())}\"\n\n    try:\n        subprocess.run([\"git\", \"tag\", tag_name], check=True)\n        return tag_name\n    except subprocess.CalledProcessError as e:\n        print(f\"Failed to create git tag: {e}\")\n        return None\n\n# Usage\nexp_name = \"important_experiment\"\n\n# Create git tag before starting experiment\ntag_name = create_experiment_tag(exp_name)\n\nexp = tracelet.start_logging(\n    exp_name=exp_name,\n    project=\"tagged_experiments\",\n    backend=\"mlflow\"\n)\n\nif tag_name:\n    exp.log_params({\"git_tag\": tag_name})\n</code></pre>"},{"location":"api/collectors/git/#cicd-integration","title":"CI/CD Integration","text":"<pre><code>import os\nfrom tracelet.collectors.git import GitCollector\n\ndef get_ci_git_info():\n    \"\"\"Get git information in CI/CD environment.\"\"\"\n    git_info = {}\n\n    # GitHub Actions\n    if \"GITHUB_ACTIONS\" in os.environ:\n        git_info.update({\n            \"ci_provider\": \"github_actions\",\n            \"commit_hash\": os.environ.get(\"GITHUB_SHA\"),\n            \"branch\": os.environ.get(\"GITHUB_REF_NAME\"),\n            \"repository\": os.environ.get(\"GITHUB_REPOSITORY\"),\n            \"actor\": os.environ.get(\"GITHUB_ACTOR\")\n        })\n\n    # GitLab CI\n    elif \"GITLAB_CI\" in os.environ:\n        git_info.update({\n            \"ci_provider\": \"gitlab_ci\",\n            \"commit_hash\": os.environ.get(\"CI_COMMIT_SHA\"),\n            \"branch\": os.environ.get(\"CI_COMMIT_REF_NAME\"),\n            \"repository\": os.environ.get(\"CI_PROJECT_PATH\"),\n            \"pipeline_id\": os.environ.get(\"CI_PIPELINE_ID\")\n        })\n\n    # Fall back to git collector\n    else:\n        git_collector = GitCollector()\n        git_collector.initialize()\n        git_info = git_collector.collect()\n        git_info[\"ci_provider\"] = \"local\"\n\n    return git_info\n\n# Usage in CI/CD pipeline\ngit_info = get_ci_git_info()\n\nexp = tracelet.start_logging(\n    exp_name=\"ci_experiment\",\n    project=\"automated_training\",\n    backend=\"mlflow\"\n)\n\nexp.log_params({\n    \"ci_provider\": git_info.get(\"ci_provider\"),\n    \"commit_hash\": git_info.get(\"commit_hash\"),\n    \"branch\": git_info.get(\"branch\")\n})\n</code></pre>"},{"location":"api/collectors/git/#best-practices","title":"Best Practices","text":""},{"location":"api/collectors/git/#repository-hygiene","title":"Repository Hygiene","text":"<pre><code>from tracelet.collectors.git import GitCollector\n\ndef validate_repository_state():\n    \"\"\"Validate repository state before experiment.\"\"\"\n    git_collector = GitCollector()\n    git_collector.initialize()\n    git_info = git_collector.collect()\n\n    issues = []\n\n    # Check for uncommitted changes\n    if git_info.get(\"is_dirty\", False):\n        issues.append(\"Uncommitted changes detected\")\n\n    # Check for untracked files\n    untracked = git_info.get(\"untracked_files\", [])\n    if untracked:\n        issues.append(f\"Untracked files: {len(untracked)}\")\n\n    # Check branch name\n    branch = git_info.get(\"branch\", \"\")\n    if not branch or branch == \"HEAD\":\n        issues.append(\"Detached HEAD state\")\n\n    return issues, git_info\n\n# Usage\nissues, git_info = validate_repository_state()\n\nif issues:\n    print(\"Repository state issues:\")\n    for issue in issues:\n        print(f\"  - {issue}\")\n\n    # Log issues as experiment metadata\n    exp = tracelet.start_logging(\n        exp_name=\"experiment_with_issues\",\n        project=\"debug\",\n        backend=\"mlflow\"\n    )\n\n    exp.log_params({\n        \"git_issues\": \", \".join(issues),\n        \"git_validation\": \"failed\"\n    })\nelse:\n    print(\"Repository state is clean\")\n</code></pre>"},{"location":"api/collectors/git/#experiment-lineage","title":"Experiment Lineage","text":"<pre><code>import json\nfrom tracelet.collectors.git import GitCollector\n\ndef track_experiment_lineage(parent_commit=None):\n    \"\"\"Track experiment lineage through git history.\"\"\"\n    git_collector = GitCollector()\n    git_collector.initialize()\n    git_info = git_collector.collect()\n\n    lineage_info = {\n        \"current_commit\": git_info.get(\"commit_hash\"),\n        \"current_branch\": git_info.get(\"branch\"),\n        \"parent_commit\": parent_commit,\n        \"timestamp\": str(datetime.now())\n    }\n\n    # Save lineage information\n    with open(\"experiment_lineage.json\", \"w\") as f:\n        json.dump(lineage_info, f, indent=2)\n\n    return lineage_info\n\n# Usage\nlineage = track_experiment_lineage(parent_commit=\"abc123def456\")\n\nexp = tracelet.start_logging(\n    exp_name=\"lineage_tracked\",\n    project=\"experiment_lineage\",\n    backend=\"mlflow\"\n)\n\nexp.log_params(lineage)\nexp.log_artifact(\"experiment_lineage.json\", \"metadata/lineage.json\")\n</code></pre>"},{"location":"api/collectors/system/","title":"System Metrics Collector","text":"<p>               Bases: <code>CollectorInterface</code></p> <p>Collector for system metrics including CPU, memory, and GPU usage</p> Source code in <code>tracelet/collectors/system.py</code> <pre><code>def __init__(self, collect_interval: float = 10.0):\n    self.collect_interval = collect_interval\n    self._stop_event = threading.Event()\n    self._collection_thread = None\n    self._metrics = {}\n    self._nvml_initialized = False\n</code></pre> <p>options: show_source: true show_bases: true merge_init_into_class: true heading_level: 2</p>"},{"location":"api/collectors/system/#tracelet.collectors.system.SystemMetricsCollector.collect","title":"<code>collect()</code>","text":"<p>Collect current system information and metrics</p> Source code in <code>tracelet/collectors/system.py</code> <pre><code>def collect(self) -&gt; dict[str, Any]:\n    \"\"\"Collect current system information and metrics\"\"\"\n    system_info = {\n        \"platform\": platform.platform(),\n        \"python_version\": platform.python_version(),\n        \"cpu_count\": psutil.cpu_count(),\n        \"cpu_count_physical\": psutil.cpu_count(logical=False),\n        \"memory_total\": psutil.virtual_memory().total,\n        \"hostname\": platform.node(),\n    }\n\n    # Add current metrics\n    metrics = self._collect_current_metrics()\n    system_info.update(metrics)\n\n    return system_info\n</code></pre>"},{"location":"api/collectors/system/#tracelet.collectors.system.SystemMetricsCollector.start","title":"<code>start()</code>","text":"<p>Start the background collection thread</p> Source code in <code>tracelet/collectors/system.py</code> <pre><code>def start(self):\n    \"\"\"Start the background collection thread\"\"\"\n    if self._collection_thread is None:\n        self._stop_event.clear()\n        self._collection_thread = threading.Thread(target=self._collection_loop, daemon=True)\n        self._collection_thread.start()\n</code></pre>"},{"location":"api/collectors/system/#tracelet.collectors.system.SystemMetricsCollector.stop","title":"<code>stop()</code>","text":"<p>Stop the background collection thread</p> Source code in <code>tracelet/collectors/system.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the background collection thread\"\"\"\n    if self._collection_thread is not None:\n        self._stop_event.set()\n        self._collection_thread.join()\n        self._collection_thread = None\n\n    if self._nvml_initialized:\n        with contextlib.suppress(Exception):\n            pynvml.nvmlShutdown()\n</code></pre>"},{"location":"api/collectors/system/#overview","title":"Overview","text":"<p>The System Metrics Collector monitors system performance during experiment execution, providing insights into resource utilization and system health.</p>"},{"location":"api/collectors/system/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\n\n# System metrics are collected automatically with default settings\nexp = tracelet.start_logging(\n    exp_name=\"system_monitoring_demo\",\n    project=\"performance_tracking\",\n    backend=\"mlflow\"\n)\n\n# System metrics collected in background automatically\n# Train your model here...\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/system/#manual-system-collection","title":"Manual System Collection","text":"<pre><code>from tracelet.collectors.system import SystemMetricsCollector\n\n# Create system collector\nsystem_collector = SystemMetricsCollector(collect_interval=5.0)\n\n# Initialize and start background collection\nsystem_collector.initialize()\nsystem_collector.start()\n\n# Collect current metrics snapshot\ncurrent_metrics = system_collector.collect()\n\nprint(\"Current System Metrics:\")\nfor category, metrics in current_metrics.items():\n    print(f\"  {category}: {metrics}\")\n\n# Stop collection\nsystem_collector.stop()\n</code></pre>"},{"location":"api/collectors/system/#configuration-options","title":"Configuration Options","text":""},{"location":"api/collectors/system/#collection-interval","title":"Collection Interval","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Configure system metrics collection\nsettings = TraceletSettings(\n    project=\"performance_monitoring\",\n    backend=[\"mlflow\"],\n    track_system=True,        # Enable system tracking\n    metrics_interval=10.0,    # Collect every 10 seconds\n    track_gpu=True,          # Include GPU metrics (if available)\n    track_disk=True,         # Include disk I/O metrics\n    track_network=True       # Include network I/O metrics\n)\n\ntracelet.start_logging(\n    exp_name=\"configured_monitoring\",\n    settings=settings\n)\n</code></pre>"},{"location":"api/collectors/system/#custom-collection-parameters","title":"Custom Collection Parameters","text":"<pre><code>from tracelet.collectors.system import SystemMetricsCollector\n\n# Custom collector configuration\ncollector = SystemMetricsCollector(\n    collect_interval=2.0,     # Collect every 2 seconds\n    include_per_cpu=True,     # Include per-CPU metrics\n    include_processes=True,   # Include top processes\n    process_limit=5          # Limit to top 5 processes\n)\n</code></pre>"},{"location":"api/collectors/system/#collected-metrics","title":"Collected Metrics","text":""},{"location":"api/collectors/system/#cpu-metrics","title":"CPU Metrics","text":"<ul> <li>cpu_percent: Overall CPU utilization percentage</li> <li>cpu_count_logical: Number of logical CPU cores</li> <li>cpu_count_physical: Number of physical CPU cores</li> <li>cpu_per_core: Per-core utilization (if enabled)</li> <li>load_average: System load averages (1, 5, 15 minutes)</li> </ul>"},{"location":"api/collectors/system/#memory-metrics","title":"Memory Metrics","text":"<ul> <li>memory_total: Total system memory (bytes)</li> <li>memory_available: Available memory (bytes)</li> <li>memory_used: Used memory (bytes)</li> <li>memory_percent: Memory utilization percentage</li> <li>swap_total: Total swap space (bytes)</li> <li>swap_used: Used swap space (bytes)</li> </ul>"},{"location":"api/collectors/system/#disk-metrics","title":"Disk Metrics","text":"<ul> <li>disk_total: Total disk space (bytes)</li> <li>disk_used: Used disk space (bytes)</li> <li>disk_free: Free disk space (bytes)</li> <li>disk_percent: Disk utilization percentage</li> <li>disk_read_bytes: Cumulative bytes read</li> <li>disk_write_bytes: Cumulative bytes written</li> </ul>"},{"location":"api/collectors/system/#network-metrics","title":"Network Metrics","text":"<ul> <li>network_bytes_sent: Cumulative bytes sent</li> <li>network_bytes_recv: Cumulative bytes received</li> <li>network_packets_sent: Cumulative packets sent</li> <li>network_packets_recv: Cumulative packets received</li> </ul>"},{"location":"api/collectors/system/#gpu-metrics-if-available","title":"GPU Metrics (if available)","text":"<ul> <li>gpu_count: Number of available GPUs</li> <li>gpu_utilization: GPU utilization per device</li> <li>gpu_memory_used: GPU memory usage per device</li> <li>gpu_memory_total: Total GPU memory per device</li> <li>gpu_temperature: GPU temperature per device</li> </ul>"},{"location":"api/collectors/system/#practical-examples","title":"Practical Examples","text":""},{"location":"api/collectors/system/#performance-monitoring-during-training","title":"Performance Monitoring During Training","text":"<pre><code>import time\nimport tracelet\nfrom tracelet.collectors.system import SystemMetricsCollector\n\n# Start experiment with system monitoring\nexp = tracelet.start_logging(\n    exp_name=\"performance_monitored_training\",\n    project=\"ml_performance\",\n    backend=\"mlflow\"\n)\n\n# Get system collector for manual snapshots\ncollector = SystemMetricsCollector(collect_interval=1.0)\ncollector.initialize()\n\n# Training simulation with periodic monitoring\nfor epoch in range(100):\n    # Simulate training work\n    time.sleep(0.1)\n\n    # Log training metrics\n    train_loss = 1.0 / (epoch + 1)\n    exp.log_metric(\"train_loss\", train_loss, iteration=epoch)\n\n    # Periodic system snapshots\n    if epoch % 10 == 0:\n        system_snapshot = collector.collect()\n\n        # Log key system metrics\n        exp.log_metric(\"cpu_percent\", system_snapshot.get(\"cpu_percent\", 0), iteration=epoch)\n        exp.log_metric(\"memory_percent\", system_snapshot.get(\"memory_percent\", 0), iteration=epoch)\n\n        # Log GPU metrics if available\n        if \"gpu\" in system_snapshot:\n            for i, gpu_info in enumerate(system_snapshot[\"gpu\"]):\n                exp.log_metric(f\"gpu_{i}_utilization\", gpu_info.get(\"utilization\", 0), iteration=epoch)\n                exp.log_metric(f\"gpu_{i}_memory_percent\", gpu_info.get(\"memory_percent\", 0), iteration=epoch)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/system/#resource-usage-analysis","title":"Resource Usage Analysis","text":"<pre><code>import tracelet\nimport matplotlib.pyplot as plt\nfrom tracelet.collectors.system import SystemMetricsCollector\n\n# Detailed resource analysis\ncollector = SystemMetricsCollector(collect_interval=0.5)\ncollector.initialize()\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"resource_analysis\",\n    project=\"performance_analysis\",\n    backend=\"mlflow\"\n)\n\n# Collect metrics during workload\nmetrics_history = []\nstart_time = time.time()\n\n# Simulate varying workload\nfor i in range(120):  # 60 seconds of collection\n    # Simulate different workload intensities\n    if i &lt; 40:\n        # Light workload\n        time.sleep(0.1)\n    elif i &lt; 80:\n        # Heavy workload simulation\n        _ = [x**2 for x in range(10000)]\n        time.sleep(0.3)\n    else:\n        # Cool down\n        time.sleep(0.2)\n\n    # Collect metrics\n    metrics = collector.collect()\n    metrics[\"timestamp\"] = time.time() - start_time\n    metrics_history.append(metrics)\n\n# Analyze and plot resource usage\ntimestamps = [m[\"timestamp\"] for m in metrics_history]\ncpu_usage = [m.get(\"cpu_percent\", 0) for m in metrics_history]\nmemory_usage = [m.get(\"memory_percent\", 0) for m in metrics_history]\n\nplt.figure(figsize=(12, 8))\n\nplt.subplot(2, 1, 1)\nplt.plot(timestamps, cpu_usage, label=\"CPU %\")\nplt.ylabel(\"CPU Usage (%)\")\nplt.legend()\nplt.title(\"System Resource Usage During Experiment\")\n\nplt.subplot(2, 1, 2)\nplt.plot(timestamps, memory_usage, label=\"Memory %\", color=\"red\")\nplt.ylabel(\"Memory Usage (%)\")\nplt.xlabel(\"Time (seconds)\")\nplt.legend()\n\nplt.tight_layout()\nplt.savefig(\"resource_usage.png\")\n\n# Log analysis results\nexp.log_artifact(\"resource_usage.png\", \"analysis/resource_usage.png\")\n\n# Log summary statistics\navg_cpu = sum(cpu_usage) / len(cpu_usage)\nmax_cpu = max(cpu_usage)\navg_memory = sum(memory_usage) / len(memory_usage)\nmax_memory = max(memory_usage)\n\nexp.log_params({\n    \"avg_cpu_usage\": avg_cpu,\n    \"max_cpu_usage\": max_cpu,\n    \"avg_memory_usage\": avg_memory,\n    \"max_memory_usage\": max_memory\n})\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/system/#gpu-monitoring","title":"GPU Monitoring","text":"<pre><code>import tracelet\nfrom tracelet.collectors.system import SystemMetricsCollector\n\ndef monitor_gpu_training():\n    \"\"\"Monitor GPU utilization during training.\"\"\"\n\n    exp = tracelet.start_logging(\n        exp_name=\"gpu_monitored_training\",\n        project=\"gpu_performance\",\n        backend=\"mlflow\"\n    )\n\n    collector = SystemMetricsCollector(collect_interval=1.0)\n    collector.initialize()\n\n    # Simulate GPU training\n    import torch\n\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda\")\n\n        # Create model and data on GPU\n        model = torch.nn.Linear(1000, 100).to(device)\n        optimizer = torch.optim.Adam(model.parameters())\n\n        for epoch in range(50):\n            # Simulate training batch\n            data = torch.randn(128, 1000).to(device)\n            target = torch.randn(128, 100).to(device)\n\n            optimizer.zero_grad()\n            output = model(data)\n            loss = torch.nn.functional.mse_loss(output, target)\n            loss.backward()\n            optimizer.step()\n\n            # Log training metrics\n            exp.log_metric(\"train_loss\", loss.item(), iteration=epoch)\n\n            # Log GPU metrics\n            system_metrics = collector.collect()\n            if \"gpu\" in system_metrics:\n                for i, gpu_info in enumerate(system_metrics[\"gpu\"]):\n                    exp.log_metric(f\"gpu_{i}_utilization\", gpu_info.get(\"utilization\", 0), iteration=epoch)\n                    exp.log_metric(f\"gpu_{i}_memory_used\", gpu_info.get(\"memory_used\", 0), iteration=epoch)\n                    exp.log_metric(f\"gpu_{i}_temperature\", gpu_info.get(\"temperature\", 0), iteration=epoch)\n    else:\n        print(\"No GPU available for monitoring\")\n\n    tracelet.stop_logging()\n\n# Run GPU monitoring\nmonitor_gpu_training()\n</code></pre>"},{"location":"api/collectors/system/#advanced-features","title":"Advanced Features","text":""},{"location":"api/collectors/system/#custom-metric-collection","title":"Custom Metric Collection","text":"<pre><code>from tracelet.collectors.system import SystemMetricsCollector\nimport psutil\nimport json\n\nclass ExtendedSystemCollector(SystemMetricsCollector):\n    \"\"\"Extended system collector with custom metrics.\"\"\"\n\n    def collect(self):\n        \"\"\"Collect standard metrics plus custom ones.\"\"\"\n        # Get base metrics\n        metrics = super().collect()\n\n        # Add custom metrics\n        custom_metrics = self._collect_custom_metrics()\n        metrics.update(custom_metrics)\n\n        return metrics\n\n    def _collect_custom_metrics(self):\n        \"\"\"Collect additional custom system metrics.\"\"\"\n        custom = {}\n\n        # Process count by state\n        processes = list(psutil.process_iter(['status']))\n        status_counts = {}\n        for proc in processes:\n            try:\n                status = proc.info['status']\n                status_counts[status] = status_counts.get(status, 0) + 1\n            except (psutil.NoSuchProcess, psutil.AccessDenied):\n                pass\n\n        custom[\"process_counts\"] = status_counts\n        custom[\"total_processes\"] = len(processes)\n\n        # Open file descriptors\n        try:\n            current_process = psutil.Process()\n            custom[\"open_files\"] = len(current_process.open_files())\n        except (psutil.AccessDenied, AttributeError):\n            custom[\"open_files\"] = -1\n\n        # System uptime\n        boot_time = psutil.boot_time()\n        uptime = time.time() - boot_time\n        custom[\"system_uptime_hours\"] = uptime / 3600\n\n        return custom\n\n# Usage\nextended_collector = ExtendedSystemCollector(collect_interval=5.0)\nextended_collector.initialize()\n\nmetrics = extended_collector.collect()\nprint(\"Extended metrics:\", json.dumps(metrics, indent=2))\n</code></pre>"},{"location":"api/collectors/system/#threshold-based-alerts","title":"Threshold-Based Alerts","text":"<pre><code>import tracelet\nfrom tracelet.collectors.system import SystemMetricsCollector\nimport time\n\nclass AlertingSystemCollector(SystemMetricsCollector):\n    \"\"\"System collector with alerting capabilities.\"\"\"\n\n    def __init__(self, collect_interval=10.0, thresholds=None):\n        super().__init__(collect_interval)\n        self.thresholds = thresholds or {\n            \"cpu_percent\": 90.0,\n            \"memory_percent\": 85.0,\n            \"disk_percent\": 95.0\n        }\n        self.alerts = []\n\n    def collect(self):\n        \"\"\"Collect metrics and check thresholds.\"\"\"\n        metrics = super().collect()\n        self._check_thresholds(metrics)\n        return metrics\n\n    def _check_thresholds(self, metrics):\n        \"\"\"Check if any metrics exceed thresholds.\"\"\"\n        timestamp = time.time()\n\n        for metric_name, threshold in self.thresholds.items():\n            if metric_name in metrics:\n                value = metrics[metric_name]\n                if value &gt; threshold:\n                    alert = {\n                        \"timestamp\": timestamp,\n                        \"metric\": metric_name,\n                        \"value\": value,\n                        \"threshold\": threshold,\n                        \"severity\": \"warning\" if value &lt; threshold * 1.1 else \"critical\"\n                    }\n                    self.alerts.append(alert)\n                    print(f\"ALERT: {metric_name} = {value:.1f}% (threshold: {threshold}%)\")\n\n    def get_alerts(self):\n        \"\"\"Get all alerts since initialization.\"\"\"\n        return self.alerts.copy()\n\n# Usage with alerting\nexp = tracelet.start_logging(\n    exp_name=\"monitored_with_alerts\",\n    project=\"system_alerting\",\n    backend=\"mlflow\"\n)\n\nalerting_collector = AlertingSystemCollector(\n    collect_interval=2.0,\n    thresholds={\n        \"cpu_percent\": 80.0,\n        \"memory_percent\": 75.0\n    }\n)\n\nalerting_collector.initialize()\nalerting_collector.start()\n\n# Simulate workload\ntime.sleep(30)\n\n# Check for alerts\nalerts = alerting_collector.get_alerts()\nif alerts:\n    print(f\"Generated {len(alerts)} alerts during experiment\")\n\n    # Log alerts as experiment metadata\n    exp.log_params({\n        \"alert_count\": len(alerts),\n        \"max_cpu_alert\": max([a[\"value\"] for a in alerts if a[\"metric\"] == \"cpu_percent\"], default=0),\n        \"critical_alerts\": len([a for a in alerts if a[\"severity\"] == \"critical\"])\n    })\n\nalerting_collector.stop()\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/system/#error-handling","title":"Error Handling","text":""},{"location":"api/collectors/system/#platform-compatibility","title":"Platform Compatibility","text":"<pre><code>import platform\nfrom tracelet.collectors.system import SystemMetricsCollector\n\ndef create_platform_aware_collector():\n    \"\"\"Create system collector appropriate for current platform.\"\"\"\n\n    system = platform.system().lower()\n\n    try:\n        if system == \"linux\":\n            # Linux-specific configuration\n            collector = SystemMetricsCollector(\n                collect_interval=5.0,\n                include_per_cpu=True,\n                include_processes=True\n            )\n        elif system == \"darwin\":  # macOS\n            # macOS-specific configuration\n            collector = SystemMetricsCollector(\n                collect_interval=5.0,\n                include_per_cpu=False  # Some limitations on macOS\n            )\n        elif system == \"windows\":\n            # Windows-specific configuration\n            collector = SystemMetricsCollector(\n                collect_interval=10.0  # Slower collection on Windows\n            )\n        else:\n            # Default configuration for unknown platforms\n            collector = SystemMetricsCollector(collect_interval=10.0)\n\n        collector.initialize()\n        return collector\n\n    except Exception as e:\n        print(f\"Failed to create system collector: {e}\")\n        return None\n\n# Usage\ncollector = create_platform_aware_collector()\nif collector:\n    metrics = collector.collect()\n    print(f\"Platform: {platform.system()}\")\n    print(f\"Available metrics: {list(metrics.keys())}\")\nelse:\n    print(\"System metrics collection not available on this platform\")\n</code></pre>"},{"location":"api/collectors/system/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>from tracelet.collectors.system import SystemMetricsCollector\nimport logging\n\ndef safe_system_collection():\n    \"\"\"Safely collect system metrics with fallback options.\"\"\"\n\n    try:\n        # Try full system collection\n        collector = SystemMetricsCollector(collect_interval=5.0)\n        collector.initialize()\n        metrics = collector.collect()\n\n        # Validate critical metrics\n        required_metrics = [\"cpu_percent\", \"memory_percent\"]\n        missing_metrics = [m for m in required_metrics if m not in metrics]\n\n        if missing_metrics:\n            logging.warning(f\"Missing critical metrics: {missing_metrics}\")\n\n        return metrics\n\n    except ImportError as e:\n        logging.error(f\"Missing system monitoring dependencies: {e}\")\n        return {\"error\": \"psutil not available\", \"basic_info\": {\"platform\": platform.system()}}\n\n    except Exception as e:\n        logging.error(f\"System collection failed: {e}\")\n        # Fallback to basic information\n        return {\n            \"error\": str(e),\n            \"fallback_metrics\": {\n                \"timestamp\": time.time(),\n                \"platform\": platform.system(),\n                \"python_version\": platform.python_version()\n            }\n        }\n\n# Usage\nsystem_info = safe_system_collection()\n\nexp = tracelet.start_logging(\n    exp_name=\"safe_monitoring\",\n    project=\"robust_experiments\",\n    backend=\"mlflow\"\n)\n\nif \"error\" not in system_info:\n    # Log successful metrics\n    exp.log_params({\n        \"system_monitoring\": \"enabled\",\n        \"cpu_cores\": system_info.get(\"cpu_count_logical\", \"unknown\"),\n        \"memory_gb\": round(system_info.get(\"memory_total\", 0) / (1024**3), 2)\n    })\nelse:\n    # Log error information\n    exp.log_params({\n        \"system_monitoring\": \"failed\",\n        \"error\": system_info[\"error\"]\n    })\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/collectors/system/#best-practices","title":"Best Practices","text":""},{"location":"api/collectors/system/#resource-aware-collection","title":"Resource-Aware Collection","text":"<pre><code>from tracelet.collectors.system import SystemMetricsCollector\nimport psutil\n\ndef configure_adaptive_collection():\n    \"\"\"Configure collection based on system resources.\"\"\"\n\n    # Get system information\n    memory_gb = psutil.virtual_memory().total / (1024**3)\n    cpu_count = psutil.cpu_count()\n\n    # Adapt collection based on system capacity\n    if memory_gb &lt; 4:  # Low memory system\n        interval = 30.0  # Less frequent collection\n        include_processes = False\n    elif memory_gb &lt; 8:  # Medium memory system\n        interval = 15.0\n        include_processes = True\n    else:  # High memory system\n        interval = 5.0\n        include_processes = True\n\n    # Adapt based on CPU count\n    if cpu_count &lt;= 2:\n        include_per_cpu = False\n    else:\n        include_per_cpu = True\n\n    collector = SystemMetricsCollector(\n        collect_interval=interval,\n        include_per_cpu=include_per_cpu,\n        include_processes=include_processes\n    )\n\n    print(f\"Configured system collection:\")\n    print(f\"  Interval: {interval}s\")\n    print(f\"  Per-CPU metrics: {include_per_cpu}\")\n    print(f\"  Process metrics: {include_processes}\")\n\n    return collector\n\n# Usage\nadaptive_collector = configure_adaptive_collection()\nadaptive_collector.initialize()\n</code></pre>"},{"location":"api/collectors/system/#experiment-performance-impact","title":"Experiment Performance Impact","text":"<pre><code>import time\nfrom tracelet.collectors.system import SystemMetricsCollector\n\ndef measure_collection_overhead():\n    \"\"\"Measure the performance impact of system collection.\"\"\"\n\n    # Baseline measurement without collection\n    start_time = time.time()\n    for i in range(1000):\n        _ = [x**2 for x in range(1000)]  # Simulate work\n    baseline_time = time.time() - start_time\n\n    # Measurement with system collection\n    collector = SystemMetricsCollector(collect_interval=0.1)  # Aggressive collection\n    collector.initialize()\n    collector.start()\n\n    start_time = time.time()\n    for i in range(1000):\n        _ = [x**2 for x in range(1000)]  # Same work\n    collection_time = time.time() - start_time\n\n    collector.stop()\n\n    overhead_percent = ((collection_time - baseline_time) / baseline_time) * 100\n\n    print(f\"Baseline time: {baseline_time:.3f}s\")\n    print(f\"With collection: {collection_time:.3f}s\")\n    print(f\"Overhead: {overhead_percent:.1f}%\")\n\n    return overhead_percent\n\n# Measure and decide on collection strategy\noverhead = measure_collection_overhead()\n\nif overhead &gt; 5.0:  # More than 5% overhead\n    print(\"High overhead detected, using conservative collection\")\n    collection_interval = 30.0\nelse:\n    print(\"Low overhead, using frequent collection\")\n    collection_interval = 5.0\n</code></pre>"},{"location":"api/core/experiment/","title":"Experiment","text":"<p>               Bases: <code>MetricSource</code></p> <p>Main experiment tracking class that orchestrates all tracking functionality</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def __init__(\n    self,\n    name: str,\n    config: Optional[ExperimentConfig] = None,\n    backend: Optional[list[str]] = None,  # Changed to list[str]\n    tags: Optional[list[str]] = None,\n    automagic: bool = False,  # Enable automagic instrumentation\n    automagic_config: Optional[\"AutomagicConfig\"] = None,  # Custom automagic configuration\n):\n    self.name = name\n    self.id = str(uuid.uuid4())\n    self.config = config or ExperimentConfig()\n    self.created_at = datetime.now(timezone.utc)\n    self.tags = tags or []\n    self._current_iteration = 0\n    self._active_collectors = []\n    self._backends = backend if backend is not None else []  # Changed to _backends\n    self._framework = None\n\n    # Automagic instrumentation\n    self._automagic_enabled = automagic or self.config.enable_automagic\n    self._automagic_config = automagic_config\n    self._automagic_instrumentor = None\n\n    # Initialize data flow orchestrator\n    self._orchestrator = DataFlowOrchestrator(max_queue_size=10000, num_workers=4)\n\n    # Initialize plugin manager\n    self._plugin_manager = PluginManager()\n\n    self._initialize()\n</code></pre> <p>options: show_source: true show_bases: true merge_init_into_class: true heading_level: 2</p>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.iteration","title":"<code>iteration: int</code>  <code>property</code>","text":"<p>Get current iteration</p>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.capture_dataset","title":"<code>capture_dataset(dataset)</code>","text":"<p>Capture dataset information using automagic instrumentation.</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def capture_dataset(self, dataset: Any) -&gt; dict[str, Any]:\n    \"\"\"Capture dataset information using automagic instrumentation.\"\"\"\n    if not self._automagic_enabled or not self._automagic_instrumentor:\n        return {}\n\n    return self._automagic_instrumentor.capture_dataset_info(dataset, self)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.capture_hyperparams","title":"<code>capture_hyperparams()</code>","text":"<p>Capture hyperparameters from calling context using automagic instrumentation.</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def capture_hyperparams(self) -&gt; dict[str, Any]:\n    \"\"\"Capture hyperparameters from calling context using automagic instrumentation.\"\"\"\n    if not self._automagic_enabled or not self._automagic_instrumentor:\n        return {}\n\n    return self._automagic_instrumentor.capture_hyperparameters(self, frame_depth=2)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.capture_model","title":"<code>capture_model(model)</code>","text":"<p>Capture model information using automagic instrumentation.</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def capture_model(self, model: Any) -&gt; dict[str, Any]:\n    \"\"\"Capture model information using automagic instrumentation.\"\"\"\n    if not self._automagic_enabled or not self._automagic_instrumentor:\n        return {}\n\n    return self._automagic_instrumentor.capture_model_info(model, self)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.emit_metric","title":"<code>emit_metric(metric)</code>","text":"<p>Emit a metric to the orchestrator</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def emit_metric(self, metric: MetricData):\n    \"\"\"Emit a metric to the orchestrator\"\"\"\n    self._orchestrator.emit_metric(metric)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.end","title":"<code>end()</code>","text":"<p>End the experiment and clean up resources (alias for stop).</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def end(self):\n    \"\"\"End the experiment and clean up resources (alias for stop).\"\"\"\n    self.stop()\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.get_source_id","title":"<code>get_source_id()</code>","text":"<p>Return unique identifier for this experiment source</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def get_source_id(self) -&gt; str:\n    \"\"\"Return unique identifier for this experiment source\"\"\"\n    return f\"experiment_{self.id}\"\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.log_artifact","title":"<code>log_artifact(local_path, artifact_path=None)</code>","text":"<p>Log a local file as an artifact</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_artifact(self, local_path: str, artifact_path: Optional[str] = None):\n    \"\"\"Log a local file as an artifact\"\"\"\n    metric = MetricData(\n        name=artifact_path or local_path,\n        value=local_path,\n        type=MetricType.ARTIFACT,\n        iteration=None,  # Artifacts don't have iterations\n        source=self.get_source_id(),\n        metadata={\"artifact_path\": artifact_path},\n    )\n    self.emit_metric(metric)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.log_hyperparameter","title":"<code>log_hyperparameter(name, value)</code>","text":"<p>Log a hyperparameter (alias for compatibility).</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_hyperparameter(self, name: str, value: Any):\n    \"\"\"Log a hyperparameter (alias for compatibility).\"\"\"\n    # Create metric data directly to avoid intermediate dictionary creation\n    metric = MetricData(\n        name=name,\n        value=value,\n        type=MetricType.PARAMETER,\n        iteration=None,  # Parameters don't have iterations\n        source=self.get_source_id(),\n    )\n    self.emit_metric(metric)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.log_metric","title":"<code>log_metric(name, value, iteration=None)</code>","text":"<p>Log a metric value</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_metric(self, name: str, value: Any, iteration: Optional[int] = None):\n    \"\"\"Log a metric value\"\"\"\n    iteration = iteration or self._current_iteration\n\n    # Create metric data\n    metric = MetricData(\n        name=name,\n        value=value,\n        type=MetricType.SCALAR if isinstance(value, (int, float)) else MetricType.CUSTOM,\n        iteration=iteration,\n        source=self.get_source_id(),\n    )\n\n    # Emit to orchestrator\n    self.emit_metric(metric)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.log_params","title":"<code>log_params(params)</code>","text":"<p>Log experiment parameters</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_params(self, params: dict[str, Any]):\n    \"\"\"Log experiment parameters\"\"\"\n    for name, value in params.items():\n        metric = MetricData(\n            name=name,\n            value=value,\n            type=MetricType.PARAMETER,\n            iteration=None,  # Parameters don't have iterations\n            source=self.get_source_id(),\n        )\n        self.emit_metric(metric)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.set_iteration","title":"<code>set_iteration(iteration)</code>","text":"<p>Set the current iteration</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def set_iteration(self, iteration: int):\n    \"\"\"Set the current iteration\"\"\"\n    self._current_iteration = iteration\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.start","title":"<code>start()</code>","text":"<p>Start the experiment tracking</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def start(self):\n    \"\"\"Start the experiment tracking\"\"\"\n    # Start the orchestrator\n    self._orchestrator.start()\n\n    # Start backend plugins\n    for backend_name in self._backends:\n        self._plugin_manager.start_plugin(backend_name)\n\n    # Start collector plugins\n    for collector_info in self._plugin_manager.get_plugins_by_type(PluginType.COLLECTOR):\n        if self._plugin_manager.initialize_plugin(collector_info.metadata.name):\n            self._plugin_manager.start_plugin(collector_info.metadata.name)\n</code></pre>"},{"location":"api/core/experiment/#tracelet.core.experiment.Experiment.stop","title":"<code>stop()</code>","text":"<p>Stop the experiment tracking and clean up resources</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the experiment tracking and clean up resources\"\"\"\n    # Clean up automagic instrumentation first\n    if self._automagic_enabled and self._automagic_instrumentor:\n        self._automagic_instrumentor.detach_experiment(self.id)\n\n    # Stop all active plugins\n    for plugin_name, plugin_info in self._plugin_manager.plugins.items():\n        if plugin_info.state == PluginState.ACTIVE:\n            self._plugin_manager.stop_plugin(plugin_name)\n\n    # Stop the orchestrator\n    self._orchestrator.stop()\n</code></pre>"},{"location":"api/core/experiment/#core-methods","title":"Core Methods","text":""},{"location":"api/core/experiment/#metric-logging","title":"Metric Logging","text":""},{"location":"api/core/experiment/#log_metric","title":"log_metric","text":"<p>Log a metric value</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_metric(self, name: str, value: Any, iteration: Optional[int] = None):\n    \"\"\"Log a metric value\"\"\"\n    iteration = iteration or self._current_iteration\n\n    # Create metric data\n    metric = MetricData(\n        name=name,\n        value=value,\n        type=MetricType.SCALAR if isinstance(value, (int, float)) else MetricType.CUSTOM,\n        iteration=iteration,\n        source=self.get_source_id(),\n    )\n\n    # Emit to orchestrator\n    self.emit_metric(metric)\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Example Usage:</p> <pre><code>import tracelet\n\n# Start experiment\nexp = tracelet.start_logging(exp_name=\"metrics_demo\", project=\"examples\", backend=\"mlflow\")\n\n# Log scalar metrics\nexp.log_metric(\"loss\", 0.1, iteration=100)\nexp.log_metric(\"accuracy\", 0.95, iteration=100)\nexp.log_metric(\"learning_rate\", 0.001, iteration=100)\n\n# Log metrics over time\nfor epoch in range(10):\n    train_loss = 1.0 / (epoch + 1)  # Decreasing loss\n    exp.log_metric(\"train_loss\", train_loss, iteration=epoch)\n\n    if epoch % 2 == 0:  # Log validation every 2 epochs\n        val_loss = train_loss * 1.1\n        exp.log_metric(\"val_loss\", val_loss, iteration=epoch)\n</code></pre>"},{"location":"api/core/experiment/#parameter-logging","title":"Parameter Logging","text":""},{"location":"api/core/experiment/#log_params","title":"log_params","text":"<p>Log experiment parameters</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_params(self, params: dict[str, Any]):\n    \"\"\"Log experiment parameters\"\"\"\n    for name, value in params.items():\n        metric = MetricData(\n            name=name,\n            value=value,\n            type=MetricType.PARAMETER,\n            iteration=None,  # Parameters don't have iterations\n            source=self.get_source_id(),\n        )\n        self.emit_metric(metric)\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Example Usage:</p> <pre><code># Log hyperparameters\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"epochs\": 100,\n    \"optimizer\": \"adam\",\n    \"model_type\": \"resnet50\"\n})\n\n# Log model architecture details\nexp.log_params({\n    \"num_layers\": 18,\n    \"hidden_dim\": 512,\n    \"dropout\": 0.2,\n    \"activation\": \"relu\"\n})\n\n# Log data preprocessing parameters\nexp.log_params({\n    \"data_augmentation\": True,\n    \"normalization\": \"imagenet\",\n    \"train_split\": 0.8,\n    \"random_seed\": 42\n})\n</code></pre>"},{"location":"api/core/experiment/#artifact-management","title":"Artifact Management","text":""},{"location":"api/core/experiment/#log_artifact","title":"log_artifact","text":"<p>Log a local file as an artifact</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def log_artifact(self, local_path: str, artifact_path: Optional[str] = None):\n    \"\"\"Log a local file as an artifact\"\"\"\n    metric = MetricData(\n        name=artifact_path or local_path,\n        value=local_path,\n        type=MetricType.ARTIFACT,\n        iteration=None,  # Artifacts don't have iterations\n        source=self.get_source_id(),\n        metadata={\"artifact_path\": artifact_path},\n    )\n    self.emit_metric(metric)\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Example Usage:</p> <pre><code>import torch\nimport matplotlib.pyplot as plt\n\n# Save and log model checkpoint\ntorch.save(model.state_dict(), \"model_checkpoint.pth\")\nexp.log_artifact(\"model_checkpoint.pth\", \"models/checkpoint_epoch_10.pth\")\n\n# Log training plots\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses, label=\"Training Loss\")\nplt.plot(val_losses, label=\"Validation Loss\")\nplt.legend()\nplt.savefig(\"training_curves.png\")\nexp.log_artifact(\"training_curves.png\", \"plots/training_curves.png\")\n\n# Log configuration files\nexp.log_artifact(\"config.yaml\", \"configs/experiment_config.yaml\")\n\n# Log processed datasets\nexp.log_artifact(\"processed_data.csv\", \"data/processed/final_dataset.csv\")\n</code></pre>"},{"location":"api/core/experiment/#experiment-control","title":"Experiment Control","text":""},{"location":"api/core/experiment/#set_iteration","title":"set_iteration","text":"<p>Set the current iteration</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def set_iteration(self, iteration: int):\n    \"\"\"Set the current iteration\"\"\"\n    self._current_iteration = iteration\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Example Usage:</p> <pre><code># Manual iteration tracking\nfor epoch in range(100):\n    exp.set_iteration(epoch)\n\n    # All subsequent metrics logged without iteration will use current iteration\n    exp.log_metric(\"loss\", train_loss)  # Uses iteration=epoch\n    exp.log_metric(\"accuracy\", train_acc)  # Uses iteration=epoch\n\n    # Override with specific iteration if needed\n    exp.log_metric(\"val_loss\", val_loss, iteration=epoch*2)  # Custom iteration\n</code></pre>"},{"location":"api/core/experiment/#start-and-stop","title":"start and stop","text":"<p>Start the experiment tracking</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def start(self):\n    \"\"\"Start the experiment tracking\"\"\"\n    # Start the orchestrator\n    self._orchestrator.start()\n\n    # Start backend plugins\n    for backend_name in self._backends:\n        self._plugin_manager.start_plugin(backend_name)\n\n    # Start collector plugins\n    for collector_info in self._plugin_manager.get_plugins_by_type(PluginType.COLLECTOR):\n        if self._plugin_manager.initialize_plugin(collector_info.metadata.name):\n            self._plugin_manager.start_plugin(collector_info.metadata.name)\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Stop the experiment tracking and clean up resources</p> Source code in <code>tracelet/core/experiment.py</code> <pre><code>def stop(self):\n    \"\"\"Stop the experiment tracking and clean up resources\"\"\"\n    # Clean up automagic instrumentation first\n    if self._automagic_enabled and self._automagic_instrumentor:\n        self._automagic_instrumentor.detach_experiment(self.id)\n\n    # Stop all active plugins\n    for plugin_name, plugin_info in self._plugin_manager.plugins.items():\n        if plugin_info.state == PluginState.ACTIVE:\n            self._plugin_manager.stop_plugin(plugin_name)\n\n    # Stop the orchestrator\n    self._orchestrator.stop()\n</code></pre> <p>options: show_source: true heading_level: 4</p> <p>Example Usage:</p> <pre><code># Manual experiment lifecycle management\nexp = tracelet.start_logging(exp_name=\"manual_control\", project=\"test\", backend=\"mlflow\")\n\n# Start tracking (usually called automatically)\nexp.start()\n\n# Log metrics during experiment\nexp.log_metric(\"initial_metric\", 1.0)\n\n# Stop tracking (usually called by tracelet.stop_logging())\nexp.stop()\n</code></pre>"},{"location":"api/core/experiment/#configuration","title":"Configuration","text":""},{"location":"api/core/experiment/#experimentconfig","title":"ExperimentConfig","text":"<p>Configuration for experiment tracking</p> <p>options: show_source: true show_bases: true heading_level: 3</p> <p>Example Usage:</p> <pre><code>from tracelet.core.experiment import ExperimentConfig\n\n# Create custom configuration\nconfig = ExperimentConfig(\n    name=\"custom_experiment\",\n    project=\"research_project\",\n    backend_name=\"mlflow\",\n    tags={\"team\": \"ml\", \"version\": \"v2.0\"},\n    tracking_uri=\"http://mlflow.company.com:5000\"\n)\n\n# Use with experiment\nexp = Experiment(name=\"test\", config=config, backend=mlflow_backend)\n</code></pre>"},{"location":"api/core/experiment/#advanced-usage-patterns","title":"Advanced Usage Patterns","text":""},{"location":"api/core/experiment/#metric-batching","title":"Metric Batching","text":"<pre><code># Efficient metric logging for large datasets\nmetrics_batch = {}\nfor batch_idx, (data, target) in enumerate(dataloader):\n    # ... training code ...\n\n    # Collect metrics\n    metrics_batch[f\"batch_{batch_idx}_loss\"] = loss.item()\n\n    # Log in batches every 100 iterations\n    if batch_idx % 100 == 0:\n        exp.log_params(metrics_batch)\n        metrics_batch.clear()\n</code></pre>"},{"location":"api/core/experiment/#hierarchical-parameter-organization","title":"Hierarchical Parameter Organization","text":"<pre><code># Organize parameters hierarchically\nexp.log_params({\n    # Model parameters\n    \"model.architecture\": \"transformer\",\n    \"model.num_layers\": 12,\n    \"model.hidden_size\": 768,\n\n    # Training parameters\n    \"training.learning_rate\": 0.0001,\n    \"training.batch_size\": 16,\n    \"training.gradient_clip\": 1.0,\n\n    # Data parameters\n    \"data.max_seq_length\": 512,\n    \"data.vocab_size\": 30000,\n})\n</code></pre>"},{"location":"api/core/experiment/#error-handling-and-validation","title":"Error Handling and Validation","text":"<pre><code>try:\n    exp.log_metric(\"accuracy\", accuracy, iteration=epoch)\nexcept Exception as e:\n    print(f\"Failed to log metric: {e}\")\n    # Continue training without failing\n\n# Validate parameters before logging\nparams = {\"lr\": learning_rate, \"batch_size\": batch_size}\nvalid_params = {k: v for k, v in params.items() if v is not None}\nexp.log_params(valid_params)\n</code></pre>"},{"location":"api/core/plugins/","title":"Plugin System","text":"<p>options: show_source: true show_bases: true heading_level: 2</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.BackendPlugin","title":"<code>BackendPlugin()</code>","text":"<p>               Bases: <code>PluginBase</code>, <code>BackendInterface</code>, <code>MetricSink</code></p> <p>Base class for backend plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self):\n    self._config = {}\n    self._active = False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.BackendPlugin.can_handle_type","title":"<code>can_handle_type(metric_type)</code>","text":"<p>Check if this backend can handle the metric type</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def can_handle_type(self, metric_type: MetricType) -&gt; bool:\n    \"\"\"Check if this backend can handle the metric type\"\"\"\n    # By default, backends handle all metric types\n    return True\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.BackendPlugin.get_sink_id","title":"<code>get_sink_id()</code>","text":"<p>Return sink ID for orchestrator integration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_sink_id(self) -&gt; str:\n    \"\"\"Return sink ID for orchestrator integration\"\"\"\n    return f\"backend_{self.get_metadata().name}\"\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.CollectorPlugin","title":"<code>CollectorPlugin()</code>","text":"<p>               Bases: <code>PluginBase</code>, <code>CollectorInterface</code></p> <p>Base class for collector plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self):\n    self._config = {}\n    self._active = False\n    self._collection_interval = 60  # seconds\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase","title":"<code>PluginBase</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all plugins</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.get_metadata","title":"<code>get_metadata()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return plugin metadata</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_metadata(cls) -&gt; PluginMetadata:\n    \"\"\"Return plugin metadata\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.get_status","title":"<code>get_status()</code>  <code>abstractmethod</code>","text":"<p>Get plugin status</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get plugin status\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.initialize","title":"<code>initialize(config)</code>  <code>abstractmethod</code>","text":"<p>Initialize the plugin with configuration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef initialize(self, config: dict[str, Any]):\n    \"\"\"Initialize the plugin with configuration\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.start","title":"<code>start()</code>  <code>abstractmethod</code>","text":"<p>Start the plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef start(self):\n    \"\"\"Start the plugin\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Stop the plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef stop(self):\n    \"\"\"Stop the plugin\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginInfo","title":"<code>PluginInfo(metadata, module_path, class_name, state=PluginState.DISCOVERED, instance=None, error=None)</code>  <code>dataclass</code>","text":"<p>Information about a discovered plugin</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager","title":"<code>PluginManager(plugin_paths=None, use_default_paths=True)</code>","text":"<p>Manages plugin discovery, loading, and lifecycle</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self, plugin_paths: Optional[list[str]] = None, use_default_paths: bool = True):\n    self.plugin_paths = plugin_paths or []\n    self.plugins: dict[str, PluginInfo] = {}\n    self._plugin_instances: dict[str, PluginBase] = {}\n    self._dependency_graph: dict[str, set[str]] = {}\n\n    # Add default plugin paths unless explicitly disabled\n    if use_default_paths:\n        self._add_default_paths()\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.discover_plugins","title":"<code>discover_plugins()</code>","text":"<p>Discover all available plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def discover_plugins(self) -&gt; list[PluginInfo]:\n    \"\"\"Discover all available plugins\"\"\"\n    discovered = []\n\n    for path in self.plugin_paths:\n        if os.path.isdir(path):\n            discovered.extend(self._discover_in_directory(path))\n        elif path.endswith(\".py\"):\n            plugin = self._discover_in_file(path)\n            if plugin:\n                discovered.append(plugin)\n\n    # Update internal registry\n    for plugin in discovered:\n        self.plugins[plugin.metadata.name] = plugin\n        logger.info(f\"Discovered plugin: {plugin.metadata.name} v{plugin.metadata.version}\")\n\n    return discovered\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_plugin_instance","title":"<code>get_plugin_instance(plugin_name)</code>","text":"<p>Get an initialized plugin instance</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_plugin_instance(self, plugin_name: str) -&gt; Optional[PluginBase]:\n    \"\"\"Get an initialized plugin instance\"\"\"\n    return self._plugin_instances.get(plugin_name)\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_plugins_by_type","title":"<code>get_plugins_by_type(plugin_type)</code>","text":"<p>Get all plugins of a specific type</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_plugins_by_type(self, plugin_type: PluginType) -&gt; list[PluginInfo]:\n    \"\"\"Get all plugins of a specific type\"\"\"\n    return [info for info in self.plugins.values() if info.metadata.type == plugin_type]\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_status","title":"<code>get_status()</code>","text":"<p>Get status of all plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get status of all plugins\"\"\"\n    status = {\n        \"discovered\": len(self.plugins),\n        \"loaded\": sum(1 for p in self.plugins.values() if p.state &gt;= PluginState.LOADED),\n        \"active\": sum(1 for p in self.plugins.values() if p.state == PluginState.ACTIVE),\n        \"errors\": sum(1 for p in self.plugins.values() if p.state == PluginState.ERROR),\n        \"plugins\": {},\n    }\n\n    for name, info in self.plugins.items():\n        plugin_status = {\n            \"state\": info.state.name.lower(),\n            \"type\": info.metadata.type.value,\n            \"version\": info.metadata.version,\n        }\n\n        if info.state == PluginState.ERROR:\n            plugin_status[\"error\"] = info.error\n\n        if name in self._plugin_instances:\n            try:\n                plugin_status[\"status\"] = self._plugin_instances[name].get_status()\n            except Exception as e:\n                plugin_status[\"status_error\"] = str(e)\n\n        status[\"plugins\"][name] = plugin_status\n\n    return status\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.initialize_plugin","title":"<code>initialize_plugin(plugin_name, config=None)</code>","text":"<p>Initialize a plugin with configuration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def initialize_plugin(self, plugin_name: str, config: Optional[dict[str, Any]] = None) -&gt; bool:\n    \"\"\"Initialize a plugin with configuration\"\"\"\n    if plugin_name not in self.plugins:\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    # Ensure plugin is validated\n    if plugin_info.state &lt; PluginState.VALIDATED and not self.validate_plugin(plugin_name):\n        return False\n\n    try:\n        # Create plugin instance\n        plugin_class = plugin_info.instance\n        instance = plugin_class()\n\n        # Initialize with config\n        instance.initialize(config or {})\n\n        # Store instance\n        self._plugin_instances[plugin_name] = instance\n        plugin_info.state = PluginState.INITIALIZED\n\n        logger.info(f\"Initialized plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to initialize plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.load_config","title":"<code>load_config(config_path)</code>","text":"<p>Load plugin configuration from file</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def load_config(self, config_path: str) -&gt; dict[str, Any]:\n    \"\"\"Load plugin configuration from file\"\"\"\n    with open(config_path) as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.load_plugin","title":"<code>load_plugin(plugin_name)</code>","text":"<p>Load a specific plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def load_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Load a specific plugin\"\"\"\n    if plugin_name not in self.plugins:\n        logger.error(f\"Plugin '{plugin_name}' not found\")\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    if plugin_info.state &gt;= PluginState.LOADED:\n        return True\n\n    try:\n        # Load the module\n        spec = importlib.util.spec_from_file_location(f\"plugin_{plugin_name}\", plugin_info.module_path)\n        if not spec or not spec.loader:\n            raise ImportError(f\"Failed to load spec for {plugin_name}\")\n\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n\n        # Get the plugin class\n        plugin_class = getattr(module, plugin_info.class_name)\n\n        # Store class reference\n        plugin_info.instance = plugin_class\n        plugin_info.state = PluginState.LOADED\n\n        logger.info(f\"Loaded plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to load plugin '{plugin_name}'\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.resolve_dependencies","title":"<code>resolve_dependencies(plugin_names)</code>","text":"<p>Resolve plugin dependencies and return load order</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def resolve_dependencies(self, plugin_names: list[str]) -&gt; list[str]:\n    \"\"\"Resolve plugin dependencies and return load order\"\"\"\n    # Build dependency graph\n    graph = {}\n    for name in plugin_names:\n        if name in self.plugins:\n            deps = self.plugins[name].metadata.dependencies\n            graph[name] = set(deps)\n\n    # Topological sort\n    result = []\n    visited = set()\n    temp_visited = set()\n\n    def visit(node):\n        if node in temp_visited:\n            raise ValueError(f\"Circular dependency detected: {node}\")\n        if node in visited:\n            return\n\n        temp_visited.add(node)\n        for dep in graph.get(node, set()):\n            visit(dep)\n        temp_visited.remove(node)\n        visited.add(node)\n        result.append(node)\n\n    for name in plugin_names:\n        if name not in visited:\n            visit(name)\n\n    return result\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.start_plugin","title":"<code>start_plugin(plugin_name)</code>","text":"<p>Start a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def start_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Start a plugin\"\"\"\n    if plugin_name not in self._plugin_instances:\n        logger.error(f\"Plugin '{plugin_name}' not initialized\")\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n    instance = self._plugin_instances[plugin_name]\n\n    try:\n        instance.start()\n        plugin_info.state = PluginState.ACTIVE\n        logger.info(f\"Started plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to start plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.stop_plugin","title":"<code>stop_plugin(plugin_name)</code>","text":"<p>Stop a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def stop_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Stop a plugin\"\"\"\n    if plugin_name not in self._plugin_instances:\n        return True\n\n    plugin_info = self.plugins[plugin_name]\n    instance = self._plugin_instances[plugin_name]\n\n    try:\n        instance.stop()\n        plugin_info.state = PluginState.STOPPED\n        logger.info(f\"Stopped plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        logger.exception(f\"Error stopping plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.validate_plugin","title":"<code>validate_plugin(plugin_name)</code>","text":"<p>Validate a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def validate_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Validate a plugin\"\"\"\n    if plugin_name not in self.plugins:\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    if plugin_info.state &lt; PluginState.LOADED and not self.load_plugin(plugin_name):\n        return False\n\n    try:\n        # Check required methods\n        required_methods = [\"get_metadata\", \"initialize\", \"start\", \"stop\", \"get_status\"]\n        plugin_class = plugin_info.instance\n\n        for method in required_methods:\n            if not hasattr(plugin_class, method):\n                raise ValueError(f\"Missing required method: {method}\")\n\n        # Validate metadata\n        metadata = plugin_class.get_metadata()\n        if not metadata.name or not metadata.version:\n            raise ValueError(\"Invalid metadata: name and version required\")\n\n        # Check dependencies\n        for dep in metadata.dependencies:\n            if dep not in self.plugins:\n                raise ValueError(f\"Missing dependency: {dep}\")\n\n        plugin_info.state = PluginState.VALIDATED\n        logger.info(f\"Validated plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Plugin validation failed for '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginMetadata","title":"<code>PluginMetadata(name, version, type, description='', author='', dependencies=list(), config_schema=None, capabilities=set())</code>  <code>dataclass</code>","text":"<p>Metadata for a plugin</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginState","title":"<code>PluginState</code>","text":"<p>               Bases: <code>IntEnum</code></p> <p>Plugin lifecycle states</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginType","title":"<code>PluginType</code>","text":"<p>               Bases: <code>Enum</code></p> <p>Types of plugins supported by the system</p>"},{"location":"api/core/plugins/#overview","title":"Overview","text":"<p>Tracelet's plugin system provides a flexible architecture for extending functionality through modular components. The system supports backend plugins, collector plugins, and framework integrations.</p>"},{"location":"api/core/plugins/#core-plugin-architecture","title":"Core Plugin Architecture","text":""},{"location":"api/core/plugins/#plugin-base-classes","title":"Plugin Base Classes","text":""},{"location":"api/core/plugins/#pluginbase","title":"PluginBase","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for all plugins</p> <p>options: show_source: true heading_level: 4</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.get_metadata","title":"<code>get_metadata()</code>  <code>abstractmethod</code> <code>classmethod</code>","text":"<p>Return plugin metadata</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@classmethod\n@abstractmethod\ndef get_metadata(cls) -&gt; PluginMetadata:\n    \"\"\"Return plugin metadata\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.get_status","title":"<code>get_status()</code>  <code>abstractmethod</code>","text":"<p>Get plugin status</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get plugin status\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.initialize","title":"<code>initialize(config)</code>  <code>abstractmethod</code>","text":"<p>Initialize the plugin with configuration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef initialize(self, config: dict[str, Any]):\n    \"\"\"Initialize the plugin with configuration\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.start","title":"<code>start()</code>  <code>abstractmethod</code>","text":"<p>Start the plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef start(self):\n    \"\"\"Start the plugin\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginBase.stop","title":"<code>stop()</code>  <code>abstractmethod</code>","text":"<p>Stop the plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>@abstractmethod\ndef stop(self):\n    \"\"\"Stop the plugin\"\"\"\n    pass\n</code></pre>"},{"location":"api/core/plugins/#backendplugin","title":"BackendPlugin","text":"<p>               Bases: <code>PluginBase</code>, <code>BackendInterface</code>, <code>MetricSink</code></p> <p>Base class for backend plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self):\n    self._config = {}\n    self._active = False\n</code></pre> <p>options: show_source: true heading_level: 4</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.BackendPlugin.can_handle_type","title":"<code>can_handle_type(metric_type)</code>","text":"<p>Check if this backend can handle the metric type</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def can_handle_type(self, metric_type: MetricType) -&gt; bool:\n    \"\"\"Check if this backend can handle the metric type\"\"\"\n    # By default, backends handle all metric types\n    return True\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.BackendPlugin.get_sink_id","title":"<code>get_sink_id()</code>","text":"<p>Return sink ID for orchestrator integration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_sink_id(self) -&gt; str:\n    \"\"\"Return sink ID for orchestrator integration\"\"\"\n    return f\"backend_{self.get_metadata().name}\"\n</code></pre>"},{"location":"api/core/plugins/#collectorplugin","title":"CollectorPlugin","text":"<p>               Bases: <code>PluginBase</code>, <code>CollectorInterface</code></p> <p>Base class for collector plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self):\n    self._config = {}\n    self._active = False\n    self._collection_interval = 60  # seconds\n</code></pre> <p>options: show_source: true heading_level: 4</p>"},{"location":"api/core/plugins/#plugin-metadata","title":"Plugin Metadata","text":"<p>Metadata for a plugin</p> <p>options: show_source: true heading_level: 3</p> <p>Example Usage:</p> <pre><code>from tracelet.core.plugins import PluginMetadata\n\n# Define plugin metadata\nmetadata = PluginMetadata(\n    name=\"custom_backend\",\n    version=\"1.0.0\",\n    description=\"Custom experiment tracking backend\",\n    author=\"ML Team\",\n    plugin_type=\"backend\",\n    entry_point=\"my_package.backends.CustomBackend\",\n    dependencies=[\"requests&gt;=2.25.0\", \"pandas&gt;=1.3.0\"],\n    config_schema={\n        \"api_endpoint\": {\"type\": \"string\", \"required\": True},\n        \"api_key\": {\"type\": \"string\", \"required\": True},\n        \"timeout\": {\"type\": \"number\", \"default\": 30}\n    }\n)\n</code></pre>"},{"location":"api/core/plugins/#plugin-manager","title":"Plugin Manager","text":"<p>Manages plugin discovery, loading, and lifecycle</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def __init__(self, plugin_paths: Optional[list[str]] = None, use_default_paths: bool = True):\n    self.plugin_paths = plugin_paths or []\n    self.plugins: dict[str, PluginInfo] = {}\n    self._plugin_instances: dict[str, PluginBase] = {}\n    self._dependency_graph: dict[str, set[str]] = {}\n\n    # Add default plugin paths unless explicitly disabled\n    if use_default_paths:\n        self._add_default_paths()\n</code></pre> <p>options: show_source: true heading_level: 3</p>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.discover_plugins","title":"<code>discover_plugins()</code>","text":"<p>Discover all available plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def discover_plugins(self) -&gt; list[PluginInfo]:\n    \"\"\"Discover all available plugins\"\"\"\n    discovered = []\n\n    for path in self.plugin_paths:\n        if os.path.isdir(path):\n            discovered.extend(self._discover_in_directory(path))\n        elif path.endswith(\".py\"):\n            plugin = self._discover_in_file(path)\n            if plugin:\n                discovered.append(plugin)\n\n    # Update internal registry\n    for plugin in discovered:\n        self.plugins[plugin.metadata.name] = plugin\n        logger.info(f\"Discovered plugin: {plugin.metadata.name} v{plugin.metadata.version}\")\n\n    return discovered\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_plugin_instance","title":"<code>get_plugin_instance(plugin_name)</code>","text":"<p>Get an initialized plugin instance</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_plugin_instance(self, plugin_name: str) -&gt; Optional[PluginBase]:\n    \"\"\"Get an initialized plugin instance\"\"\"\n    return self._plugin_instances.get(plugin_name)\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_plugins_by_type","title":"<code>get_plugins_by_type(plugin_type)</code>","text":"<p>Get all plugins of a specific type</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_plugins_by_type(self, plugin_type: PluginType) -&gt; list[PluginInfo]:\n    \"\"\"Get all plugins of a specific type\"\"\"\n    return [info for info in self.plugins.values() if info.metadata.type == plugin_type]\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.get_status","title":"<code>get_status()</code>","text":"<p>Get status of all plugins</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def get_status(self) -&gt; dict[str, Any]:\n    \"\"\"Get status of all plugins\"\"\"\n    status = {\n        \"discovered\": len(self.plugins),\n        \"loaded\": sum(1 for p in self.plugins.values() if p.state &gt;= PluginState.LOADED),\n        \"active\": sum(1 for p in self.plugins.values() if p.state == PluginState.ACTIVE),\n        \"errors\": sum(1 for p in self.plugins.values() if p.state == PluginState.ERROR),\n        \"plugins\": {},\n    }\n\n    for name, info in self.plugins.items():\n        plugin_status = {\n            \"state\": info.state.name.lower(),\n            \"type\": info.metadata.type.value,\n            \"version\": info.metadata.version,\n        }\n\n        if info.state == PluginState.ERROR:\n            plugin_status[\"error\"] = info.error\n\n        if name in self._plugin_instances:\n            try:\n                plugin_status[\"status\"] = self._plugin_instances[name].get_status()\n            except Exception as e:\n                plugin_status[\"status_error\"] = str(e)\n\n        status[\"plugins\"][name] = plugin_status\n\n    return status\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.initialize_plugin","title":"<code>initialize_plugin(plugin_name, config=None)</code>","text":"<p>Initialize a plugin with configuration</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def initialize_plugin(self, plugin_name: str, config: Optional[dict[str, Any]] = None) -&gt; bool:\n    \"\"\"Initialize a plugin with configuration\"\"\"\n    if plugin_name not in self.plugins:\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    # Ensure plugin is validated\n    if plugin_info.state &lt; PluginState.VALIDATED and not self.validate_plugin(plugin_name):\n        return False\n\n    try:\n        # Create plugin instance\n        plugin_class = plugin_info.instance\n        instance = plugin_class()\n\n        # Initialize with config\n        instance.initialize(config or {})\n\n        # Store instance\n        self._plugin_instances[plugin_name] = instance\n        plugin_info.state = PluginState.INITIALIZED\n\n        logger.info(f\"Initialized plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to initialize plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.load_config","title":"<code>load_config(config_path)</code>","text":"<p>Load plugin configuration from file</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def load_config(self, config_path: str) -&gt; dict[str, Any]:\n    \"\"\"Load plugin configuration from file\"\"\"\n    with open(config_path) as f:\n        return json.load(f)\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.load_plugin","title":"<code>load_plugin(plugin_name)</code>","text":"<p>Load a specific plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def load_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Load a specific plugin\"\"\"\n    if plugin_name not in self.plugins:\n        logger.error(f\"Plugin '{plugin_name}' not found\")\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    if plugin_info.state &gt;= PluginState.LOADED:\n        return True\n\n    try:\n        # Load the module\n        spec = importlib.util.spec_from_file_location(f\"plugin_{plugin_name}\", plugin_info.module_path)\n        if not spec or not spec.loader:\n            raise ImportError(f\"Failed to load spec for {plugin_name}\")\n\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n\n        # Get the plugin class\n        plugin_class = getattr(module, plugin_info.class_name)\n\n        # Store class reference\n        plugin_info.instance = plugin_class\n        plugin_info.state = PluginState.LOADED\n\n        logger.info(f\"Loaded plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to load plugin '{plugin_name}'\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.resolve_dependencies","title":"<code>resolve_dependencies(plugin_names)</code>","text":"<p>Resolve plugin dependencies and return load order</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def resolve_dependencies(self, plugin_names: list[str]) -&gt; list[str]:\n    \"\"\"Resolve plugin dependencies and return load order\"\"\"\n    # Build dependency graph\n    graph = {}\n    for name in plugin_names:\n        if name in self.plugins:\n            deps = self.plugins[name].metadata.dependencies\n            graph[name] = set(deps)\n\n    # Topological sort\n    result = []\n    visited = set()\n    temp_visited = set()\n\n    def visit(node):\n        if node in temp_visited:\n            raise ValueError(f\"Circular dependency detected: {node}\")\n        if node in visited:\n            return\n\n        temp_visited.add(node)\n        for dep in graph.get(node, set()):\n            visit(dep)\n        temp_visited.remove(node)\n        visited.add(node)\n        result.append(node)\n\n    for name in plugin_names:\n        if name not in visited:\n            visit(name)\n\n    return result\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.start_plugin","title":"<code>start_plugin(plugin_name)</code>","text":"<p>Start a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def start_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Start a plugin\"\"\"\n    if plugin_name not in self._plugin_instances:\n        logger.error(f\"Plugin '{plugin_name}' not initialized\")\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n    instance = self._plugin_instances[plugin_name]\n\n    try:\n        instance.start()\n        plugin_info.state = PluginState.ACTIVE\n        logger.info(f\"Started plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Failed to start plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.stop_plugin","title":"<code>stop_plugin(plugin_name)</code>","text":"<p>Stop a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def stop_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Stop a plugin\"\"\"\n    if plugin_name not in self._plugin_instances:\n        return True\n\n    plugin_info = self.plugins[plugin_name]\n    instance = self._plugin_instances[plugin_name]\n\n    try:\n        instance.stop()\n        plugin_info.state = PluginState.STOPPED\n        logger.info(f\"Stopped plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        logger.exception(f\"Error stopping plugin '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#tracelet.core.plugins.PluginManager.validate_plugin","title":"<code>validate_plugin(plugin_name)</code>","text":"<p>Validate a plugin</p> Source code in <code>tracelet/core/plugins.py</code> <pre><code>def validate_plugin(self, plugin_name: str) -&gt; bool:\n    \"\"\"Validate a plugin\"\"\"\n    if plugin_name not in self.plugins:\n        return False\n\n    plugin_info = self.plugins[plugin_name]\n\n    if plugin_info.state &lt; PluginState.LOADED and not self.load_plugin(plugin_name):\n        return False\n\n    try:\n        # Check required methods\n        required_methods = [\"get_metadata\", \"initialize\", \"start\", \"stop\", \"get_status\"]\n        plugin_class = plugin_info.instance\n\n        for method in required_methods:\n            if not hasattr(plugin_class, method):\n                raise ValueError(f\"Missing required method: {method}\")\n\n        # Validate metadata\n        metadata = plugin_class.get_metadata()\n        if not metadata.name or not metadata.version:\n            raise ValueError(\"Invalid metadata: name and version required\")\n\n        # Check dependencies\n        for dep in metadata.dependencies:\n            if dep not in self.plugins:\n                raise ValueError(f\"Missing dependency: {dep}\")\n\n        plugin_info.state = PluginState.VALIDATED\n        logger.info(f\"Validated plugin: {plugin_name}\")\n        return True\n\n    except Exception as e:\n        plugin_info.state = PluginState.ERROR\n        plugin_info.error = str(e)\n        logger.exception(f\"Plugin validation failed for '{plugin_name}': {e}\")\n        return False\n</code></pre>"},{"location":"api/core/plugins/#plugin-discovery-and-registration","title":"Plugin Discovery and Registration","text":"<pre><code>from tracelet.core.plugins import PluginManager\n\n# Initialize plugin manager\nplugin_manager = PluginManager()\n\n# Discover installed plugins\nplugin_manager.discover_plugins()\n\n# List available plugins\nbackend_plugins = plugin_manager.list_plugins(\"backend\")\ncollector_plugins = plugin_manager.list_plugins(\"collector\")\n\nprint(f\"Available backend plugins: {[p.name for p in backend_plugins]}\")\nprint(f\"Available collector plugins: {[p.name for p in collector_plugins]}\")\n</code></pre>"},{"location":"api/core/plugins/#manual-plugin-registration","title":"Manual Plugin Registration","text":"<pre><code>from my_package.plugins import CustomBackendPlugin\n\n# Register plugin manually\nplugin_manager.register_plugin(CustomBackendPlugin)\n\n# Get specific plugin\nplugin_class = plugin_manager.get_plugin(\"backend\", \"custom_backend\")\nif plugin_class:\n    plugin_instance = plugin_class()\n</code></pre>"},{"location":"api/core/plugins/#creating-custom-plugins","title":"Creating Custom Plugins","text":""},{"location":"api/core/plugins/#backend-plugin-example","title":"Backend Plugin Example","text":"<pre><code>from tracelet.core.plugins import BackendPlugin, PluginMetadata\nfrom typing import Dict, Any, Optional\n\nclass CustomBackendPlugin(BackendPlugin):\n    \"\"\"Custom backend for proprietary experiment tracking system.\"\"\"\n\n    def get_metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"custom_backend\",\n            version=\"1.0.0\",\n            description=\"Integration with Custom Tracking System\",\n            author=\"Your Team\",\n            plugin_type=\"backend\",\n            entry_point=\"custom_package.CustomBackendPlugin\",\n            dependencies=[\"custom-sdk&gt;=2.0.0\"],\n            config_schema={\n                \"server_url\": {\"type\": \"string\", \"required\": True},\n                \"project_id\": {\"type\": \"string\", \"required\": True},\n                \"api_token\": {\"type\": \"string\", \"required\": True}\n            }\n        )\n\n    def initialize(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Initialize the backend with configuration.\"\"\"\n        self.server_url = config[\"server_url\"]\n        self.project_id = config[\"project_id\"]\n        self.api_token = config[\"api_token\"]\n\n        # Initialize SDK\n        import custom_sdk\n        self.client = custom_sdk.Client(\n            url=self.server_url,\n            token=self.api_token\n        )\n\n        # Create or get project\n        self.project = self.client.get_project(self.project_id)\n        self.experiment = None\n\n    def start(self) -&gt; None:\n        \"\"\"Start experiment tracking.\"\"\"\n        self.experiment = self.project.create_experiment(\n            name=self.config.get(\"experiment_name\", \"Default Experiment\")\n        )\n        self.experiment.start()\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop experiment tracking.\"\"\"\n        if self.experiment:\n            self.experiment.finish()\n            self.experiment = None\n\n    def log_metric(self, name: str, value: float, iteration: Optional[int] = None) -&gt; None:\n        \"\"\"Log a metric value.\"\"\"\n        if self.experiment:\n            self.experiment.log_metric(\n                name=name,\n                value=value,\n                step=iteration\n            )\n\n    def log_params(self, params: Dict[str, Any]) -&gt; None:\n        \"\"\"Log parameters.\"\"\"\n        if self.experiment:\n            self.experiment.log_params(params)\n\n    def log_artifact(self, local_path: str, artifact_path: str) -&gt; None:\n        \"\"\"Log an artifact.\"\"\"\n        if self.experiment:\n            self.experiment.upload_artifact(local_path, artifact_path)\n\n    def get_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get backend status.\"\"\"\n        return {\n            \"backend_name\": \"custom_backend\",\n            \"connected\": self.experiment is not None,\n            \"experiment_id\": getattr(self.experiment, \"id\", None),\n            \"project_id\": self.project_id\n        }\n</code></pre>"},{"location":"api/core/plugins/#collector-plugin-example","title":"Collector Plugin Example","text":"<pre><code>from tracelet.core.plugins import CollectorPlugin, PluginMetadata\nimport psutil\nimport time\nfrom typing import Dict, Any\n\nclass AdvancedSystemCollectorPlugin(CollectorPlugin):\n    \"\"\"Advanced system metrics collector with custom metrics.\"\"\"\n\n    def get_metadata(self) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"advanced_system_collector\",\n            version=\"1.2.0\",\n            description=\"Advanced system metrics with process-level details\",\n            author=\"System Team\",\n            plugin_type=\"collector\",\n            entry_point=\"system_package.AdvancedSystemCollectorPlugin\",\n            dependencies=[\"psutil&gt;=5.8.0\", \"GPUtil&gt;=1.4.0\"],\n            config_schema={\n                \"include_processes\": {\"type\": \"boolean\", \"default\": False},\n                \"process_limit\": {\"type\": \"number\", \"default\": 10},\n                \"include_network\": {\"type\": \"boolean\", \"default\": True}\n            }\n        )\n\n    def initialize(self, config: Dict[str, Any]) -&gt; None:\n        \"\"\"Initialize collector with configuration.\"\"\"\n        self.include_processes = config.get(\"include_processes\", False)\n        self.process_limit = config.get(\"process_limit\", 10)\n        self.include_network = config.get(\"include_network\", True)\n        self.last_network = None\n\n    def start(self) -&gt; None:\n        \"\"\"Start collection (if background collection needed).\"\"\"\n        self.start_time = time.time()\n        if self.include_network:\n            self.last_network = psutil.net_io_counters()\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop collection.\"\"\"\n        pass\n\n    def collect(self) -&gt; Dict[str, Any]:\n        \"\"\"Collect system metrics.\"\"\"\n        metrics = {\n            \"timestamp\": time.time(),\n            \"uptime\": time.time() - self.start_time\n        }\n\n        # CPU metrics\n        cpu_percent = psutil.cpu_percent(interval=1)\n        cpu_count = psutil.cpu_count()\n        metrics.update({\n            \"cpu_percent\": cpu_percent,\n            \"cpu_count_logical\": cpu_count,\n            \"cpu_count_physical\": psutil.cpu_count(logical=False),\n            \"cpu_freq\": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else {}\n        })\n\n        # Memory metrics\n        memory = psutil.virtual_memory()\n        swap = psutil.swap_memory()\n        metrics.update({\n            \"memory_total\": memory.total,\n            \"memory_available\": memory.available,\n            \"memory_percent\": memory.percent,\n            \"swap_total\": swap.total,\n            \"swap_used\": swap.used,\n            \"swap_percent\": swap.percent\n        })\n\n        # Disk metrics\n        disk_usage = psutil.disk_usage('/')\n        disk_io = psutil.disk_io_counters()\n        metrics.update({\n            \"disk_total\": disk_usage.total,\n            \"disk_used\": disk_usage.used,\n            \"disk_percent\": disk_usage.percent,\n            \"disk_read_bytes\": disk_io.read_bytes if disk_io else 0,\n            \"disk_write_bytes\": disk_io.write_bytes if disk_io else 0\n        })\n\n        # Network metrics (with rate calculation)\n        if self.include_network:\n            current_network = psutil.net_io_counters()\n            if self.last_network:\n                time_delta = 1.0  # Approximate\n                metrics.update({\n                    \"network_bytes_sent\": current_network.bytes_sent,\n                    \"network_bytes_recv\": current_network.bytes_recv,\n                    \"network_bytes_sent_rate\": (current_network.bytes_sent - self.last_network.bytes_sent) / time_delta,\n                    \"network_bytes_recv_rate\": (current_network.bytes_recv - self.last_network.bytes_recv) / time_delta\n                })\n            self.last_network = current_network\n\n        # Process-level metrics\n        if self.include_processes:\n            processes = []\n            for proc in psutil.process_iter(['pid', 'name', 'cpu_percent', 'memory_percent']):\n                try:\n                    processes.append(proc.info)\n                except (psutil.NoSuchProcess, psutil.AccessDenied):\n                    pass\n\n            # Sort by CPU usage and take top N\n            processes.sort(key=lambda x: x['cpu_percent'] or 0, reverse=True)\n            metrics[\"top_processes\"] = processes[:self.process_limit]\n\n        # GPU metrics (if available)\n        try:\n            import GPUtil\n            gpus = GPUtil.getGPUs()\n            gpu_metrics = []\n            for gpu in gpus:\n                gpu_metrics.append({\n                    \"id\": gpu.id,\n                    \"name\": gpu.name,\n                    \"load\": gpu.load * 100,\n                    \"memory_util\": gpu.memoryUtil * 100,\n                    \"temperature\": gpu.temperature\n                })\n            metrics[\"gpu\"] = gpu_metrics\n        except ImportError:\n            pass\n\n        return metrics\n\n    def get_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get collector status.\"\"\"\n        return {\n            \"collector_name\": \"advanced_system_collector\",\n            \"active\": True,\n            \"config\": {\n                \"include_processes\": self.include_processes,\n                \"include_network\": self.include_network\n            }\n        }\n</code></pre>"},{"location":"api/core/plugins/#plugin-installation-and-packaging","title":"Plugin Installation and Packaging","text":""},{"location":"api/core/plugins/#setuptools-entry-points","title":"setuptools Entry Points","text":"<pre><code># setup.py for your plugin package\nfrom setuptools import setup, find_packages\n\nsetup(\n    name=\"tracelet-custom-plugins\",\n    version=\"1.0.0\",\n    packages=find_packages(),\n    install_requires=[\n        \"tracelet&gt;=1.0.0\",\n        \"psutil&gt;=5.8.0\"\n    ],\n    entry_points={\n        \"tracelet.plugins\": [\n            \"custom_backend = my_package.plugins:CustomBackendPlugin\",\n            \"advanced_system = my_package.collectors:AdvancedSystemCollectorPlugin\"\n        ]\n    }\n)\n</code></pre>"},{"location":"api/core/plugins/#plugin-discovery","title":"Plugin Discovery","text":"<pre><code># Tracelet automatically discovers plugins via entry points\nimport pkg_resources\n\ndef discover_plugins():\n    \"\"\"Discover installed Tracelet plugins.\"\"\"\n    plugins = {}\n\n    for entry_point in pkg_resources.iter_entry_points(\"tracelet.plugins\"):\n        try:\n            plugin_class = entry_point.load()\n            plugin_instance = plugin_class()\n            metadata = plugin_instance.get_metadata()\n            plugins[metadata.name] = {\n                \"class\": plugin_class,\n                \"metadata\": metadata,\n                \"entry_point\": entry_point\n            }\n        except Exception as e:\n            print(f\"Failed to load plugin {entry_point.name}: {e}\")\n\n    return plugins\n</code></pre>"},{"location":"api/core/plugins/#advanced-plugin-features","title":"Advanced Plugin Features","text":""},{"location":"api/core/plugins/#plugin-dependencies","title":"Plugin Dependencies","text":"<pre><code>from tracelet.core.plugins import PluginManager\nimport importlib\n\ndef check_plugin_dependencies(plugin_metadata):\n    \"\"\"Check if plugin dependencies are satisfied.\"\"\"\n    missing_deps = []\n\n    for dep in plugin_metadata.dependencies:\n        try:\n            # Parse dependency specification (name&gt;=version)\n            if \"&gt;=\" in dep:\n                package_name = dep.split(\"&gt;=\")[0]\n            else:\n                package_name = dep\n\n            importlib.import_module(package_name)\n        except ImportError:\n            missing_deps.append(dep)\n\n    return missing_deps\n\n# Usage\nplugin_manager = PluginManager()\nplugins = plugin_manager.discover_plugins()\n\nfor plugin_name, plugin_info in plugins.items():\n    missing = check_plugin_dependencies(plugin_info[\"metadata\"])\n    if missing:\n        print(f\"Plugin {plugin_name} missing dependencies: {missing}\")\n</code></pre>"},{"location":"api/core/plugins/#configuration-validation","title":"Configuration Validation","text":"<pre><code>import jsonschema\nfrom typing import Dict, Any\n\ndef validate_plugin_config(config: Dict[str, Any], schema: Dict[str, Any]) -&gt; bool:\n    \"\"\"Validate plugin configuration against schema.\"\"\"\n    try:\n        jsonschema.validate(config, schema)\n        return True\n    except jsonschema.ValidationError as e:\n        print(f\"Configuration validation failed: {e}\")\n        return False\n\n# Example usage in plugin\nclass ValidatedBackendPlugin(BackendPlugin):\n    def initialize(self, config: Dict[str, Any]) -&gt; None:\n        metadata = self.get_metadata()\n\n        if not validate_plugin_config(config, metadata.config_schema):\n            raise ValueError(\"Invalid plugin configuration\")\n\n        # Proceed with initialization\n        super().initialize(config)\n</code></pre>"},{"location":"api/core/plugins/#plugin-lifecycle-management","title":"Plugin Lifecycle Management","text":"<pre><code>from tracelet.core.plugins import PluginManager\nfrom typing import List\n\nclass PluginLifecycleManager:\n    \"\"\"Manages plugin lifecycle across experiment sessions.\"\"\"\n\n    def __init__(self):\n        self.plugin_manager = PluginManager()\n        self.active_plugins: List[PluginBase] = []\n\n    def start_plugins(self, plugin_configs: Dict[str, Dict[str, Any]]):\n        \"\"\"Start configured plugins.\"\"\"\n        for plugin_name, config in plugin_configs.items():\n            try:\n                plugin_class = self.plugin_manager.get_plugin_by_name(plugin_name)\n                if plugin_class:\n                    plugin_instance = plugin_class()\n                    plugin_instance.initialize(config)\n                    plugin_instance.start()\n                    self.active_plugins.append(plugin_instance)\n            except Exception as e:\n                print(f\"Failed to start plugin {plugin_name}: {e}\")\n\n    def stop_all_plugins(self):\n        \"\"\"Stop all active plugins.\"\"\"\n        for plugin in self.active_plugins:\n            try:\n                plugin.stop()\n            except Exception as e:\n                print(f\"Error stopping plugin: {e}\")\n\n        self.active_plugins.clear()\n\n    def get_plugin_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get status of all active plugins.\"\"\"\n        status = {}\n        for plugin in self.active_plugins:\n            try:\n                plugin_status = plugin.get_status()\n                status[plugin_status.get(\"name\", \"unknown\")] = plugin_status\n            except Exception as e:\n                status[\"error\"] = str(e)\n\n        return status\n</code></pre>"},{"location":"api/core/plugins/#best-practices","title":"Best Practices","text":""},{"location":"api/core/plugins/#plugin-development","title":"Plugin Development","text":"<ol> <li>Follow Interface Contracts: Implement all required methods from base classes</li> <li>Error Handling: Handle errors gracefully and provide meaningful error messages</li> <li>Configuration Validation: Validate configuration early and provide clear feedback</li> <li>Resource Management: Properly clean up resources in <code>stop()</code> method</li> <li>Documentation: Provide clear documentation and examples</li> </ol>"},{"location":"api/core/plugins/#plugin-usage","title":"Plugin Usage","text":"<ol> <li>Dependency Management: Check plugin dependencies before use</li> <li>Configuration Security: Don't expose sensitive information in configuration</li> <li>Performance: Monitor plugin performance impact on main application</li> <li>Testing: Test plugins in isolation and integration scenarios</li> <li>Version Compatibility: Ensure plugin versions are compatible with Tracelet core</li> </ol>"},{"location":"api/frameworks/pytorch/","title":"PyTorch Framework Integration","text":"<p>               Bases: <code>FrameworkInterface</code></p> <p>PyTorch framework integration that patches tensorboard for metric tracking</p> Source code in <code>tracelet/frameworks/pytorch.py</code> <pre><code>def __init__(self, patch_tensorboard: bool = True):\n    self._experiment = None\n    self._original_add_scalar = None\n    self._original_add_scalars = None\n    self._original_add_histogram = None\n    self._original_add_image = None\n    self._original_add_text = None\n    self._original_add_figure = None\n    self._original_add_embedding = None\n    self._original_add_video = None\n    self._original_add_audio = None\n    self._original_add_mesh = None\n    self._original_add_hparams = None\n    self._patch_tensorboard = patch_tensorboard\n    self._tensorboard_available = self._check_tensorboard()\n</code></pre> <p>options: show_source: true show_bases: true merge_init_into_class: true heading_level: 2</p>"},{"location":"api/frameworks/pytorch/#tracelet.frameworks.pytorch.PyTorchFramework.log_enhanced_metric","title":"<code>log_enhanced_metric(name, value, metric_type, iteration, metadata=None)</code>","text":"<p>Log an enhanced metric with specific type and metadata</p> Source code in <code>tracelet/frameworks/pytorch.py</code> <pre><code>def log_enhanced_metric(\n    self, name: str, value: Any, metric_type: MetricType, iteration: int, metadata: dict | None = None\n):\n    \"\"\"Log an enhanced metric with specific type and metadata\"\"\"\n    if self._experiment:\n        from ..core.orchestrator import MetricData\n\n        metric = MetricData(\n            name=name,\n            value=value,\n            type=metric_type,\n            iteration=iteration,\n            source=self._experiment.get_source_id(),\n            metadata=metadata or {},\n        )\n        self._experiment.emit_metric(metric)\n</code></pre>"},{"location":"api/frameworks/pytorch/#overview","title":"Overview","text":"<p>The PyTorch Framework integration provides seamless experiment tracking for PyTorch models with automatic TensorBoard interception.</p>"},{"location":"api/frameworks/pytorch/#key-features","title":"Key Features","text":"<ul> <li>Zero-Code Integration: Automatically captures <code>SummaryWriter.add_scalar()</code> calls</li> <li>TensorBoard Compatibility: Works with existing TensorBoard logging code</li> <li>Enhanced Metrics: Supports metadata and metric type classification</li> <li>Manual Logging: Direct metric logging without TensorBoard</li> </ul>"},{"location":"api/frameworks/pytorch/#basic-usage","title":"Basic Usage","text":""},{"location":"api/frameworks/pytorch/#automatic-tensorboard-interception","title":"Automatic TensorBoard Interception","text":"<pre><code>import torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\nimport tracelet\n\n# Start experiment with PyTorch framework\ntracelet.start_logging(\n    exp_name=\"pytorch_auto_demo\",\n    project=\"pytorch_examples\",\n    backend=\"mlflow\"\n)\n\n# Use TensorBoard as normal - metrics automatically captured!\nwriter = SummaryWriter()\n\n# Training loop\nmodel = nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\nfor epoch in range(100):\n    # Synthetic training step\n    data = torch.randn(32, 10)\n    target = torch.randn(32, 1)\n\n    optimizer.zero_grad()\n    output = model(data)\n    loss = criterion(output, target)\n    loss.backward()\n    optimizer.step()\n\n    # This gets automatically sent to MLflow!\n    writer.add_scalar('Loss/Train', loss.item(), epoch)\n    writer.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/frameworks/pytorch/#manual-framework-initialization","title":"Manual Framework Initialization","text":"<pre><code>from tracelet.frameworks.pytorch import PyTorchFramework\nfrom tracelet.core.experiment import Experiment\nimport tracelet\n\n# Manual framework setup\nexp = tracelet.start_logging(\n    exp_name=\"manual_pytorch\",\n    project=\"framework_demo\",\n    backend=\"mlflow\"\n)\n\n# Get the PyTorch framework instance\npytorch_framework = exp._frameworks.get(\"pytorch\")  # Internal access\n\n# Or initialize separately\nframework = PyTorchFramework(patch_tensorboard=True)\nframework.initialize(exp)\nframework.start_tracking()\n</code></pre>"},{"location":"api/frameworks/pytorch/#advanced-features","title":"Advanced Features","text":""},{"location":"api/frameworks/pytorch/#enhanced-metric-logging","title":"Enhanced Metric Logging","text":"<p>Log an enhanced metric with specific type and metadata</p> Source code in <code>tracelet/frameworks/pytorch.py</code> <pre><code>def log_enhanced_metric(\n    self, name: str, value: Any, metric_type: MetricType, iteration: int, metadata: dict | None = None\n):\n    \"\"\"Log an enhanced metric with specific type and metadata\"\"\"\n    if self._experiment:\n        from ..core.orchestrator import MetricData\n\n        metric = MetricData(\n            name=name,\n            value=value,\n            type=metric_type,\n            iteration=iteration,\n            source=self._experiment.get_source_id(),\n            metadata=metadata or {},\n        )\n        self._experiment.emit_metric(metric)\n</code></pre> <p>options: show_source: true heading_level: 3</p> <pre><code>import tracelet\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"enhanced_metrics\",\n    project=\"advanced_pytorch\",\n    backend=\"mlflow\"\n)\n\n# Get framework for enhanced logging\npytorch_framework = exp._frameworks[\"pytorch\"]\n\n# Log enhanced metrics with metadata\npytorch_framework.log_enhanced_metric(\n    name=\"validation_accuracy\",\n    value=0.95,\n    metric_type=\"accuracy\",\n    iteration=100,\n    metadata={\n        \"dataset\": \"validation\",\n        \"model_checkpoint\": \"epoch_100\",\n        \"data_split\": \"val\"\n    }\n)\n\npytorch_framework.log_enhanced_metric(\n    name=\"training_loss\",\n    value=0.1,\n    metric_type=\"loss\",\n    iteration=100,\n    metadata={\n        \"optimizer\": \"adam\",\n        \"learning_rate\": 0.001,\n        \"batch_size\": 32\n    }\n)\n</code></pre>"},{"location":"api/frameworks/pytorch/#multi-writer-support","title":"Multi-Writer Support","text":"<pre><code>import torch\nfrom torch.utils.tensorboard import SummaryWriter\nimport tracelet\n\n# Start experiment\ntracelet.start_logging(\n    exp_name=\"multi_writer\",\n    project=\"tensorboard_demo\",\n    backend=\"mlflow\"\n)\n\n# Multiple writers - all captured automatically\ntrain_writer = SummaryWriter(log_dir=\"runs/train\")\nval_writer = SummaryWriter(log_dir=\"runs/validation\")\n\nfor epoch in range(50):\n    # Training metrics\n    train_loss = 1.0 / (epoch + 1)  # Decreasing loss\n    train_writer.add_scalar(\"Loss\", train_loss, epoch)\n    train_writer.add_scalar(\"Accuracy\", min(0.9, epoch * 0.02), epoch)\n\n    # Validation metrics (every 5 epochs)\n    if epoch % 5 == 0:\n        val_loss = train_loss * 1.1\n        val_writer.add_scalar(\"Loss\", val_loss, epoch)\n        val_writer.add_scalar(\"Accuracy\", min(0.85, epoch * 0.018), epoch)\n\ntrain_writer.close()\nval_writer.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/frameworks/pytorch/#custom-metric-processing","title":"Custom Metric Processing","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass CustomMetricProcessor:\n    def __init__(self):\n        self.metric_history = {}\n\n    def process_metric(self, name, value, iteration):\n        \"\"\"Custom processing for specific metrics.\"\"\"\n        if name not in self.metric_history:\n            self.metric_history[name] = []\n\n        self.metric_history[name].append((iteration, value))\n\n        # Log smoothed version for loss metrics\n        if \"loss\" in name.lower():\n            if len(self.metric_history[name]) &gt;= 5:\n                recent_values = [v for _, v in self.metric_history[name][-5:]]\n                smoothed_value = sum(recent_values) / len(recent_values)\n\n                # Get active experiment and log smoothed metric\n                exp = tracelet.get_active_experiment()\n                if exp:\n                    exp.log_metric(f\"{name}_smoothed\", smoothed_value, iteration)\n\n# Usage\nprocessor = CustomMetricProcessor()\n\ntracelet.start_logging(\n    exp_name=\"custom_processing\",\n    project=\"advanced_features\",\n    backend=\"mlflow\"\n)\n\nwriter = SummaryWriter()\n\nfor epoch in range(100):\n    # Noisy loss simulation\n    import random\n    base_loss = 1.0 / (epoch + 1)\n    noisy_loss = base_loss + random.uniform(-0.1, 0.1)\n\n    # Log original (gets processed by custom processor)\n    writer.add_scalar(\"train_loss\", noisy_loss, epoch)\n    processor.process_metric(\"train_loss\", noisy_loss, epoch)\n\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"api/frameworks/pytorch/#integration-patterns","title":"Integration Patterns","text":""},{"location":"api/frameworks/pytorch/#pytorch-lightning-integration","title":"PyTorch Lightning Integration","text":"<pre><code>import pytorch_lightning as pl\nimport tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\nclass LightningModelWithTracelet(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Linear(28*28, 10)\n\n        # Start Tracelet experiment\n        self.experiment = tracelet.start_logging(\n            exp_name=\"lightning_integration\",\n            project=\"pytorch_lightning\",\n            backend=\"mlflow\"\n        )\n\n    def training_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self.layer(x)\n        loss = torch.nn.functional.cross_entropy(y_hat, y)\n\n        # Log with TensorBoard (automatically captured by Tracelet)\n        self.log('train_loss', loss)\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        x, y = batch\n        x = x.view(x.size(0), -1)\n        y_hat = self.layer(x)\n        loss = torch.nn.functional.cross_entropy(y_hat, y)\n\n        self.log('val_loss', loss)\n        return loss\n\n    def configure_optimizers(self):\n        return torch.optim.Adam(self.parameters(), lr=0.001)\n\n    def on_train_end(self):\n        # Clean up Tracelet\n        tracelet.stop_logging()\n</code></pre>"},{"location":"api/frameworks/pytorch/#distributed-training","title":"Distributed Training","text":"<pre><code>import torch\nimport torch.distributed as dist\nimport torch.multiprocessing as mp\nfrom torch.utils.tensorboard import SummaryWriter\nimport tracelet\n\ndef train_worker(rank, world_size):\n    # Initialize distributed training\n    dist.init_process_group(\"nccl\", rank=rank, world_size=world_size)\n\n    # Only rank 0 handles experiment tracking\n    if rank == 0:\n        tracelet.start_logging(\n            exp_name=\"distributed_training\",\n            project=\"multi_gpu\",\n            backend=\"mlflow\"\n        )\n        writer = SummaryWriter()\n\n    # Create model and move to GPU\n    model = torch.nn.Linear(1000, 10).cuda(rank)\n    model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n\n    for epoch in range(100):\n        # Training code here...\n        loss = torch.randn(1).cuda(rank)  # Simulated loss\n\n        # Gather losses from all ranks\n        gathered_losses = [torch.zeros_like(loss) for _ in range(world_size)]\n        dist.all_gather(gathered_losses, loss)\n\n        # Only rank 0 logs metrics\n        if rank == 0:\n            avg_loss = torch.stack(gathered_losses).mean().item()\n            writer.add_scalar(\"train_loss\", avg_loss, epoch)\n\n    if rank == 0:\n        writer.close()\n        tracelet.stop_logging()\n\n# Launch distributed training\ndef main():\n    world_size = torch.cuda.device_count()\n    mp.spawn(train_worker, args=(world_size,), nprocs=world_size)\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"api/frameworks/pytorch/#configuration-options","title":"Configuration Options","text":""},{"location":"api/frameworks/pytorch/#framework-settings","title":"Framework Settings","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Configure PyTorch framework behavior\nsettings = TraceletSettings(\n    project=\"pytorch_config\",\n    backend=[\"mlflow\"],\n    # Framework-specific settings would go here if supported\n)\n\ntracelet.start_logging(\n    exp_name=\"configured_pytorch\",\n    settings=settings\n)\n</code></pre>"},{"location":"api/frameworks/pytorch/#tensorboard-patch-control","title":"TensorBoard Patch Control","text":"<pre><code># Disable TensorBoard patching\nfrom tracelet.frameworks.pytorch import PyTorchFramework\n\nframework = PyTorchFramework(patch_tensorboard=False)\n\n# Manual metric logging only\nexp = tracelet.start_logging(\n    exp_name=\"manual_only\",\n    project=\"no_tensorboard\",\n    backend=\"mlflow\"\n)\n\n# Log metrics directly through framework\nframework.initialize(exp)\nframework.log_metric(\"manual_metric\", 0.5, iteration=1)\n</code></pre>"},{"location":"api/frameworks/pytorch/#error-handling","title":"Error Handling","text":""},{"location":"api/frameworks/pytorch/#tensorboard-import-issues","title":"TensorBoard Import Issues","text":"<pre><code>try:\n    import tracelet\n\n    tracelet.start_logging(\n        exp_name=\"safe_tensorboard\",\n        project=\"error_handling\",\n        backend=\"mlflow\"\n    )\n\n    from torch.utils.tensorboard import SummaryWriter\n    writer = SummaryWriter()\n\nexcept ImportError as e:\n    print(f\"TensorBoard not available: {e}\")\n    # Fall back to manual logging\n    exp = tracelet.get_active_experiment()\n    exp.log_metric(\"fallback_metric\", 1.0)\n</code></pre>"},{"location":"api/frameworks/pytorch/#framework-initialization-errors","title":"Framework Initialization Errors","text":"<pre><code>import tracelet\n\ntry:\n    exp = tracelet.start_logging(\n        exp_name=\"framework_error_test\",\n        project=\"error_handling\",\n        backend=\"mlflow\"\n    )\n\n    # Framework should initialize automatically\n    assert \"pytorch\" in exp._frameworks\n\nexcept Exception as e:\n    print(f\"Framework initialization failed: {e}\")\n    # Continue without framework features\n    exp.log_metric(\"basic_metric\", 1.0)\n</code></pre>"},{"location":"api/frameworks/pytorch/#best-practices","title":"Best Practices","text":""},{"location":"api/frameworks/pytorch/#metric-naming-conventions","title":"Metric Naming Conventions","text":"<pre><code># Use consistent naming patterns\nwriter.add_scalar(\"train/loss\", train_loss, epoch)\nwriter.add_scalar(\"train/accuracy\", train_acc, epoch)\nwriter.add_scalar(\"val/loss\", val_loss, epoch)\nwriter.add_scalar(\"val/accuracy\", val_acc, epoch)\nwriter.add_scalar(\"lr/learning_rate\", current_lr, epoch)\nwriter.add_scalar(\"gpu/memory_usage\", gpu_memory, epoch)\n</code></pre>"},{"location":"api/frameworks/pytorch/#resource-management","title":"Resource Management","text":"<pre><code>import atexit\nimport tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Ensure cleanup on exit\nexp = tracelet.start_logging(\n    exp_name=\"resource_managed\",\n    project=\"best_practices\",\n    backend=\"mlflow\"\n)\n\nwriter = SummaryWriter()\n\ndef cleanup():\n    writer.close()\n    tracelet.stop_logging()\n\natexit.register(cleanup)\n\ntry:\n    # Training code here\n    pass\nfinally:\n    cleanup()\n</code></pre>"},{"location":"api/frameworks/pytorch/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Batch metric logging for better performance\nimport tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\ntracelet.start_logging(\n    exp_name=\"optimized_logging\",\n    project=\"performance\",\n    backend=\"mlflow\"\n)\n\nwriter = SummaryWriter()\n\n# Log metrics less frequently for long training runs\nlog_interval = 10  # Log every 10 epochs\n\nfor epoch in range(1000):\n    # Training code...\n    train_loss = 1.0 / (epoch + 1)\n\n    if epoch % log_interval == 0:\n        writer.add_scalar(\"train_loss\", train_loss, epoch)\n        # Also log accumulated metrics\n        writer.add_scalar(\"avg_loss_10_epochs\", train_loss, epoch)\n\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/","title":"Backend Overview","text":"<p>Tracelet supports multiple experiment tracking backends, allowing you to choose the platform that best fits your needs.</p>"},{"location":"backends/#supported-backends","title":"Supported Backends","text":"Backend Type Hosting Best For MLflow Open Source Self/Cloud Traditional ML workflows ClearML Enterprise/SaaS SaaS/Self Enterprise MLOps Weights &amp; Biases SaaS/Open Source SaaS/Self Deep learning research AIM Open Source Self Lightweight tracking"},{"location":"backends/#choosing-a-backend","title":"Choosing a Backend","text":""},{"location":"backends/#mlflow","title":"MLflow","text":"<ul> <li>\u2705 Best for: Traditional ML, production deployments</li> <li>\u2705 Strengths: Model registry, serving, mature ecosystem</li> <li>\u274c Limitations: Basic visualization, manual setup</li> </ul>"},{"location":"backends/#clearml","title":"ClearML","text":"<ul> <li>\u2705 Best for: Enterprise teams, automated pipelines</li> <li>\u2705 Strengths: Rich UI, automatic logging, pipeline orchestration</li> <li>\u274c Limitations: Complex setup, resource intensive</li> </ul>"},{"location":"backends/#weights-biases","title":"Weights &amp; Biases","text":"<ul> <li>\u2705 Best for: Deep learning research, collaboration</li> <li>\u2705 Strengths: Best-in-class visualization, sharing, reports</li> <li>\u274c Limitations: SaaS dependency, pricing for teams</li> </ul>"},{"location":"backends/#aim","title":"AIM","text":"<ul> <li>\u2705 Best for: Simple tracking, local development</li> <li>\u2705 Strengths: Lightweight, fast queries, local-first</li> <li>\u274c Limitations: Fewer features, smaller ecosystem</li> </ul>"},{"location":"backends/#backend-comparison","title":"Backend Comparison","text":""},{"location":"backends/#feature-matrix","title":"Feature Matrix","text":"Feature MLflow ClearML W&amp;B AIM Metrics Logging \u2705 \u2705 \u2705 \u2705 Hyperparameters \u2705 \u2705 \u2705 \u2705 Artifacts \u2705 \u2705 \u2705 \u26a0\ufe0f Model Registry \u2705 \u2705 \u2705 \u274c Visualizations \u26a0\ufe0f \u2705 \u2705 \u2705 Collaboration \u26a0\ufe0f \u2705 \u2705 \u26a0\ufe0f Auto-logging \u26a0\ufe0f \u2705 \u2705 \u274c Pipeline Orchestration \u274c \u2705 \u26a0\ufe0f \u274c Self-hosting \u2705 \u2705 \u2705 \u2705 Free Tier \u2705 \u2705 \u2705 \u2705 <p>Legend: \u2705 Full support, \u26a0\ufe0f Limited support, \u274c Not supported</p>"},{"location":"backends/#multi-backend-support","title":"Multi-Backend Support","text":"<p>Use multiple backends simultaneously:</p> <pre><code>import tracelet\n\n# Log to both MLflow and W&amp;B\ntracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\"],\n    exp_name=\"multi_backend_experiment\",\n    project=\"comparison_study\"\n)\n\n# All metrics go to both platforms\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)  # \u2192 MLflow + W&amp;B\n</code></pre> <p>Benefits:</p> <ul> <li>Backup: Redundant logging prevents data loss</li> <li>Comparison: Evaluate different platform features</li> <li>Migration: Gradual transition between platforms</li> <li>Team preferences: Support different tool preferences</li> </ul>"},{"location":"backends/#performance-comparison","title":"Performance Comparison","text":"<p>Typical overhead per logged metric:</p> Backend Latency Memory Notes MLflow ~5ms Low Local file-based ClearML ~15ms Medium Rich automatic logging W&amp;B ~20ms Medium Network-dependent AIM ~2ms Low Optimized for speed"},{"location":"backends/#getting-started","title":"Getting Started","text":"<ol> <li>Choose your backend based on your needs</li> <li>Install dependencies for your chosen backend</li> <li>Configure authentication if using hosted services</li> <li>Start logging with a simple example</li> </ol> <p>Quick setup links:</p> <ul> <li>MLflow Installation \u2192</li> <li>ClearML Setup \u2192</li> <li>W&amp;B Setup \u2192</li> <li>AIM Installation \u2192</li> </ul>"},{"location":"backends/#migration-between-backends","title":"Migration Between Backends","text":"<p>Switching backends is easy with Tracelet:</p> <pre><code># Change from MLflow to W&amp;B\n# tracelet.start_logging(backend=\"mlflow\")  # Old\ntracelet.start_logging(backend=\"wandb\")     # New\n\n# Your existing TensorBoard code remains unchanged!\nwriter.add_scalar(\"loss\", loss_value, step)\n</code></pre> <p>See our Migration Guide for detailed instructions.</p>"},{"location":"backends/#next-steps","title":"Next Steps","text":"<ul> <li>Choose and configure your backend</li> <li>Try the multi-backend example</li> <li>Learn about advanced features</li> </ul>"},{"location":"backends/aim/","title":"AIM Backend","text":"<p>AIM is a lightweight, open-source experiment tracking system optimized for speed and simplicity.</p>"},{"location":"backends/aim/#overview","title":"Overview","text":"<p>AIM provides fast experiment tracking with a focus on performance and ease of use. It's perfect for:</p> <ul> <li>Local development and experimentation</li> <li>High-frequency metric logging</li> <li>Simple deployment scenarios</li> <li>Teams that prefer open-source solutions</li> </ul>"},{"location":"backends/aim/#installation","title":"Installation","text":"pip <p><code>bash     pip install tracelet aim</code></p> uv <p><code>bash     uv add tracelet aim</code></p>"},{"location":"backends/aim/#quick-start","title":"Quick Start","text":""},{"location":"backends/aim/#local-repository","title":"Local Repository","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start with local AIM repository\ntracelet.start_logging(\n    exp_name=\"aim_experiment\",\n    project=\"my_project\",\n    backend=\"aim\"\n)\n\n# Use TensorBoard as normal - metrics go to AIM\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)\nwriter.add_scalar(\"accuracy\", 0.9, 1)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/aim/#view-results","title":"View Results","text":"<p>Start the AIM UI:</p> <pre><code>aim up\n</code></pre> <p>Visit <code>http://localhost:43800</code> to view your experiments.</p> <p>!!! note \"AIM Ports\" - UI server (<code>aim up</code>): Default port <code>43800</code> - API server: Default port <code>53800</code></p>"},{"location":"backends/aim/#configuration","title":"Configuration","text":"<p>Current Limitation</p> <p>Backend-specific configuration is currently not supported. The AIM backend uses default settings: - Repository path: Current directory (<code>.</code>) - Experiment name: \"Tracelet Experiments\" - Remote server: Not supported</p> <pre><code>Advanced configuration will be added in a future release.\n</code></pre>"},{"location":"backends/aim/#current-usage","title":"Current Usage","text":"<pre><code># This works with default settings\ntracelet.start_logging(\n    backend=\"aim\",\n    exp_name=\"my_experiment\",     # Sets experiment run name\n    project=\"my_project\"          # Sets project context\n)\n</code></pre>"},{"location":"backends/aim/#planned-configuration-future-release","title":"Planned Configuration (Future Release)","text":"<pre><code># This will be supported in future versions\ntracelet.start_logging(\n    backend=\"aim\",\n    config={\n        \"repo_path\": \"./aim_repo\",           # Custom repository path\n        \"experiment_name\": \"My Experiments\", # Custom experiment name\n        \"run_name\": \"baseline_run\",         # Custom run name\n        \"tags\": {                           # Run tags\n            \"model\": \"resnet\",\n            \"dataset\": \"cifar10\"\n        },\n        \"remote_uri\": \"http://aim-server:53800\"  # Remote server\n    }\n)\n</code></pre>"},{"location":"backends/aim/#features","title":"Features","text":""},{"location":"backends/aim/#metrics-logging","title":"Metrics Logging","text":"<p>AIM automatically captures all TensorBoard metrics:</p> <pre><code>writer = SummaryWriter()\n\n# Scalars\nwriter.add_scalar(\"train/loss\", loss, epoch)\nwriter.add_scalar(\"val/accuracy\", acc, epoch)\n\n# Histograms (converted to AIM distributions)\nwriter.add_histogram(\"model/weights\", model.parameters(), epoch)\n\n# Images\nwriter.add_image(\"predictions\", image_tensor, epoch)\n\n# Text\nwriter.add_text(\"notes\", \"Training progressing well\", epoch)\n</code></pre>"},{"location":"backends/aim/#parameter-logging","title":"Parameter Logging","text":"<pre><code>exp = tracelet.get_active_experiment()\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"optimizer\": \"adam\",\n    \"model_architecture\": \"resnet50\"\n})\n</code></pre>"},{"location":"backends/aim/#artifact-storage","title":"Artifact Storage","text":"<pre><code># Log model artifacts\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\n\n# Log dataset info\nexp.log_artifact(\"data_stats.json\", artifact_path=\"data/\")\n</code></pre> <p>AIM Artifact Limitations</p> <p>AIM doesn't have full artifact storage like MLflow. Files are referenced by path rather than uploaded to a central store.</p>"},{"location":"backends/aim/#advanced-features","title":"Advanced Features","text":""},{"location":"backends/aim/#context-based-metrics","title":"Context-based Metrics","text":"<p>AIM supports rich metric contexts for better organization:</p> <pre><code># Metrics are automatically organized by source and name\nwriter.add_scalar(\"train/loss\", loss, step)      # Context: train\nwriter.add_scalar(\"val/loss\", val_loss, step)    # Context: val\n</code></pre>"},{"location":"backends/aim/#high-frequency-logging","title":"High-Frequency Logging","text":"<p>AIM is optimized for high-frequency metric logging:</p> <pre><code># Log every batch without performance concerns\nfor batch_idx, (data, target) in enumerate(dataloader):\n    # ... training code ...\n    writer.add_scalar(\"batch/loss\", batch_loss, batch_idx)\n    writer.add_scalar(\"batch/lr\", current_lr, batch_idx)\n</code></pre>"},{"location":"backends/aim/#multi-run-comparison","title":"Multi-Run Comparison","text":"<p>AIM's UI excels at comparing multiple runs:</p> <pre><code>for lr in [0.001, 0.01, 0.1]:\n    tracelet.start_logging(\n        backend=\"aim\",\n        exp_name=f\"lr_sweep_{lr}\",\n        config={\"tags\": {\"learning_rate\": lr}}\n    )\n    # ... training with this LR ...\n    tracelet.stop_logging()\n</code></pre>"},{"location":"backends/aim/#deployment-options","title":"Deployment Options","text":""},{"location":"backends/aim/#local-development","title":"Local Development","text":"<pre><code># Initialize repository\naim init\n\n# Start tracking server\naim up --host 0.0.0.0 --port 43800\n</code></pre>"},{"location":"backends/aim/#docker-deployment","title":"Docker Deployment","text":"<pre><code># docker-compose.yml\nversion: \"3.8\"\nservices:\n  aim:\n    image: aimhubio/aim:latest\n    ports:\n      - \"43800:43800\" # UI port\n      - \"53800:53800\" # API port\n    volumes:\n      - ./aim_data:/opt/aim\n    command: aim up --host 0.0.0.0 --port 43800\n</code></pre>"},{"location":"backends/aim/#production-server","title":"Production Server","text":"<pre><code># Install AIM server\npip install aim\n\n# Run UI server\naim up --host 0.0.0.0 --port 43800\n\n# Or run API server for remote connections\naim server --host 0.0.0.0 --port 53800\n</code></pre>"},{"location":"backends/aim/#best-practices","title":"Best Practices","text":""},{"location":"backends/aim/#repository-organization","title":"Repository Organization","text":"<pre><code># Current approach - organize by experiment name\ntracelet.start_logging(\n    backend=\"aim\",\n    exp_name=\"hyperparameter_tuning_resnet\",\n    project=\"computer_vision\"\n)\n\n# Future: Custom repo paths (not currently supported)\n# tracelet.start_logging(\n#     backend=\"aim\",\n#     config={\"repo_path\": \"./experiments/my_project\"}\n# )\n</code></pre>"},{"location":"backends/aim/#parameter-logging-strategy","title":"Parameter Logging Strategy","text":"<pre><code># Use structured parameter names for organization\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"model.architecture\": \"resnet50\",\n    \"model.layers\": 50,\n    \"data.dataset\": \"cifar10\",\n    \"training.stage\": \"development\",\n    \"training.version\": \"v1.2\",\n    \"optimizer.name\": \"adam\",\n    \"optimizer.lr\": 0.001\n})\n</code></pre>"},{"location":"backends/aim/#metric-naming","title":"Metric Naming","text":"<pre><code># Use hierarchical naming\nwriter.add_scalar(\"train/loss/total\", loss, step)\nwriter.add_scalar(\"train/loss/classification\", cls_loss, step)\nwriter.add_scalar(\"train/metrics/accuracy\", acc, step)\nwriter.add_scalar(\"val/metrics/f1_score\", f1, step)\n</code></pre>"},{"location":"backends/aim/#troubleshooting","title":"Troubleshooting","text":""},{"location":"backends/aim/#common-issues","title":"Common Issues","text":"<p>Repository not found:</p> <pre><code># Initialize AIM repository\naim init\n</code></pre> <p>Port already in use:</p> <pre><code># Use different port\naim up --port 43801\n</code></pre> <p>Remote connection failed: Currently not supported. Remote AIM server connections will be available in a future release when backend configuration is implemented.</p>"},{"location":"backends/aim/#performance-tuning","title":"Performance Tuning","text":"<pre><code># Current: AIM backend uses optimal defaults for performance\n# Repository is created in current directory on fast local storage\n\n# Future: Custom configuration will support\n# config = {\n#     \"repo_path\": \"/fast/ssd/aim_repo\",  # Use SSD storage\n#     \"buffer_size\": 1000,               # Batch metrics\n# }\n</code></pre>"},{"location":"backends/aim/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature AIM MLflow ClearML W&amp;B Setup complexity \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50 \u2b50\u2b50\u2b50 Logging performance \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50 Visualization quality \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Query capabilities \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Resource usage \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50"},{"location":"backends/aim/#migration","title":"Migration","text":""},{"location":"backends/aim/#from-tensorboard","title":"From TensorBoard","text":"<pre><code># Before: Pure TensorBoard\n# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter(\"./runs\")\n\n# After: TensorBoard + AIM via Tracelet\nimport tracelet\ntracelet.start_logging(backend=\"aim\")\nwriter = SummaryWriter()  # Same code!\n</code></pre>"},{"location":"backends/aim/#to-other-backends","title":"To Other Backends","text":"<pre><code># Easy switch to different backend\n# tracelet.start_logging(backend=\"aim\")      # Old\ntracelet.start_logging(backend=\"wandb\")     # New\n# All TensorBoard code remains unchanged\n</code></pre>"},{"location":"backends/aim/#complete-example","title":"Complete Example","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Start AIM tracking\ntracelet.start_logging(\n    exp_name=\"aim_pytorch_example\",\n    project=\"tutorials\",\n    backend=\"aim\"\n)\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Data\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32)\n\n# Training with automatic AIM logging\nwriter = SummaryWriter()\n\nfor epoch in range(10):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        # Metrics automatically sent to AIM\n        writer.add_scalar(\"batch/loss\", loss.item(),\n                         epoch * len(dataloader) + batch_idx)\n        total_loss += loss.item()\n\n    # Epoch metrics\n    avg_loss = total_loss / len(dataloader)\n    writer.add_scalar(\"epoch/loss\", avg_loss, epoch)\n\n    # Model weights histogram\n    for name, param in model.named_parameters():\n        writer.add_histogram(f\"weights/{name}\", param, epoch)\n\n# Log final parameters\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"model.type\": \"simple_mlp\",\n    \"model.input_size\": 784,\n    \"model.hidden_size\": 128,\n    \"model.output_size\": 10,\n    \"optimizer.type\": \"adam\",\n    \"optimizer.learning_rate\": 0.001,\n    \"training.batch_size\": 32,\n    \"training.epochs\": 10\n})\n\n# Cleanup\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Training completed! View results with: aim up\")\n</code></pre>"},{"location":"backends/aim/#next-steps","title":"Next Steps","text":"<ul> <li>View the multi-backend comparison example</li> <li>Learn about AIM's advanced querying features</li> <li>Set up remote AIM deployment</li> </ul>"},{"location":"backends/clearml/","title":"ClearML Backend","text":"<p>ClearML is an enterprise-grade MLOps platform that provides comprehensive experiment tracking, pipeline orchestration, and model management.</p>"},{"location":"backends/clearml/#overview","title":"Overview","text":"<p>ClearML offers powerful features for enterprise teams:</p> <ul> <li>Automatic experiment tracking and logging</li> <li>Rich visualization and comparison tools</li> <li>Pipeline orchestration and automation</li> <li>Model registry with versioning</li> <li>Resource management and scaling</li> <li>Comprehensive audit trails</li> </ul> <p>Perfect for enterprise MLOps and automated ML pipelines.</p>"},{"location":"backends/clearml/#installation","title":"Installation","text":"pip <p><code>bash     pip install tracelet[clearml]     # or     pip install tracelet clearml</code></p> uv <p><code>bash     uv add tracelet[clearml]     # or     uv add tracelet clearml</code></p> <p>Minimum Requirements:</p> <ul> <li>ClearML &gt;= 1.15.0</li> <li>Python &gt;= 3.9</li> </ul>"},{"location":"backends/clearml/#quick-start","title":"Quick Start","text":""},{"location":"backends/clearml/#saas-platform-recommended","title":"SaaS Platform (Recommended)","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start with ClearML SaaS (app.clear.ml)\ntracelet.start_logging(\n    exp_name=\"clearml_experiment\",\n    project=\"my_project\",\n    backend=\"clearml\"\n)\n\n# Use TensorBoard as normal - metrics go to ClearML\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)\nwriter.add_scalar(\"accuracy\", 0.9, 1)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/clearml/#view-results","title":"View Results","text":"<p>Visit app.clear.ml to view your experiments in the web UI.</p>"},{"location":"backends/clearml/#setup-and-authentication","title":"Setup and Authentication","text":""},{"location":"backends/clearml/#initial-setup","title":"Initial Setup","text":"<ol> <li>Create account at app.clear.ml</li> <li>Get credentials from Settings \u2192 Workspace \u2192 Create new credentials</li> <li>Configure ClearML:</li> </ol> <pre><code># Interactive setup (recommended)\nclearml-init\n\n# Manual configuration\nmkdir -p ~/.clearml\ncat &gt; ~/.clearml/clearml.conf &lt;&lt; EOF\napi {\n    web_server: https://app.clear.ml\n    api_server: https://api.clear.ml\n    files_server: https://files.clear.ml\n    credentials {\n        \"access_key\" = \"YOUR_ACCESS_KEY\"\n        \"secret_key\" = \"YOUR_SECRET_KEY\"\n    }\n}\nEOF\n</code></pre>"},{"location":"backends/clearml/#environment-variables","title":"Environment Variables","text":"<pre><code># ClearML SaaS configuration\nexport CLEARML_WEB_HOST=https://app.clear.ml\nexport CLEARML_API_HOST=https://api.clear.ml\nexport CLEARML_FILES_HOST=https://files.clear.ml\nexport CLEARML_API_ACCESS_KEY=your_access_key\nexport CLEARML_API_SECRET_KEY=your_secret_key\n\n# Optional: Project defaults\nexport CLEARML_PROJECT_NAME=\"Default Project\"\nexport CLEARML_TASK_NAME=\"Default Task\"\n</code></pre>"},{"location":"backends/clearml/#offline-mode","title":"Offline Mode","text":"<pre><code>import os\n\n# Enable offline mode for development\nos.environ[\"CLEARML_OFFLINE_MODE\"] = \"1\"\n\ntracelet.start_logging(\n    backend=\"clearml\",\n    exp_name=\"offline_experiment\",\n    project=\"development\"\n)\n</code></pre>"},{"location":"backends/clearml/#configuration","title":"Configuration","text":""},{"location":"backends/clearml/#basic-configuration","title":"Basic Configuration","text":"<pre><code>tracelet.start_logging(\n    backend=\"clearml\",\n    exp_name=\"my_experiment\",\n    project=\"computer_vision\",  # ClearML project name\n    config={\n        \"task_name\": \"resnet_training\",     # Custom task name\n        \"tags\": [\"pytorch\", \"baseline\"],    # Task tags\n        \"task_type\": \"training\",            # Task type\n        \"output_uri\": \"s3://my-bucket/\"     # Artifact storage\n    }\n)\n</code></pre>"},{"location":"backends/clearml/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Comprehensive setup\ntracelet.start_logging(\n    backend=\"clearml\",\n    exp_name=\"advanced_experiment\",\n    project=\"ml_research\",\n    config={\n        \"task_name\": \"hyperparameter_sweep\",\n        \"task_type\": \"training\",\n        \"tags\": [\"experiment\", \"sweep\", \"v2.0\"],\n        \"output_uri\": \"s3://clearml-artifacts/\",\n        \"auto_connect_frameworks\": True,     # Auto-detect frameworks\n        \"auto_connect_arg_parser\": True,     # Capture argparse\n        \"continue_last_task\": False,         # Create new task\n        \"reuse_last_task_id\": False\n    }\n)\n</code></pre>"},{"location":"backends/clearml/#features","title":"Features","text":""},{"location":"backends/clearml/#automatic-framework-detection","title":"Automatic Framework Detection","text":"<p>ClearML automatically detects and logs from popular frameworks:</p> <pre><code># These are automatically captured when detected:\n# - PyTorch models and hyperparameters\n# - TensorBoard metrics and plots\n# - Matplotlib figures\n# - Pandas DataFrames\n# - scikit-learn models\n# - Command-line arguments\n</code></pre>"},{"location":"backends/clearml/#manual-logging","title":"Manual Logging","text":"<pre><code>exp = tracelet.get_active_experiment()\n\n# Parameters\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"epochs\": 100,\n    \"optimizer\": \"adam\"\n})\n\n# Artifacts\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\nexp.log_artifact(\"config.yaml\")\n\n# Tables and reports\nimport pandas as pd\ndf = pd.DataFrame({\"metric\": [1, 2, 3], \"value\": [0.8, 0.9, 0.95]})\nexp.log_artifact(df, artifact_path=\"results.csv\")\n</code></pre>"},{"location":"backends/clearml/#rich-visualizations","title":"Rich Visualizations","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n\n# All these are automatically captured and enhanced in ClearML:\nwriter.add_scalar(\"train/loss\", loss, step)\nwriter.add_histogram(\"model/weights\", weights, step)\nwriter.add_image(\"predictions\", image_grid, step)\nwriter.add_text(\"notes\", training_notes, step)\n\n# ClearML adds automatic comparison and analysis tools\n</code></pre>"},{"location":"backends/clearml/#advanced-features","title":"Advanced Features","text":""},{"location":"backends/clearml/#pipeline-integration","title":"Pipeline Integration","text":"<pre><code># ClearML can orchestrate multi-step pipelines\nfrom clearml import Task\n\n# Parent pipeline task\npipeline_task = Task.init(\n    project_name=\"ML Pipeline\",\n    task_name=\"Data Processing Pipeline\"\n)\n\n# Child tasks for each step\nwith tracelet.logging(\"clearml\", exp_name=\"data_preprocessing\"):\n    # ... preprocessing code ...\n    pass\n\nwith tracelet.logging(\"clearml\", exp_name=\"model_training\"):\n    # ... training code ...\n    pass\n</code></pre>"},{"location":"backends/clearml/#hyperparameter-optimization","title":"Hyperparameter Optimization","text":"<pre><code># ClearML HyperParameter Optimization\nfrom clearml.automation import HyperParameterOptimizer\n\n# Define search space\noptimizer = HyperParameterOptimizer(\n    base_task_id=\"task_template_id\",\n    hyper_parameters={\n        \"learning_rate\": [0.001, 0.01, 0.1],\n        \"batch_size\": [16, 32, 64]\n    }\n)\n\n# Launch optimization\noptimizer.start()\n</code></pre>"},{"location":"backends/clearml/#model-registry","title":"Model Registry","text":"<pre><code># Register models automatically\nexp = tracelet.get_active_experiment()\n\n# Save model - automatically registered\ntorch.save(model.state_dict(), \"best_model.pth\")\nexp.log_artifact(\"best_model.pth\", artifact_path=\"models/\")\n\n# Add model metadata\nexp.log_params({\n    \"model.accuracy\": 0.95,\n    \"model.f1_score\": 0.93,\n    \"model.version\": \"v1.2\"\n})\n</code></pre>"},{"location":"backends/clearml/#deployment-options","title":"Deployment Options","text":""},{"location":"backends/clearml/#saas-platform-recommended_1","title":"SaaS Platform (Recommended)","text":"<pre><code># No setup required - just configure credentials\ntracelet.start_logging(backend=\"clearml\")\n</code></pre>"},{"location":"backends/clearml/#self-hosted-server","title":"Self-Hosted Server","text":"<pre><code># docker-compose.yml for ClearML Server\nversion: \"3.6\"\nservices:\n  clearml-server:\n    image: allegroai/clearml:latest\n    ports:\n      - \"8080:8080\" # Web UI\n      - \"8008:8008\" # API server\n      - \"8081:8081\" # File server\n    volumes:\n      - ./clearml-data:/opt/clearml/data\n    environment:\n      CLEARML_ELASTIC_SERVICE_HOST: elasticsearch\n      CLEARML_ELASTIC_SERVICE_PORT: 9200\n      CLEARML_REDIS_SERVICE_HOST: redis\n      CLEARML_REDIS_SERVICE_PORT: 6379\n      CLEARML_MONGODB_SERVICE_HOST: mongo\n      CLEARML_MONGODB_SERVICE_PORT: 27017\n\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:7.6.2\n    environment:\n      - discovery.type=single-node\n\n  redis:\n    image: redis:5.0-alpine\n\n  mongo:\n    image: mongo:3.6.5\n</code></pre>"},{"location":"backends/clearml/#best-practices","title":"Best Practices","text":""},{"location":"backends/clearml/#project-organization","title":"Project Organization","text":"<pre><code># Use hierarchical project names\ntracelet.start_logging(\n    backend=\"clearml\",\n    project=\"Computer Vision/Image Classification\",  # Nested projects\n    exp_name=\"resnet50_cifar10_baseline\"\n)\n</code></pre>"},{"location":"backends/clearml/#task-naming","title":"Task Naming","text":"<pre><code># Descriptive task names with versioning\ntracelet.start_logging(\n    backend=\"clearml\",\n    project=\"ML Research\",\n    exp_name=\"bert_fine_tuning_v2.1\",\n    config={\n        \"tags\": [\"bert\", \"nlp\", \"fine-tuning\", \"v2.1\"]\n    }\n)\n</code></pre>"},{"location":"backends/clearml/#artifact-management","title":"Artifact Management","text":"<pre><code># Organize artifacts by purpose\nexp = tracelet.get_active_experiment()\n\n# Models\nexp.log_artifact(\"model.pth\", artifact_path=\"models/final/\")\nexp.log_artifact(\"checkpoint.pth\", artifact_path=\"models/checkpoints/\")\n\n# Data\nexp.log_artifact(\"train_stats.json\", artifact_path=\"data/statistics/\")\nexp.log_artifact(\"predictions.csv\", artifact_path=\"results/predictions/\")\n\n# Configs\nexp.log_artifact(\"config.yaml\", artifact_path=\"configs/\")\n</code></pre>"},{"location":"backends/clearml/#troubleshooting","title":"Troubleshooting","text":""},{"location":"backends/clearml/#common-issues","title":"Common Issues","text":"<p>Authentication failure:</p> <pre><code># Re-run setup\nclearml-init\n\n# Or check credentials manually\ncat ~/.clearml/clearml.conf\n</code></pre> <p>Task not appearing in UI:</p> <pre><code># Ensure task is properly closed\ntracelet.stop_logging()\n\n# Check project name spelling\n# ClearML is case-sensitive\n</code></pre> <p>Large artifact upload fails:</p> <pre><code># Configure timeout for large files\nimport os\nos.environ[\"CLEARML_FILES_SERVER_TIMEOUT\"] = \"300\"  # 5 minutes\n</code></pre>"},{"location":"backends/clearml/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimize for high-frequency logging\nimport os\nos.environ[\"CLEARML_OFFLINE_MODE\"] = \"1\"  # For development\nos.environ[\"CLEARML_LOG_LEVEL\"] = \"WARNING\"  # Reduce verbosity\n</code></pre>"},{"location":"backends/clearml/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature ClearML MLflow W&amp;B AIM Auto-logging \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 Pipeline orchestration \u2b50\u2b50\u2b50 \u2b50 \u2b50\u2b50 \u2b50 Enterprise features \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 Visualization \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Setup complexity \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50"},{"location":"backends/clearml/#migration","title":"Migration","text":""},{"location":"backends/clearml/#from-tensorboard","title":"From TensorBoard","text":"<pre><code># Before: Pure TensorBoard\n# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter(\"./runs\")\n\n# After: TensorBoard + ClearML via Tracelet\nimport tracelet\ntracelet.start_logging(backend=\"clearml\")\nwriter = SummaryWriter()  # Same code, enhanced tracking!\n</code></pre>"},{"location":"backends/clearml/#from-mlflow","title":"From MLflow","text":"<pre><code># Change backend while keeping same code\n# tracelet.start_logging(backend=\"mlflow\")     # Old\ntracelet.start_logging(backend=\"clearml\")     # New\n# All TensorBoard code unchanged, gains ClearML features\n</code></pre>"},{"location":"backends/clearml/#complete-example","title":"Complete Example","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\nimport argparse\n\n# Command line arguments (automatically captured by ClearML)\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--lr\", type=float, default=0.001)\nparser.add_argument(\"--batch_size\", type=int, default=32)\nparser.add_argument(\"--epochs\", type=int, default=10)\nargs = parser.parse_args()\n\n# Start ClearML tracking\ntracelet.start_logging(\n    exp_name=\"clearml_pytorch_example\",\n    project=\"tutorials\",\n    backend=\"clearml\",\n    config={\n        \"tags\": [\"pytorch\", \"tutorial\", \"automated\"],\n        \"task_type\": \"training\"\n    }\n)\n\n# Model setup (automatically detected and logged)\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\noptimizer = optim.Adam(model.parameters(), lr=args.lr)\ncriterion = nn.CrossEntropyLoss()\n\n# Data\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=args.batch_size)\n\n# Training with automatic ClearML logging\nwriter = SummaryWriter()\n\nfor epoch in range(args.epochs):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        # Metrics automatically enhanced in ClearML\n        writer.add_scalar(\"batch/loss\", loss.item(),\n                         epoch * len(dataloader) + batch_idx)\n        total_loss += loss.item()\n\n    # Epoch metrics with automatic comparison\n    avg_loss = total_loss / len(dataloader)\n    writer.add_scalar(\"epoch/loss\", avg_loss, epoch)\n\n    # Model analysis (automatic histograms in ClearML)\n    for name, param in model.named_parameters():\n        writer.add_histogram(f\"weights/{name}\", param, epoch)\n\n# Manual parameter logging (supplements automatic detection)\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"model.type\": \"simple_mlp\",\n    \"model.architecture\": \"784-&gt;128-&gt;10\",\n    \"training.final_loss\": avg_loss,\n    \"training.total_batches\": len(dataloader) * args.epochs\n})\n\n# Save model (automatically tracked in model registry)\ntorch.save(model.state_dict(), \"model.pth\")\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\n\n# Save training metadata\nimport json\nmetadata = {\n    \"training_completed\": True,\n    \"final_epoch\": args.epochs,\n    \"total_parameters\": sum(p.numel() for p in model.parameters())\n}\nwith open(\"training_metadata.json\", \"w\") as f:\n    json.dump(metadata, f)\nexp.log_artifact(\"training_metadata.json\", artifact_path=\"metadata/\")\n\n# Cleanup\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Training completed! View results at: https://app.clear.ml\")\n</code></pre>"},{"location":"backends/clearml/#next-steps","title":"Next Steps","text":"<ul> <li>Explore ClearML's pipeline features</li> <li>Set up hyperparameter optimization</li> <li>Compare with other backends</li> </ul>"},{"location":"backends/mlflow/","title":"MLflow Backend","text":"<p>MLflow is an open-source platform for managing the ML lifecycle, including experimentation, reproducibility, deployment, and a central model registry.</p>"},{"location":"backends/mlflow/#overview","title":"Overview","text":"<p>MLflow provides comprehensive experiment tracking with:</p> <ul> <li>Local and remote tracking servers</li> <li>Model registry for versioning</li> <li>Artifact storage and management</li> <li>Experiment comparison and analysis</li> <li>Production deployment capabilities</li> </ul> <p>Perfect for traditional ML workflows and production environments.</p>"},{"location":"backends/mlflow/#installation","title":"Installation","text":"pip <p><code>bash     pip install tracelet[mlflow]     # or     pip install tracelet mlflow</code></p> uv <p><code>bash     uv add tracelet[mlflow]     # or     uv add tracelet mlflow</code></p> <p>Minimum Requirements:</p> <ul> <li>MLflow &gt;= 3.1.1</li> <li>Python &gt;= 3.9</li> </ul>"},{"location":"backends/mlflow/#quick-start","title":"Quick Start","text":""},{"location":"backends/mlflow/#local-tracking","title":"Local Tracking","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start with local MLflow tracking\ntracelet.start_logging(\n    exp_name=\"mlflow_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Use TensorBoard as normal - metrics go to MLflow\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)\nwriter.add_scalar(\"accuracy\", 0.9, 1)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/mlflow/#view-results","title":"View Results","text":"<p>Start the MLflow UI:</p> <pre><code>mlflow ui\n</code></pre> <p>Visit <code>http://localhost:5000</code> to view your experiments.</p>"},{"location":"backends/mlflow/#configuration","title":"Configuration","text":""},{"location":"backends/mlflow/#local-setup-default","title":"Local Setup (Default)","text":"<pre><code># Uses local mlruns directory\ntracelet.start_logging(\n    backend=\"mlflow\",\n    exp_name=\"local_experiment\",\n    project=\"my_project\"\n)\n</code></pre>"},{"location":"backends/mlflow/#remote-tracking-server","title":"Remote Tracking Server","text":"<pre><code>import os\n\n# Set tracking URI for remote server\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"http://mlflow-server:5000\"\n\ntracelet.start_logging(\n    backend=\"mlflow\",\n    exp_name=\"remote_experiment\",\n    project=\"distributed_training\"\n)\n</code></pre>"},{"location":"backends/mlflow/#environment-variables","title":"Environment Variables","text":"<pre><code># MLflow server configuration\nexport MLFLOW_TRACKING_URI=http://localhost:5000\nexport MLFLOW_EXPERIMENT_NAME=\"Default Experiment\"\n\n# Optional: Authentication for hosted MLflow\nexport MLFLOW_TRACKING_USERNAME=username\nexport MLFLOW_TRACKING_PASSWORD=password\nexport MLFLOW_TRACKING_TOKEN=your_token\n</code></pre>"},{"location":"backends/mlflow/#backend-specific-settings","title":"Backend-Specific Settings","text":"<pre><code># Available via TraceletSettings\nimport tracelet\n\ntracelet.start_logging(\n    backend=\"mlflow\",\n    exp_name=\"configured_experiment\",\n    config={\n        \"backend_url\": \"http://localhost:5000\",  # Custom tracking URI\n        \"experiment_name\": \"Custom Experiment\", # MLflow experiment name\n        \"run_name\": \"baseline_run\",             # Optional run name\n        \"tags\": {\"team\": \"ml\", \"version\": \"v1.0\"}\n    }\n)\n</code></pre>"},{"location":"backends/mlflow/#features","title":"Features","text":""},{"location":"backends/mlflow/#automatic-metric-logging","title":"Automatic Metric Logging","text":"<p>All TensorBoard calls are automatically captured:</p> <pre><code>writer = SummaryWriter()\n\n# Scalars\nwriter.add_scalar(\"train/loss\", loss, epoch)\nwriter.add_scalar(\"val/accuracy\", acc, epoch)\n\n# Histograms\nwriter.add_histogram(\"model/weights\", model.parameters(), epoch)\n\n# Images\nwriter.add_image(\"predictions\", image_tensor, epoch)\n\n# Text\nwriter.add_text(\"notes\", \"Training progressing well\", epoch)\n</code></pre>"},{"location":"backends/mlflow/#parameter-logging","title":"Parameter Logging","text":"<pre><code>exp = tracelet.get_active_experiment()\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 32,\n    \"optimizer\": \"adam\",\n    \"model_architecture\": \"resnet50\"\n})\n</code></pre>"},{"location":"backends/mlflow/#artifact-management","title":"Artifact Management","text":"<pre><code># Log model artifacts\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\n\n# Log entire directories\nexp.log_artifact(\"checkpoints/\", artifact_path=\"training/\")\n\n# Log config files\nexp.log_artifact(\"config.yaml\")\n</code></pre>"},{"location":"backends/mlflow/#model-registry-integration","title":"Model Registry Integration","text":"<pre><code>import mlflow.pytorch\n\n# Log model to registry\nmlflow.pytorch.log_model(\n    model,\n    \"model\",\n    registered_model_name=\"MyModel\"\n)\n</code></pre>"},{"location":"backends/mlflow/#advanced-features","title":"Advanced Features","text":""},{"location":"backends/mlflow/#experiment-organization","title":"Experiment Organization","text":"<pre><code># Organize experiments by project\ntracelet.start_logging(\n    backend=\"mlflow\",\n    project=\"computer_vision\",        # Sets experiment name\n    exp_name=\"resnet_baseline\",       # Sets run name\n    config={\n        \"tags\": {\n            \"model\": \"resnet50\",\n            \"dataset\": \"cifar10\"\n        }\n    }\n)\n</code></pre>"},{"location":"backends/mlflow/#multi-run-experiments","title":"Multi-Run Experiments","text":"<pre><code># Parameter sweep\nfor lr in [0.001, 0.01, 0.1]:\n    tracelet.start_logging(\n        backend=\"mlflow\",\n        exp_name=f\"lr_sweep_{lr}\",\n        config={\"tags\": {\"learning_rate\": lr}}\n    )\n    # ... training with this LR ...\n    tracelet.stop_logging()\n</code></pre>"},{"location":"backends/mlflow/#nested-runs","title":"Nested Runs","text":"<pre><code># Parent run for hyperparameter sweep\nwith mlflow.start_run(run_name=\"hyperparameter_sweep\"):\n    for params in param_grid:\n        with mlflow.start_run(run_name=f\"child_{params}\", nested=True):\n            tracelet.start_logging(backend=\"mlflow\")\n            # ... train with params ...\n            tracelet.stop_logging()\n</code></pre>"},{"location":"backends/mlflow/#deployment-options","title":"Deployment Options","text":""},{"location":"backends/mlflow/#local-development","title":"Local Development","text":"<pre><code># Basic UI\nmlflow ui\n\n# Custom port and host\nmlflow ui --host 0.0.0.0 --port 5001\n</code></pre>"},{"location":"backends/mlflow/#docker-deployment","title":"Docker Deployment","text":"<pre><code># docker-compose.yml\nversion: \"3.8\"\nservices:\n  mlflow:\n    image: python:3.11-slim\n    ports:\n      - \"5000:5000\"\n    volumes:\n      - ./mlruns:/mlflow/mlruns\n      - ./artifacts:/mlflow/artifacts\n    working_dir: /mlflow\n    command: |\n      sh -c \"\n        pip install mlflow==3.1.1 &amp;&amp;\n        mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root /mlflow/artifacts\n      \"\n</code></pre>"},{"location":"backends/mlflow/#production-server","title":"Production Server","text":"<pre><code># Install MLflow server\npip install mlflow\n\n# Start tracking server with artifact storage\nmlflow server \\\n  --host 0.0.0.0 \\\n  --port 5000 \\\n  --default-artifact-root s3://my-mlflow-bucket/artifacts \\\n  --backend-store-uri postgresql://user:password@postgres:5432/mlflow\n</code></pre>"},{"location":"backends/mlflow/#best-practices","title":"Best Practices","text":""},{"location":"backends/mlflow/#experiment-naming","title":"Experiment Naming","text":"<pre><code># Use hierarchical naming\ntracelet.start_logging(\n    backend=\"mlflow\",\n    project=\"image_classification\",  # Experiment name\n    exp_name=\"resnet50_baseline_v1\", # Run name\n)\n</code></pre>"},{"location":"backends/mlflow/#parameter-organization","title":"Parameter Organization","text":"<pre><code># Structure parameters by category\nexp.log_params({\n    \"model.architecture\": \"resnet50\",\n    \"model.layers\": 50,\n    \"data.dataset\": \"cifar10\",\n    \"data.augmentation\": True,\n    \"training.optimizer\": \"adam\",\n    \"training.learning_rate\": 0.001,\n    \"training.batch_size\": 32\n})\n</code></pre>"},{"location":"backends/mlflow/#metric-naming","title":"Metric Naming","text":"<pre><code># Use consistent metric naming\nwriter.add_scalar(\"train/loss/total\", total_loss, step)\nwriter.add_scalar(\"train/loss/classification\", cls_loss, step)\nwriter.add_scalar(\"train/metrics/accuracy\", acc, step)\nwriter.add_scalar(\"val/metrics/f1_score\", f1, step)\n</code></pre>"},{"location":"backends/mlflow/#artifact-organization","title":"Artifact Organization","text":"<pre><code># Organize artifacts by type\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\nexp.log_artifact(\"config.yaml\", artifact_path=\"configs/\")\nexp.log_artifact(\"results.json\", artifact_path=\"results/\")\n</code></pre>"},{"location":"backends/mlflow/#troubleshooting","title":"Troubleshooting","text":""},{"location":"backends/mlflow/#common-issues","title":"Common Issues","text":"<p>Connection refused:</p> <pre><code># Start MLflow server\nmlflow ui\n\n# Or check if server is running\ncurl http://localhost:5000/health\n</code></pre> <p>Permission denied on mlruns:</p> <pre><code># Fix permissions\nchmod -R 755 mlruns\n</code></pre> <p>Experiment already exists:</p> <pre><code># MLflow will reuse existing experiments by name\n# No action needed - this is expected behavior\n</code></pre>"},{"location":"backends/mlflow/#performance-tuning","title":"Performance Tuning","text":"<pre><code># For high-throughput logging\nexport MLFLOW_ENABLE_ASYNC_LOGGING=true\n\n# Optimize database backend\nmlflow server --backend-store-uri sqlite:///mlflow.db\n</code></pre>"},{"location":"backends/mlflow/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature MLflow ClearML W&amp;B AIM Setup complexity \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Model registry \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50 Artifact storage \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Visualization \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Production ready \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50"},{"location":"backends/mlflow/#migration","title":"Migration","text":""},{"location":"backends/mlflow/#from-tensorboard","title":"From TensorBoard","text":"<pre><code># Before: Pure TensorBoard\n# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter(\"./runs\")\n\n# After: TensorBoard + MLflow via Tracelet\nimport tracelet\ntracelet.start_logging(backend=\"mlflow\")\nwriter = SummaryWriter()  # Same code!\n</code></pre>"},{"location":"backends/mlflow/#to-other-backends","title":"To Other Backends","text":"<pre><code># Easy switch to different backend\n# tracelet.start_logging(backend=\"mlflow\")      # Old\ntracelet.start_logging(backend=\"wandb\")        # New\n# All TensorBoard code remains unchanged\n</code></pre>"},{"location":"backends/mlflow/#complete-example","title":"Complete Example","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Start MLflow tracking\ntracelet.start_logging(\n    exp_name=\"mlflow_pytorch_example\",\n    project=\"tutorials\",\n    backend=\"mlflow\"\n)\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Linear(128, 10)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Data\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32)\n\n# Training with automatic MLflow logging\nwriter = SummaryWriter()\n\nfor epoch in range(10):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        # Metrics automatically sent to MLflow\n        writer.add_scalar(\"batch/loss\", loss.item(),\n                         epoch * len(dataloader) + batch_idx)\n        total_loss += loss.item()\n\n    # Epoch metrics\n    avg_loss = total_loss / len(dataloader)\n    writer.add_scalar(\"epoch/loss\", avg_loss, epoch)\n\n    # Model weights histogram\n    for name, param in model.named_parameters():\n        writer.add_histogram(f\"weights/{name}\", param, epoch)\n\n# Log final parameters\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"model.type\": \"simple_mlp\",\n    \"model.input_size\": 784,\n    \"model.hidden_size\": 128,\n    \"model.output_size\": 10,\n    \"optimizer.type\": \"adam\",\n    \"optimizer.learning_rate\": 0.001,\n    \"training.batch_size\": 32,\n    \"training.epochs\": 10\n})\n\n# Save model artifact\ntorch.save(model.state_dict(), \"model.pth\")\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\n\n# Cleanup\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Training completed! View results with: mlflow ui\")\n</code></pre>"},{"location":"backends/mlflow/#next-steps","title":"Next Steps","text":"<ul> <li>Compare with other backends</li> <li>Learn about MLflow Model Registry</li> <li>Set up MLflow in production</li> </ul>"},{"location":"backends/multi-backend/","title":"Multi-Backend Usage","text":"<p>Tracelet supports logging to multiple experiment tracking backends simultaneously, allowing you to leverage the strengths of different platforms or maintain backups across systems.</p>"},{"location":"backends/multi-backend/#overview","title":"Overview","text":"<p>Multi-backend logging enables you to:</p> <ul> <li>Compare platforms side-by-side with identical experiments</li> <li>Maintain backups across different tracking systems</li> <li>Support team preferences where different members prefer different tools</li> <li>Gradual migration from one platform to another</li> <li>Leverage unique features of each platform simultaneously</li> </ul>"},{"location":"backends/multi-backend/#basic-multi-backend-setup","title":"Basic Multi-Backend Setup","text":""},{"location":"backends/multi-backend/#simultaneous-logging","title":"Simultaneous Logging","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Log to multiple backends at once\ntracelet.start_logging(\n    exp_name=\"multi_backend_experiment\",\n    project=\"platform_comparison\",\n    backend=[\"mlflow\", \"wandb\"]  # List of backends\n)\n\n# All metrics automatically go to both platforms\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)\nwriter.add_scalar(\"accuracy\", 0.9, 1)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/multi-backend/#three-way-comparison","title":"Three-Way Comparison","text":"<pre><code># Log to three backends simultaneously\ntracelet.start_logging(\n    exp_name=\"comprehensive_comparison\",\n    project=\"backend_evaluation\",\n    backend=[\"mlflow\", \"clearml\", \"wandb\"]\n)\n\n# Single codebase, three tracking platforms\nwriter = SummaryWriter()\nfor epoch in range(10):\n    loss = train_one_epoch()\n    writer.add_scalar(\"train/loss\", loss, epoch)\n    # \u2192 Goes to MLflow, ClearML, and W&amp;B simultaneously\n</code></pre>"},{"location":"backends/multi-backend/#backend-specific-configuration","title":"Backend-Specific Configuration","text":""},{"location":"backends/multi-backend/#individual-backend-settings","title":"Individual Backend Settings","text":"<pre><code># Configure each backend independently\ntracelet.start_logging(\n    exp_name=\"configured_multi_backend\",\n    project=\"custom_setup\",\n    backend=[\"mlflow\", \"wandb\", \"clearml\"],\n    config={\n        \"mlflow\": {\n            \"backend_url\": \"http://mlflow-server:5000\",\n            \"experiment_name\": \"Production Experiments\"\n        },\n        \"wandb\": {\n            \"entity\": \"research_team\",\n            \"tags\": [\"production\", \"comparison\"],\n            \"group\": \"multi_backend_runs\"\n        },\n        \"clearml\": {\n            \"task_name\": \"Multi-Backend Training\",\n            \"tags\": [\"mlflow\", \"wandb\", \"comparison\"],\n            \"output_uri\": \"s3://artifacts-bucket/\"\n        }\n    }\n)\n</code></pre>"},{"location":"backends/multi-backend/#environment-based-configuration","title":"Environment-Based Configuration","text":"<pre><code>import os\n\n# Set different configurations via environment\nos.environ[\"MLFLOW_TRACKING_URI\"] = \"http://localhost:5000\"\nos.environ[\"WANDB_PROJECT\"] = \"multi_backend_project\"\nos.environ[\"CLEARML_PROJECT_NAME\"] = \"Multi Platform Testing\"\n\n# Backends will use their respective environment configs\ntracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\", \"clearml\"],\n    exp_name=\"env_configured_experiment\"\n)\n</code></pre>"},{"location":"backends/multi-backend/#platform-comparison-example","title":"Platform Comparison Example","text":""},{"location":"backends/multi-backend/#complete-multi-backend-training","title":"Complete Multi-Backend Training","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\nimport time\n\ndef run_multi_backend_experiment():\n    \"\"\"Run the same experiment across multiple backends for comparison.\"\"\"\n\n    # Start logging to all available backends\n    exp = tracelet.start_logging(\n        exp_name=\"multi_backend_comparison\",\n        project=\"platform_evaluation\",\n        backend=[\"mlflow\", \"wandb\", \"clearml\", \"aim\"],\n        config={\n            \"wandb\": {\n                \"tags\": [\"comparison\", \"multi-backend\"],\n                \"notes\": \"Comparing tracking platforms\"\n            },\n            \"clearml\": {\n                \"tags\": [\"comparison\", \"evaluation\"],\n                \"task_type\": \"training\"\n            }\n        }\n    )\n\n    # Model and data setup\n    model = nn.Sequential(\n        nn.Linear(100, 64),\n        nn.ReLU(),\n        nn.Dropout(0.1),\n        nn.Linear(64, 32),\n        nn.ReLU(),\n        nn.Linear(32, 1)\n    )\n\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n\n    # Synthetic regression data\n    X = torch.randn(1000, 100)\n    y = torch.randn(1000, 1)\n    dataset = TensorDataset(X, y)\n    dataloader = DataLoader(dataset, batch_size=32)\n\n    # Log hyperparameters to all backends\n    exp.log_params({\n        \"model.architecture\": \"3-layer MLP\",\n        \"model.input_size\": 100,\n        \"model.hidden_sizes\": [64, 32],\n        \"model.dropout\": 0.1,\n        \"optimizer.type\": \"adam\",\n        \"optimizer.lr\": 0.001,\n        \"data.batch_size\": 32,\n        \"data.total_samples\": 1000\n    })\n\n    # Training loop with comprehensive logging\n    writer = SummaryWriter()\n    start_time = time.time()\n\n    for epoch in range(25):\n        epoch_loss = 0\n        for batch_idx, (data, target) in enumerate(dataloader):\n            optimizer.zero_grad()\n            output = model(data)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            # Batch-level metrics\n            global_step = epoch * len(dataloader) + batch_idx\n            writer.add_scalar(\"batch/loss\", loss.item(), global_step)\n            epoch_loss += loss.item()\n\n        # Epoch-level metrics\n        avg_loss = epoch_loss / len(dataloader)\n        writer.add_scalar(\"epoch/loss\", avg_loss, epoch)\n        writer.add_scalar(\"epoch/learning_rate\", optimizer.param_groups[0]['lr'], epoch)\n\n        # Model analysis every 5 epochs\n        if epoch % 5 == 0:\n            for name, param in model.named_parameters():\n                writer.add_histogram(f\"weights/{name}\", param, epoch)\n                if param.grad is not None:\n                    writer.add_histogram(f\"gradients/{name}\", param.grad, epoch)\n\n        print(f\"Epoch {epoch:2d}/24: Loss = {avg_loss:.4f}\")\n\n    training_time = time.time() - start_time\n\n    # Log final metrics\n    exp.log_params({\n        \"training.final_loss\": avg_loss,\n        \"training.total_time\": training_time,\n        \"training.epochs\": 25,\n        \"training.batches_per_epoch\": len(dataloader)\n    })\n\n    # Save artifacts\n    torch.save(model.state_dict(), \"multi_backend_model.pth\")\n    exp.log_artifact(\"multi_backend_model.pth\", artifact_path=\"models/\")\n\n    # Create training summary\n    summary = {\n        \"experiment\": \"multi_backend_comparison\",\n        \"backends\": [\"mlflow\", \"wandb\", \"clearml\", \"aim\"],\n        \"final_loss\": avg_loss,\n        \"training_time_seconds\": training_time,\n        \"model_parameters\": sum(p.numel() for p in model.parameters())\n    }\n\n    import json\n    with open(\"experiment_summary.json\", \"w\") as f:\n        json.dump(summary, f, indent=2)\n    exp.log_artifact(\"experiment_summary.json\")\n\n    writer.close()\n    tracelet.stop_logging()\n\n    return {\n        \"final_loss\": avg_loss,\n        \"training_time\": training_time,\n        \"backends_used\": [\"mlflow\", \"wandb\", \"clearml\", \"aim\"]\n    }\n\n# Run the experiment\nif __name__ == \"__main__\":\n    results = run_multi_backend_experiment()\n    print(f\"\u2705 Multi-backend experiment completed!\")\n    print(f\"\ud83d\udcca Final loss: {results['final_loss']:.4f}\")\n    print(f\"\u23f1\ufe0f  Training time: {results['training_time']:.2f}s\")\n    print(f\"\ud83d\udd04 Backends: {', '.join(results['backends_used'])}\")\n</code></pre>"},{"location":"backends/multi-backend/#backend-specific-features","title":"Backend-Specific Features","text":""},{"location":"backends/multi-backend/#leveraging-unique-capabilities","title":"Leveraging Unique Capabilities","text":"<pre><code># Start multi-backend logging\ntracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\", \"clearml\"],\n    exp_name=\"feature_showcase\"\n)\n\n# Use each platform's strengths:\n\n# 1. MLflow: Model registry and serving\nimport mlflow.pytorch\nmlflow.pytorch.log_model(model, \"model\", registered_model_name=\"ProductionModel\")\n\n# 2. W&amp;B: Interactive plots and sweeps\nimport wandb\nwandb.log({\"custom_plot\": wandb.plot.line_series(\n    xs=epochs,\n    ys=[train_losses, val_losses],\n    keys=[\"train\", \"val\"],\n    title=\"Loss Comparison\"\n)})\n\n# 3. ClearML: Automatic framework detection\n# (Automatically captures more context and metadata)\n\n# All platforms still get the same core metrics via Tracelet\nwriter.add_scalar(\"shared_metric\", value, step)\n</code></pre>"},{"location":"backends/multi-backend/#performance-considerations","title":"Performance Considerations","text":""},{"location":"backends/multi-backend/#overhead-analysis","title":"Overhead Analysis","text":"<pre><code>import time\n\ndef measure_logging_overhead():\n    \"\"\"Compare single vs multi-backend performance.\"\"\"\n\n    metrics = []\n\n    # Single backend\n    start = time.time()\n    tracelet.start_logging(backend=\"mlflow\", exp_name=\"single_backend_test\")\n\n    writer = SummaryWriter()\n    for i in range(1000):\n        writer.add_scalar(\"test_metric\", i * 0.001, i)\n\n    tracelet.stop_logging()\n    single_time = time.time() - start\n\n    # Multi-backend\n    start = time.time()\n    tracelet.start_logging(\n        backend=[\"mlflow\", \"wandb\", \"clearml\"],\n        exp_name=\"multi_backend_test\"\n    )\n\n    writer = SummaryWriter()\n    for i in range(1000):\n        writer.add_scalar(\"test_metric\", i * 0.001, i)\n\n    tracelet.stop_logging()\n    multi_time = time.time() - start\n\n    print(f\"Single backend: {single_time:.2f}s\")\n    print(f\"Multi backend:  {multi_time:.2f}s\")\n    print(f\"Overhead:       {multi_time - single_time:.2f}s ({((multi_time/single_time - 1) * 100):.1f}%)\")\n</code></pre>"},{"location":"backends/multi-backend/#optimization-strategies","title":"Optimization Strategies","text":"<pre><code># Optimize multi-backend performance\nimport os\n\n# Reduce verbosity\nos.environ[\"WANDB_SILENT\"] = \"true\"\nos.environ[\"CLEARML_LOG_LEVEL\"] = \"WARNING\"\n\n# Use offline modes for development\nos.environ[\"WANDB_MODE\"] = \"offline\"\nos.environ[\"CLEARML_OFFLINE_MODE\"] = \"1\"\n\n# Batch metrics for efficiency\nos.environ[\"WANDB_LOG_INTERVAL_SECONDS\"] = \"10\"\n</code></pre>"},{"location":"backends/multi-backend/#selective-backend-usage","title":"Selective Backend Usage","text":""},{"location":"backends/multi-backend/#conditional-backend-selection","title":"Conditional Backend Selection","text":"<pre><code>import os\n\n# Choose backends based on environment\ndef get_backends_for_env():\n    env = os.environ.get(\"ENVIRONMENT\", \"development\")\n\n    if env == \"development\":\n        return [\"mlflow\"]  # Local only\n    elif env == \"staging\":\n        return [\"mlflow\", \"wandb\"]  # Add visualization\n    elif env == \"production\":\n        return [\"mlflow\", \"wandb\", \"clearml\"]  # Full tracking\n    else:\n        return [\"mlflow\"]  # Default fallback\n\nbackends = get_backends_for_env()\ntracelet.start_logging(backend=backends, exp_name=\"env_aware_experiment\")\n</code></pre>"},{"location":"backends/multi-backend/#feature-based-selection","title":"Feature-Based Selection","text":"<pre><code>def select_backends_by_features(need_visualization=True, need_collaboration=True, need_artifacts=True):\n    \"\"\"Select backends based on required features.\"\"\"\n    backends = [\"mlflow\"]  # Always include MLflow as base\n\n    if need_visualization:\n        backends.append(\"wandb\")  # Best visualizations\n\n    if need_collaboration:\n        backends.append(\"clearml\")  # Team features\n\n    if need_artifacts and \"wandb\" not in backends:\n        backends.append(\"wandb\")  # Good artifact management\n\n    return backends\n\n# Use based on experiment needs\nexperiment_backends = select_backends_by_features(\n    need_visualization=True,\n    need_collaboration=False,\n    need_artifacts=True\n)\n\ntracelet.start_logging(backend=experiment_backends, exp_name=\"feature_selected_experiment\")\n</code></pre>"},{"location":"backends/multi-backend/#migration-workflows","title":"Migration Workflows","text":""},{"location":"backends/multi-backend/#gradual-platform-migration","title":"Gradual Platform Migration","text":"<pre><code># Phase 1: Run both old and new platforms\ntracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\"],  # Old + New\n    exp_name=\"migration_phase_1\"\n)\n\n# Phase 2: Compare results and validate new platform\n# (Run experiments on both, verify data consistency)\n\n# Phase 3: Switch to new platform only\ntracelet.start_logging(\n    backend=[\"wandb\"],  # New only\n    exp_name=\"migration_complete\"\n)\n</code></pre>"},{"location":"backends/multi-backend/#ab-testing-platforms","title":"A/B Testing Platforms","text":"<pre><code>import random\n\n# Randomly assign experiments to different backends for comparison\nbackend_choice = random.choice([\n    [\"mlflow\"],\n    [\"wandb\"],\n    [\"clearml\"],\n    [\"mlflow\", \"wandb\"]  # Multi-backend group\n])\n\ntracelet.start_logging(\n    backend=backend_choice,\n    exp_name=f\"platform_ab_test_{hash(str(backend_choice)) % 1000}\",\n    config={\"tags\": [f\"backend_test_{len(backend_choice)}_platforms\"]}\n)\n</code></pre>"},{"location":"backends/multi-backend/#best-practices","title":"Best Practices","text":""},{"location":"backends/multi-backend/#naming-conventions","title":"Naming Conventions","text":"<pre><code># Use consistent naming across all backends\ntracelet.start_logging(\n    backend=[\"mlflow\", \"wandb\", \"clearml\"],\n    exp_name=\"resnet50_cifar10_v2.1\",  # Version in name\n    project=\"computer_vision_research\", # Consistent project\n    config={\n        \"shared_tags\": [\"resnet\", \"cifar10\", \"v2.1\", \"multi_backend\"],\n        \"mlflow\": {\"experiment_name\": \"CV Research\"},\n        \"wandb\": {\"group\": \"resnet_experiments\"},\n        \"clearml\": {\"task_type\": \"training\"}\n    }\n)\n</code></pre>"},{"location":"backends/multi-backend/#configuration-management","title":"Configuration Management","text":"<pre><code># Centralized configuration for multi-backend setups\nBACKEND_CONFIGS = {\n    \"development\": {\n        \"backends\": [\"mlflow\"],\n        \"config\": {\n            \"mlflow\": {\"backend_url\": \"sqlite:///dev.db\"}\n        }\n    },\n    \"staging\": {\n        \"backends\": [\"mlflow\", \"wandb\"],\n        \"config\": {\n            \"mlflow\": {\"backend_url\": \"http://staging-mlflow:5000\"},\n            \"wandb\": {\"mode\": \"offline\"}\n        }\n    },\n    \"production\": {\n        \"backends\": [\"mlflow\", \"wandb\", \"clearml\"],\n        \"config\": {\n            \"mlflow\": {\"backend_url\": \"http://prod-mlflow:5000\"},\n            \"wandb\": {\"entity\": \"production_team\"},\n            \"clearml\": {\"output_uri\": \"s3://prod-artifacts/\"}\n        }\n    }\n}\n\n# Use configuration\nenv = os.environ.get(\"ENVIRONMENT\", \"development\")\nconfig = BACKEND_CONFIGS[env]\n\ntracelet.start_logging(\n    backend=config[\"backends\"],\n    exp_name=\"configured_experiment\",\n    config=config[\"config\"]\n)\n</code></pre>"},{"location":"backends/multi-backend/#troubleshooting-multi-backend-issues","title":"Troubleshooting Multi-Backend Issues","text":""},{"location":"backends/multi-backend/#common-problems","title":"Common Problems","text":"<pre><code># Handle backend-specific failures gracefully\ntry:\n    tracelet.start_logging(\n        backend=[\"mlflow\", \"wandb\", \"clearml\", \"aim\"],\n        exp_name=\"robust_experiment\"\n    )\nexcept Exception as e:\n    print(f\"Some backends failed to initialize: {e}\")\n    # Fallback to working backends\n    tracelet.start_logging(\n        backend=[\"mlflow\"],  # Reliable fallback\n        exp_name=\"fallback_experiment\"\n    )\n</code></pre>"},{"location":"backends/multi-backend/#debugging-backend-issues","title":"Debugging Backend Issues","text":"<pre><code># Enable detailed logging for debugging\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Check which backends are active\nexp = tracelet.get_active_experiment()\nif hasattr(exp, 'get_active_backends'):\n    active_backends = exp.get_active_backends()\n    print(f\"Active backends: {active_backends}\")\n</code></pre>"},{"location":"backends/multi-backend/#results-comparison","title":"Results Comparison","text":"<p>After running multi-backend experiments, compare results across platforms:</p> <ol> <li>MLflow: Check <code>http://localhost:5000</code> for model registry and basic metrics</li> <li>W&amp;B: Visit wandb.ai for interactive visualizations and collaboration</li> <li>ClearML: Check app.clear.ml for comprehensive experiment analysis</li> <li>AIM: Run <code>aim up</code> and visit <code>http://localhost:43800</code> for fast querying</li> </ol> <p>Each platform will show the same core metrics but with their unique visualization and analysis capabilities!</p>"},{"location":"backends/multi-backend/#next-steps","title":"Next Steps","text":"<ul> <li>Try the complete multi-backend example</li> <li>Learn about platform-specific features</li> <li>Set up production multi-backend workflows</li> </ul>"},{"location":"backends/wandb/","title":"Weights &amp; Biases Backend","text":"<p>Weights &amp; Biases (wandb) is a collaborative platform for experiment tracking, visualization, and model management, particularly popular in deep learning research.</p>"},{"location":"backends/wandb/#overview","title":"Overview","text":"<p>W&amp;B provides best-in-class features for ML teams:</p> <ul> <li>Beautiful, interactive visualizations</li> <li>Collaborative experiment sharing</li> <li>Automatic hyperparameter tracking</li> <li>Model registry and artifact versioning</li> <li>Real-time collaboration and reports</li> <li>Integration with popular ML frameworks</li> </ul> <p>Perfect for deep learning research and team collaboration.</p>"},{"location":"backends/wandb/#installation","title":"Installation","text":"pip <p><code>bash     pip install tracelet[wandb]     # Note: wandb is already included as core dependency</code></p> uv <p><code>bash     uv add tracelet[wandb]     # Note: wandb is already included as core dependency</code></p> <p>Minimum Requirements:</p> <ul> <li>wandb &gt;= 0.16.0 (included in tracelet core dependencies)</li> <li>Python &gt;= 3.9</li> </ul>"},{"location":"backends/wandb/#quick-start","title":"Quick Start","text":""},{"location":"backends/wandb/#online-mode-default","title":"Online Mode (Default)","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start with W&amp;B online tracking\ntracelet.start_logging(\n    exp_name=\"wandb_experiment\",\n    project=\"my_project\",\n    backend=\"wandb\"\n)\n\n# Use TensorBoard as normal - metrics go to W&amp;B\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss\", 0.5, 1)\nwriter.add_scalar(\"accuracy\", 0.9, 1)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"backends/wandb/#view-results","title":"View Results","text":"<p>Visit wandb.ai to view your experiments in the web dashboard.</p>"},{"location":"backends/wandb/#setup-and-authentication","title":"Setup and Authentication","text":""},{"location":"backends/wandb/#initial-setup","title":"Initial Setup","text":"<ol> <li>Create account at wandb.ai</li> <li>Get API key from Settings \u2192 API keys</li> <li>Login via CLI:</li> </ol> <pre><code># Interactive login (recommended)\nwandb login\n\n# Or set API key directly\nexport WANDB_API_KEY=your_api_key_here\n</code></pre>"},{"location":"backends/wandb/#environment-variables","title":"Environment Variables","text":"<pre><code># Authentication\nexport WANDB_API_KEY=your_api_key_here\n\n# Project defaults\nexport WANDB_PROJECT=my_default_project\nexport WANDB_ENTITY=your_username_or_team\n\n# Mode configuration\nexport WANDB_MODE=online    # online, offline, disabled\nexport WANDB_DIR=/path/to/wandb/logs\n</code></pre>"},{"location":"backends/wandb/#offline-mode","title":"Offline Mode","text":"<pre><code>import os\n\n# Enable offline mode (automatic fallback in examples)\nos.environ[\"WANDB_MODE\"] = \"offline\"\n\ntracelet.start_logging(\n    backend=\"wandb\",\n    exp_name=\"offline_experiment\",\n    project=\"development\"\n)\n\n# Sync later when online\n# wandb sync wandb/offline-run-xxx\n</code></pre>"},{"location":"backends/wandb/#configuration","title":"Configuration","text":""},{"location":"backends/wandb/#basic-configuration","title":"Basic Configuration","text":"<pre><code>tracelet.start_logging(\n    backend=\"wandb\",\n    exp_name=\"my_experiment\",\n    project=\"computer_vision\",    # W&amp;B project name\n    config={\n        \"entity\": \"your_username\",        # W&amp;B entity (user/team)\n        \"name\": \"resnet_baseline\",        # Run name\n        \"tags\": [\"pytorch\", \"baseline\"],  # Run tags\n        \"notes\": \"Initial baseline run\"   # Run description\n    }\n)\n</code></pre>"},{"location":"backends/wandb/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Comprehensive setup\ntracelet.start_logging(\n    backend=\"wandb\",\n    exp_name=\"advanced_experiment\",\n    project=\"ml_research\",\n    config={\n        \"entity\": \"research_team\",\n        \"name\": \"hyperparameter_sweep_v2\",\n        \"tags\": [\"experiment\", \"sweep\", \"optimized\"],\n        \"notes\": \"Systematic hyperparameter optimization\",\n        \"group\": \"resnet_experiments\",    # Group related runs\n        \"job_type\": \"training\",           # Job type for organization\n        \"mode\": \"online\",                 # online, offline, disabled\n        \"save_code\": True,                # Save code snapshot\n        \"resume\": \"auto\"                  # Resume previous run if exists\n    }\n)\n</code></pre>"},{"location":"backends/wandb/#run-groups-and-jobs","title":"Run Groups and Jobs","text":"<pre><code># Organize experiments with groups and job types\nfor lr in [0.001, 0.01, 0.1]:\n    tracelet.start_logging(\n        backend=\"wandb\",\n        project=\"hyperparameter_sweep\",\n        exp_name=f\"lr_{lr}\",\n        config={\n            \"group\": \"learning_rate_sweep\",    # Groups related runs\n            \"job_type\": \"hp_search\",           # Job type\n            \"tags\": [f\"lr_{lr}\"]\n        }\n    )\n    # ... training code ...\n    tracelet.stop_logging()\n</code></pre>"},{"location":"backends/wandb/#features","title":"Features","text":""},{"location":"backends/wandb/#automatic-logging","title":"Automatic Logging","text":"<p>W&amp;B captures extensive information automatically:</p> <pre><code># These metrics are automatically enhanced in W&amp;B:\nwriter = SummaryWriter()\nwriter.add_scalar(\"train/loss\", loss, step)\nwriter.add_histogram(\"model/gradients\", gradients, step)\nwriter.add_image(\"predictions\", image_grid, step)\n\n# W&amp;B adds:\n# - Interactive plots and zooming\n# - Automatic metric correlation analysis\n# - Real-time streaming\n# - Mobile notifications\n</code></pre>"},{"location":"backends/wandb/#hyperparameter-tracking","title":"Hyperparameter Tracking","text":"<pre><code>exp = tracelet.get_active_experiment()\n\n# Structured hyperparameter logging\nexp.log_params({\n    \"model\": {\n        \"architecture\": \"resnet50\",\n        \"layers\": 50,\n        \"dropout\": 0.1\n    },\n    \"training\": {\n        \"learning_rate\": 0.001,\n        \"batch_size\": 32,\n        \"optimizer\": \"adam\"\n    },\n    \"data\": {\n        \"dataset\": \"cifar10\",\n        \"augmentation\": True\n    }\n})\n</code></pre>"},{"location":"backends/wandb/#rich-artifacts","title":"Rich Artifacts","text":"<pre><code># Log models and datasets\nexp.log_artifact(\"model.pth\", artifact_path=\"models/\")\nexp.log_artifact(\"dataset.tar.gz\", artifact_path=\"data/\")\n\n# Log tables for structured data\nimport pandas as pd\npredictions_df = pd.DataFrame({\n    \"image_id\": range(100),\n    \"predicted_class\": predictions,\n    \"confidence\": confidences\n})\nexp.log_artifact(predictions_df, artifact_path=\"predictions.csv\")\n</code></pre>"},{"location":"backends/wandb/#interactive-visualizations","title":"Interactive Visualizations","text":"<pre><code># Custom plots and charts\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots()\nax.plot(epochs, losses)\nax.set_title(\"Training Loss\")\n\n# Log as W&amp;B artifact with interactive viewing\nexp.log_artifact(fig, artifact_path=\"plots/training_loss.png\")\n</code></pre>"},{"location":"backends/wandb/#advanced-features","title":"Advanced Features","text":""},{"location":"backends/wandb/#hyperparameter-sweeps","title":"Hyperparameter Sweeps","text":"<pre><code># sweep_config.yaml\nprogram: train.py\nmethod: random\nmetric:\n  goal: maximize\n  name: val_accuracy\nparameters:\n  learning_rate:\n    values: [0.001, 0.01, 0.1]\n  batch_size:\n    values: [16, 32, 64]\n  dropout:\n    min: 0.1\n    max: 0.5\n</code></pre> <pre><code># Run sweep\nimport wandb\n\n# Initialize sweep\nsweep_id = wandb.sweep(sweep_config, project=\"hyperparameter_optimization\")\n\n# Run sweep agents\nwandb.agent(sweep_id, function=train_function, count=20)\n</code></pre>"},{"location":"backends/wandb/#model-registry","title":"Model Registry","text":"<pre><code># Log model to registry with versioning\nimport wandb\n\n# Create artifact\nmodel_artifact = wandb.Artifact(\n    name=\"resnet_model\",\n    type=\"model\",\n    description=\"ResNet50 trained on CIFAR-10\"\n)\n\n# Add model files\nmodel_artifact.add_file(\"model.pth\")\nmodel_artifact.add_file(\"model_config.json\")\n\n# Log to registry\nexp.log_artifact(model_artifact)\n\n# Use model in another run\nartifact = wandb.use_artifact(\"resnet_model:latest\")\nartifact.download()\n</code></pre>"},{"location":"backends/wandb/#reports-and-sharing","title":"Reports and Sharing","text":"<pre><code># Create shareable reports\nimport wandb\n\n# Generate report from runs\nreport = wandb.Report(\n    project=\"my_project\",\n    title=\"Model Comparison Report\",\n    description=\"Comparing different architectures\"\n)\n\n# Add plots and analyses\nreport.blocks = [\n    wandb.report.PanelGrid(\n        panels=[\n            wandb.report.ScalarChart(\n                title=\"Training Loss Comparison\",\n                config={\"metrics\": [\"train/loss\"]}\n            )\n        ]\n    )\n]\n\n# Save and share\nreport.save()\n</code></pre>"},{"location":"backends/wandb/#integration-features","title":"Integration Features","text":""},{"location":"backends/wandb/#framework-auto-logging","title":"Framework Auto-logging","text":"<pre><code># Automatic integration with popular frameworks\nimport pytorch_lightning as pl\n\nclass LightningModel(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        # W&amp;B automatically logs:\n        # - Hyperparameters\n        # - Metrics\n        # - Model topology\n        # - System metrics\n\n# Just start tracelet - everything else is automatic\ntracelet.start_logging(backend=\"wandb\")\ntrainer = pl.Trainer()\ntrainer.fit(model)\n</code></pre>"},{"location":"backends/wandb/#jupyter-integration","title":"Jupyter Integration","text":"<pre><code># Enhanced Jupyter notebook support\nimport wandb\n\n# Initialize in notebook\nwandb.init(project=\"notebook_experiments\")\n\n# Automatic cell tracking\n%wandb notebook  # Magic command for cell tracking\n</code></pre>"},{"location":"backends/wandb/#best-practices","title":"Best Practices","text":""},{"location":"backends/wandb/#project-organization","title":"Project Organization","text":"<pre><code># Use consistent naming conventions\ntracelet.start_logging(\n    backend=\"wandb\",\n    project=\"computer-vision-research\",     # Descriptive project name\n    exp_name=\"resnet50_cifar10_baseline\",   # Structured run name\n    config={\n        \"entity\": \"ml_team\",\n        \"tags\": [\"resnet\", \"cifar10\", \"baseline\", \"v1.0\"]\n    }\n)\n</code></pre>"},{"location":"backends/wandb/#experiment-tracking","title":"Experiment Tracking","text":"<pre><code># Log comprehensive context\nexp = tracelet.get_active_experiment()\n\n# Code version\nexp.log_params({\n    \"git.commit\": \"abc123def\",\n    \"git.branch\": \"feature/new_model\",\n    \"code.version\": \"v1.2.0\"\n})\n\n# Environment\nexp.log_params({\n    \"env.python_version\": \"3.11.0\",\n    \"env.cuda_version\": \"11.8\",\n    \"env.gpu_type\": \"RTX 4090\"\n})\n\n# Dataset info\nexp.log_params({\n    \"data.name\": \"CIFAR-10\",\n    \"data.size\": 50000,\n    \"data.preprocessing\": \"normalize+augment\"\n})\n</code></pre>"},{"location":"backends/wandb/#collaboration","title":"Collaboration","text":"<pre><code># Share experiments with team\ntracelet.start_logging(\n    backend=\"wandb\",\n    project=\"team_experiments\",\n    config={\n        \"entity\": \"research_team\",          # Team workspace\n        \"tags\": [\"shared\", \"review\"],\n        \"notes\": \"Ready for team review\"\n    }\n)\n</code></pre>"},{"location":"backends/wandb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"backends/wandb/#common-issues","title":"Common Issues","text":"<p>Authentication failure:</p> <pre><code># Re-login\nwandb login\n\n# Or check API key\necho $WANDB_API_KEY\n</code></pre> <p>Offline mode not working:</p> <pre><code># Explicitly set offline mode\nimport os\nos.environ[\"WANDB_MODE\"] = \"offline\"\n\n# Verify mode\nimport wandb\nprint(wandb.env.get_mode())  # Should print \"offline\"\n</code></pre> <p>Large file upload timeouts:</p> <pre><code># Increase timeout for large artifacts\nimport os\nos.environ[\"WANDB_HTTP_TIMEOUT\"] = \"300\"  # 5 minutes\n</code></pre>"},{"location":"backends/wandb/#performance-optimization","title":"Performance Optimization","text":"<pre><code># Optimize for high-frequency logging\nimport os\nos.environ[\"WANDB_LOG_INTERVAL_SECONDS\"] = \"5\"  # Batch logs every 5 seconds\nos.environ[\"WANDB_DISABLE_GIT\"] = \"true\"        # Disable git tracking if not needed\n</code></pre>"},{"location":"backends/wandb/#comparison-with-other-backends","title":"Comparison with Other Backends","text":"Feature W&amp;B MLflow ClearML AIM Visualization quality \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 Collaboration features \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Hyperparameter sweeps \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Framework integration \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 Setup simplicity \u2b50\u2b50\u2b50 \u2b50\u2b50\u2b50 \u2b50\u2b50 \u2b50\u2b50\u2b50"},{"location":"backends/wandb/#migration","title":"Migration","text":""},{"location":"backends/wandb/#from-tensorboard","title":"From TensorBoard","text":"<pre><code># Before: Pure TensorBoard\n# from torch.utils.tensorboard import SummaryWriter\n# writer = SummaryWriter(\"./runs\")\n\n# After: TensorBoard + W&amp;B via Tracelet\nimport tracelet\ntracelet.start_logging(backend=\"wandb\")\nwriter = SummaryWriter()  # Same code, enhanced with W&amp;B!\n</code></pre>"},{"location":"backends/wandb/#from-mlflow","title":"From MLflow","text":"<pre><code># Easy backend switch\n# tracelet.start_logging(backend=\"mlflow\")     # Old\ntracelet.start_logging(backend=\"wandb\")       # New\n# All existing TensorBoard code works unchanged\n</code></pre>"},{"location":"backends/wandb/#complete-example","title":"Complete Example","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torch.utils.data import DataLoader, TensorDataset\nimport matplotlib.pyplot as plt\n\n# Start W&amp;B tracking with comprehensive config\ntracelet.start_logging(\n    exp_name=\"wandb_pytorch_example\",\n    project=\"tutorials\",\n    backend=\"wandb\",\n    config={\n        \"entity\": \"your_username\",\n        \"tags\": [\"pytorch\", \"tutorial\", \"example\"],\n        \"notes\": \"Complete example showing W&amp;B integration\",\n        \"group\": \"tutorial_runs\",\n        \"job_type\": \"training\"\n    }\n)\n\n# Model setup\nmodel = nn.Sequential(\n    nn.Linear(784, 128),\n    nn.ReLU(),\n    nn.Dropout(0.2),\n    nn.Linear(128, 64),\n    nn.ReLU(),\n    nn.Linear(64, 10)\n)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss()\n\n# Log model architecture\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"model.architecture\": \"mlp\",\n    \"model.layers\": [784, 128, 64, 10],\n    \"model.dropout\": 0.2,\n    \"model.total_params\": sum(p.numel() for p in model.parameters()),\n    \"optimizer.type\": \"adam\",\n    \"optimizer.lr\": 0.001\n})\n\n# Data\nX = torch.randn(1000, 784)\ny = torch.randint(0, 10, (1000,))\ndataset = TensorDataset(X, y)\ndataloader = DataLoader(dataset, batch_size=32)\n\n# Training with automatic W&amp;B logging\nwriter = SummaryWriter()\nlosses = []\n\nfor epoch in range(20):\n    total_loss = 0\n    for batch_idx, (data, target) in enumerate(dataloader):\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n\n        # Metrics automatically enhanced in W&amp;B\n        global_step = epoch * len(dataloader) + batch_idx\n        writer.add_scalar(\"batch/loss\", loss.item(), global_step)\n        total_loss += loss.item()\n\n    # Epoch metrics\n    avg_loss = total_loss / len(dataloader)\n    losses.append(avg_loss)\n    writer.add_scalar(\"epoch/loss\", avg_loss, epoch)\n    writer.add_scalar(\"epoch/learning_rate\", optimizer.param_groups[0]['lr'], epoch)\n\n    # Model analysis\n    for name, param in model.named_parameters():\n        writer.add_histogram(f\"weights/{name}\", param, epoch)\n        if param.grad is not None:\n            writer.add_histogram(f\"gradients/{name}\", param.grad, epoch)\n\n# Create and log training plot\nfig, ax = plt.subplots(figsize=(10, 6))\nax.plot(losses)\nax.set_title(\"Training Loss Over Time\")\nax.set_xlabel(\"Epoch\")\nax.set_ylabel(\"Loss\")\nax.grid(True)\n\n# Save and log plot\nplt.savefig(\"training_loss.png\")\nexp.log_artifact(\"training_loss.png\", artifact_path=\"plots/\")\nplt.close()\n\n# Log final metrics\nexp.log_params({\n    \"training.final_loss\": losses[-1],\n    \"training.best_loss\": min(losses),\n    \"training.epochs_completed\": len(losses),\n    \"training.total_batches\": len(dataloader) * len(losses)\n})\n\n# Save and log model\ntorch.save(model.state_dict(), \"final_model.pth\")\nexp.log_artifact(\"final_model.pth\", artifact_path=\"models/\")\n\n# Create model summary\nmodel_info = {\n    \"architecture\": \"3-layer MLP\",\n    \"input_size\": 784,\n    \"hidden_sizes\": [128, 64],\n    \"output_size\": 10,\n    \"total_parameters\": sum(p.numel() for p in model.parameters()),\n    \"final_loss\": losses[-1]\n}\n\nimport json\nwith open(\"model_summary.json\", \"w\") as f:\n    json.dump(model_info, f, indent=2)\nexp.log_artifact(\"model_summary.json\", artifact_path=\"metadata/\")\n\n# Cleanup\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Training completed! View results at: https://wandb.ai\")\nprint(\"\ud83d\udcca Check your dashboard for interactive visualizations and model analysis\")\n</code></pre>"},{"location":"backends/wandb/#next-steps","title":"Next Steps","text":"<ul> <li>Set up hyperparameter sweeps</li> <li>Explore W&amp;B Reports for sharing results</li> <li>Compare with other backends</li> </ul>"},{"location":"development/architecture/","title":"Architecture Overview","text":"<p>This document provides a detailed overview of Tracelet's architecture for developers and contributors.</p>"},{"location":"development/architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    A[User Code] --&gt; B[Framework Integrations]\n    A --&gt; C[TensorBoard SummaryWriter]\n    A --&gt; D[Direct API Calls]\n\n    B --&gt; E[Orchestrator]\n    C --&gt; E\n    D --&gt; E\n\n    E --&gt; F[Plugin System]\n\n    F --&gt; G[Backend Plugins]\n    F --&gt; H[Collector Plugins]\n\n    G --&gt; I[MLflow Backend]\n    G --&gt; J[W&amp;B Backend]\n    G --&gt; K[ClearML Backend]\n    G --&gt; L[AIM Backend]\n\n    H --&gt; M[Git Collector]\n    H --&gt; N[System Collector]\n\n    I --&gt; O[MLflow Server]\n    J --&gt; P[W&amp;B Platform]\n    K --&gt; Q[ClearML Platform]\n    L --&gt; R[AIM Repository]\n</code></pre>"},{"location":"development/architecture/#core-components","title":"Core Components","text":""},{"location":"development/architecture/#1-orchestrator-traceletcoreorchestrator","title":"1. Orchestrator (<code>tracelet.core.orchestrator</code>)","text":"<p>The Orchestrator is the central component that coordinates all metric flow and routing.</p> <p>Key Responsibilities:</p> <ul> <li>Receives metrics from various sources (TensorBoard, Lightning, direct API)</li> <li>Routes metrics to appropriate backends based on configuration</li> <li>Manages thread-safe operations and queueing</li> <li>Handles backpressure and error recovery</li> </ul> <p>Architecture:</p> <pre><code>class Orchestrator:\n    def __init__(self):\n        self._backends: List[BackendInterface] = []\n        self._collectors: List[CollectorInterface] = []\n        self._queue: Queue = Queue()\n        self._worker_threads: List[Thread] = []\n        self._running: bool = False\n\n    def route_metric(self, metric: Metric) -&gt; None:\n        \"\"\"Route metric to all configured backends\"\"\"\n\n    def start(self) -&gt; None:\n        \"\"\"Start worker threads and collectors\"\"\"\n\n    def stop(self) -&gt; None:\n        \"\"\"Stop all operations gracefully\"\"\"\n</code></pre>"},{"location":"development/architecture/#2-plugin-system-traceletcoreplugins","title":"2. Plugin System (<code>tracelet.core.plugins</code>)","text":"<p>The Plugin System provides a modular architecture for extending Tracelet.</p> <p>Plugin Types:</p> <ul> <li>Backend Plugins: Implement experiment tracking backends</li> <li>Framework Plugins: Integrate with ML frameworks</li> <li>Collector Plugins: Gather environment and system data</li> </ul> <p>Plugin Interface:</p> <pre><code>class PluginInterface:\n    @classmethod\n    def get_metadata(cls) -&gt; PluginMetadata:\n        \"\"\"Return plugin metadata\"\"\"\n\n    def initialize(self, config: dict) -&gt; None:\n        \"\"\"Initialize plugin with configuration\"\"\"\n\n    def cleanup(self) -&gt; None:\n        \"\"\"Clean up resources\"\"\"\n\nclass BackendPlugin(PluginInterface):\n    def create_backend(self, config: dict) -&gt; BackendInterface:\n        \"\"\"Factory method to create backend instance\"\"\"\n\nclass FrameworkPlugin(PluginInterface):\n    def patch_framework(self, orchestrator: Orchestrator) -&gt; None:\n        \"\"\"Patch framework to capture metrics\"\"\"\n</code></pre>"},{"location":"development/architecture/#3-experiment-management-traceletcoreexperiment","title":"3. Experiment Management (<code>tracelet.core.experiment</code>)","text":"<p>The Experiment class provides the main API for interacting with Tracelet.</p> <p>Key Features:</p> <ul> <li>Unified API across all backends</li> <li>Parameter and metric logging</li> <li>Artifact management</li> <li>Context management</li> </ul> <pre><code>class Experiment:\n    def __init__(self, name: str, project: str):\n        self.name = name\n        self.project = project\n        self._orchestrator = Orchestrator()\n\n    def log_metric(self, name: str, value: float, step: int = None) -&gt; None:\n        \"\"\"Log a scalar metric\"\"\"\n\n    def log_params(self, params: dict) -&gt; None:\n        \"\"\"Log experiment parameters\"\"\"\n\n    def log_artifact(self, artifact_path: str) -&gt; None:\n        \"\"\"Log an artifact file\"\"\"\n</code></pre>"},{"location":"development/architecture/#framework-integrations","title":"Framework Integrations","text":""},{"location":"development/architecture/#tensorboard-integration-traceletframeworkspytorch","title":"TensorBoard Integration (<code>tracelet.frameworks.pytorch</code>)","text":"<p>Method: Monkey patching <code>torch.utils.tensorboard.SummaryWriter</code></p> <p>Implementation Strategy:</p> <ol> <li>Store original methods</li> <li>Wrap methods with metric capture</li> <li>Forward calls to original methods</li> <li>Send captured metrics to orchestrator</li> </ol> <pre><code>class PyTorchFramework:\n    def __init__(self):\n        self._original_methods = {}\n        self._patched = False\n\n    def _patch_tensorboard(self):\n        \"\"\"Patch TensorBoard SummaryWriter methods\"\"\"\n        from torch.utils.tensorboard import SummaryWriter\n\n        # Store original method\n        self._original_methods['add_scalar'] = SummaryWriter.add_scalar\n\n        # Create wrapped method\n        def wrapped_add_scalar(self, tag, scalar_value, global_step=None, walltime=None):\n            # Capture metric\n            self._orchestrator.route_metric(Metric(tag, scalar_value, global_step))\n\n            # Call original method\n            return self._original_methods['add_scalar'](self, tag, scalar_value, global_step, walltime)\n\n        # Apply patch\n        SummaryWriter.add_scalar = wrapped_add_scalar\n</code></pre>"},{"location":"development/architecture/#pytorch-lightning-integration-traceletframeworkslightning","title":"PyTorch Lightning Integration (<code>tracelet.frameworks.lightning</code>)","text":"<p>Method: Monkey patching <code>pytorch_lightning.LightningModule.log</code></p> <p>Implementation Strategy:</p> <ol> <li>Patch the <code>log</code> method of LightningModule</li> <li>Intercept all <code>self.log()</code> calls</li> <li>Extract metric information and route to orchestrator</li> <li>Preserve original Lightning logging behavior</li> </ol> <pre><code>class LightningFramework:\n    def _patch_lightning_logging(self):\n        \"\"\"Patch Lightning's logging system\"\"\"\n        from pytorch_lightning.core.module import LightningModule\n\n        # Store original method\n        self._original_log = LightningModule.log\n\n        def wrapped_log(module_self, name: str, value, *args, **kwargs):\n            # Call original method first\n            result = self._original_log(module_self, name, value, *args, **kwargs)\n\n            # Extract step information\n            step = self._get_current_step(module_self)\n\n            # Route to Tracelet\n            if isinstance(value, (int, float)):\n                self._orchestrator.route_metric(Metric(name, float(value), step))\n\n            return result\n\n        # Apply patch\n        LightningModule.log = wrapped_log\n</code></pre>"},{"location":"development/architecture/#backend-implementations","title":"Backend Implementations","text":""},{"location":"development/architecture/#backend-interface","title":"Backend Interface","text":"<p>All backends implement a common interface:</p> <pre><code>class BackendInterface:\n    def log_metric(self, name: str, value: float, step: int) -&gt; None:\n        \"\"\"Log a scalar metric\"\"\"\n\n    def log_params(self, params: dict) -&gt; None:\n        \"\"\"Log experiment parameters\"\"\"\n\n    def log_artifact(self, artifact_path: str, artifact_name: str = None) -&gt; None:\n        \"\"\"Log an artifact\"\"\"\n\n    def start_experiment(self, name: str, project: str) -&gt; str:\n        \"\"\"Start a new experiment, return experiment ID\"\"\"\n\n    def end_experiment(self) -&gt; None:\n        \"\"\"End the current experiment\"\"\"\n</code></pre>"},{"location":"development/architecture/#mlflow-backend-traceletbackendsmlflow","title":"MLflow Backend (<code>tracelet.backends.mlflow</code>)","text":"<p>Implementation Details:</p> <ul> <li>Uses <code>mlflow.tracking</code> client</li> <li>Manages MLflow runs automatically</li> <li>Supports local and remote tracking servers</li> <li>Handles nested runs for complex experiments</li> </ul> <pre><code>class MLflowBackend(BackendInterface):\n    def __init__(self, config: dict):\n        import mlflow\n        self.mlflow = mlflow\n        self.tracking_uri = config.get('tracking_uri', 'file:./mlruns')\n        self.mlflow.set_tracking_uri(self.tracking_uri)\n\n    def start_experiment(self, name: str, project: str) -&gt; str:\n        experiment = self.mlflow.set_experiment(project)\n        run = self.mlflow.start_run(run_name=name)\n        return run.info.run_id\n</code></pre>"},{"location":"development/architecture/#wb-backend-traceletbackendswandb","title":"W&amp;B Backend (<code>tracelet.backends.wandb</code>)","text":"<p>Implementation Details:</p> <ul> <li>Uses <code>wandb</code> Python SDK</li> <li>Manages wandb runs and projects</li> <li>Supports both cloud and local modes</li> <li>Handles W&amp;B-specific features (sweeps, artifacts)</li> </ul>"},{"location":"development/architecture/#clearml-backend-traceletbackendsclearml","title":"ClearML Backend (<code>tracelet.backends.clearml</code>)","text":"<p>Implementation Details:</p> <ul> <li>Uses <code>clearml</code> SDK</li> <li>Manages ClearML tasks and projects</li> <li>Supports offline mode for testing</li> <li>Integrates with ClearML's experiment management</li> </ul>"},{"location":"development/architecture/#data-flow","title":"Data Flow","text":""},{"location":"development/architecture/#metric-flow-pipeline","title":"Metric Flow Pipeline","text":"<ol> <li>Capture: Framework integrations capture metrics from user code</li> <li>Queue: Metrics are queued for processing to handle high-frequency logging</li> <li>Route: Orchestrator routes metrics to all configured backends</li> <li>Transform: Each backend transforms metrics to its native format</li> <li>Send: Backends send metrics to their respective platforms</li> </ol> <pre><code># Example metric flow\ndef metric_flow_example():\n    # 1. User code logs metric\n    writer.add_scalar('accuracy', 0.95, step=100)\n\n    # 2. TensorBoard patch captures metric\n    metric = Metric(name='accuracy', value=0.95, step=100)\n\n    # 3. Orchestrator queues metric\n    orchestrator.route_metric(metric)\n\n    # 4. Worker thread processes queue\n    for backend in backends:\n        backend.log_metric(metric.name, metric.value, metric.step)\n\n    # 5. Backends send to their platforms\n    # MLflow: mlflow.log_metric('accuracy', 0.95, step=100)\n    # W&amp;B: wandb.log({'accuracy': 0.95}, step=100)\n    # ClearML: logger.report_scalar('accuracy', 'accuracy', 0.95, 100)\n</code></pre>"},{"location":"development/architecture/#threading-model","title":"Threading Model","text":"<p>Main Thread: User code execution and framework patching Worker Threads: Background processing of metric queue Collector Threads: Periodic collection of system metrics</p> <pre><code>class ThreadingModel:\n    def __init__(self):\n        self._metric_queue = Queue()\n        self._worker_pool = ThreadPoolExecutor(max_workers=4)\n        self._collector_timer = Timer(interval=30.0, function=self._collect_metrics)\n\n    def _process_metrics(self):\n        \"\"\"Worker thread processes metric queue\"\"\"\n        while self._running:\n            try:\n                metric = self._metric_queue.get(timeout=1.0)\n                self._route_to_backends(metric)\n            except Empty:\n                continue\n\n    def _collect_metrics(self):\n        \"\"\"Periodic collection of system metrics\"\"\"\n        for collector in self._collectors:\n            metrics = collector.collect()\n            for metric in metrics:\n                self._metric_queue.put(metric)\n</code></pre>"},{"location":"development/architecture/#configuration-management","title":"Configuration Management","text":""},{"location":"development/architecture/#configuration-hierarchy","title":"Configuration Hierarchy","text":"<ol> <li>Default Configuration: Built-in defaults</li> <li>Environment Variables: System-wide overrides</li> <li>Configuration Files: Project-specific settings</li> <li>API Parameters: Runtime overrides</li> </ol> <pre><code>class ConfigurationManager:\n    def __init__(self):\n        self.config = self._load_default_config()\n        self._apply_env_overrides()\n        self._load_config_files()\n\n    def _load_default_config(self) -&gt; dict:\n        return {\n            'backends': ['mlflow'],\n            'track_system': True,\n            'track_git': True,\n            'metrics_interval': 30.0,\n            'max_queue_size': 10000\n        }\n</code></pre>"},{"location":"development/architecture/#settings-management-traceletsettings","title":"Settings Management (<code>tracelet.settings</code>)","text":"<p>Pydantic-based Settings:</p> <ul> <li>Type validation</li> <li>Environment variable integration</li> <li>Documentation generation</li> <li>IDE support</li> </ul> <pre><code>from pydantic import BaseSettings\n\nclass TraceletSettings(BaseSettings):\n    project: str = \"default\"\n    backend: List[str] = [\"mlflow\"]\n    track_system: bool = True\n    track_git: bool = True\n    track_env: bool = True\n    metrics_interval: float = 30.0\n\n    class Config:\n        env_prefix = \"TRACELET_\"\n        case_sensitive = False\n</code></pre>"},{"location":"development/architecture/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"development/architecture/#error-strategies","title":"Error Strategies","text":"<p>Graceful Degradation: Continue operation even if some backends fail Retry Logic: Automatic retry with exponential backoff Circuit Breaker: Temporarily disable failing backends Fallback Modes: Switch to local backends if remote ones fail</p> <pre><code>class ErrorHandling:\n    def __init__(self):\n        self._retry_config = {\n            'max_retries': 3,\n            'backoff_factor': 2.0,\n            'max_backoff': 60.0\n        }\n        self._circuit_breakers = {}\n\n    async def safe_backend_call(self, backend: BackendInterface, operation: str, *args, **kwargs):\n        \"\"\"Safely call backend with error handling\"\"\"\n        breaker = self._circuit_breakers.get(backend.name)\n\n        if breaker and breaker.is_open():\n            return None  # Skip if circuit breaker is open\n\n        try:\n            method = getattr(backend, operation)\n            return await method(*args, **kwargs)\n        except Exception as e:\n            self._handle_backend_error(backend, e)\n            return None\n</code></pre>"},{"location":"development/architecture/#performance-considerations","title":"Performance Considerations","text":""},{"location":"development/architecture/#optimization-strategies","title":"Optimization Strategies","text":"<p>Batching: Batch multiple metrics for efficient network usage Compression: Compress large artifacts and images Async Operations: Non-blocking metric logging Connection Pooling: Reuse connections to backend services</p>"},{"location":"development/architecture/#memory-management","title":"Memory Management","text":"<p>Bounded Queues: Prevent memory leaks from unbounded growth Periodic Cleanup: Clean up completed experiments Resource Limits: Configurable limits on image sizes and queue sizes</p>"},{"location":"development/architecture/#profiling-and-monitoring","title":"Profiling and Monitoring","text":"<p>Internal Metrics: Track Tracelet's own performance Debugging Tools: Built-in profiling and debugging capabilities Health Checks: Monitor backend connectivity and health</p>"},{"location":"development/architecture/#security-considerations","title":"Security Considerations","text":""},{"location":"development/architecture/#api-key-management","title":"API Key Management","text":"<ul> <li>Environment variable storage</li> <li>Secure credential storage</li> <li>Key rotation support</li> <li>Minimal privilege access</li> </ul>"},{"location":"development/architecture/#data-privacy","title":"Data Privacy","text":"<ul> <li>Configurable data filtering</li> <li>PII detection and masking</li> <li>Local-only modes for sensitive data</li> <li>Audit logging capabilities</li> </ul>"},{"location":"development/architecture/#extension-points","title":"Extension Points","text":""},{"location":"development/architecture/#custom-backends","title":"Custom Backends","text":"<p>Developers can create custom backends by:</p> <ol> <li>Implementing <code>BackendInterface</code></li> <li>Creating a backend plugin</li> <li>Registering the plugin with Tracelet</li> </ol>"},{"location":"development/architecture/#custom-collectors","title":"Custom Collectors","text":"<p>Add new data collectors by:</p> <ol> <li>Implementing <code>CollectorInterface</code></li> <li>Creating a collector plugin</li> <li>Configuring collection intervals</li> </ol>"},{"location":"development/architecture/#custom-framework-integrations","title":"Custom Framework Integrations","text":"<p>Integrate new frameworks by:</p> <ol> <li>Implementing <code>FrameworkInterface</code></li> <li>Creating framework-specific patches</li> <li>Testing with framework workflows</li> </ol> <p>This architecture provides a solid foundation for experiment tracking while remaining flexible and extensible for future enhancements.</p>"},{"location":"development/contributing/","title":"Contributing to Tracelet","text":"<p>We welcome contributions to Tracelet! This guide will help you get started with contributing to the project.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.9 or higher</li> <li>Git</li> <li>uv package manager (recommended)</li> </ul>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":"<ol> <li>Fork and Clone</li> </ol> <pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/tracelet.git\ncd tracelet\n</code></pre> <ol> <li>Install Development Dependencies</li> </ol> <pre><code># Using uv (recommended)\nuv sync --all-extras\n\n# Or using pip\npip install -e \".[dev,all]\"\n</code></pre> <ol> <li>Install Pre-commit Hooks</li> </ol> <pre><code>uv run pre-commit install\n</code></pre> <ol> <li>Verify Installation</li> </ol> <pre><code># Run tests to ensure everything works\nuv run pytest tests/unit -v\n\n# Run linting\nuv run ruff check\nuv run ruff format --check\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#branch-strategy","title":"Branch Strategy","text":"<ul> <li>main: Production-ready code</li> <li>feature/xxx: New features</li> <li>fix/xxx: Bug fixes</li> <li>docs/xxx: Documentation updates</li> </ul>"},{"location":"development/contributing/#making-changes","title":"Making Changes","text":"<ol> <li>Create a Feature Branch</li> </ol> <pre><code>git checkout -b feature/my-new-feature\n</code></pre> <ol> <li> <p>Make Your Changes</p> </li> <li> <p>Write code following our style guidelines</p> </li> <li>Add tests for new functionality</li> <li> <p>Update documentation as needed</p> </li> <li> <p>Test Your Changes</p> </li> </ol> <pre><code># Run unit tests\nuv run pytest tests/unit -v\n\n# Run integration tests (optional)\nuv run pytest tests/integration -v\n\n# Run linting\nuv run ruff check\nuv run ruff format\n</code></pre> <ol> <li>Commit Your Changes</li> </ol> <pre><code>git add .\ngit commit -m \"feat: add new awesome feature\"\n</code></pre> <p>Use Conventional Commits format:</p> <ul> <li><code>feat:</code> for new features</li> <li><code>fix:</code> for bug fixes</li> <li><code>docs:</code> for documentation changes</li> <li><code>test:</code> for test additions/modifications</li> <li><code>refactor:</code> for code refactoring</li> <li> <p><code>style:</code> for formatting changes</p> </li> <li> <p>Push and Create PR</p> </li> </ul> <pre><code>git push origin feature/my-new-feature\n</code></pre> <p>Then create a Pull Request on GitHub.</p>"},{"location":"development/contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"development/contributing/#python-code-style","title":"Python Code Style","text":"<p>We use Ruff for linting and formatting:</p> <pre><code># Check style\nuv run ruff check\n\n# Auto-fix issues\nuv run ruff check --fix\n\n# Format code\nuv run ruff format\n</code></pre>"},{"location":"development/contributing/#code-organization","title":"Code Organization","text":"<pre><code>tracelet/\n\u251c\u2500\u2500 __init__.py           # Main public API\n\u251c\u2500\u2500 core/                 # Core functionality\n\u2502   \u251c\u2500\u2500 experiment.py     # Experiment management\n\u2502   \u251c\u2500\u2500 orchestrator.py   # Metric routing\n\u2502   \u2514\u2500\u2500 plugins.py        # Plugin system\n\u251c\u2500\u2500 backends/             # Backend implementations\n\u2502   \u251c\u2500\u2500 mlflow.py\n\u2502   \u251c\u2500\u2500 wandb.py\n\u2502   \u2514\u2500\u2500 clearml.py\n\u251c\u2500\u2500 frameworks/           # Framework integrations\n\u2502   \u251c\u2500\u2500 pytorch.py\n\u2502   \u2514\u2500\u2500 lightning.py\n\u251c\u2500\u2500 collectors/           # Data collectors\n\u2502   \u251c\u2500\u2500 git.py\n\u2502   \u2514\u2500\u2500 system.py\n\u2514\u2500\u2500 plugins/              # Plugin implementations\n    \u251c\u2500\u2500 mlflow_backend.py\n    \u2514\u2500\u2500 wandb_backend.py\n</code></pre>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Classes: PascalCase (<code>ExperimentTracker</code>)</li> <li>Functions/Variables: snake_case (<code>log_metric</code>)</li> <li>Constants: UPPER_SNAKE_CASE (<code>DEFAULT_TIMEOUT</code>)</li> <li>Private: Leading underscore (<code>_internal_method</code>)</li> </ul>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/contributing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                 # Unit tests\n\u2502   \u251c\u2500\u2500 core/\n\u2502   \u251c\u2500\u2500 backends/\n\u2502   \u2514\u2500\u2500 frameworks/\n\u251c\u2500\u2500 integration/          # Integration tests\n\u2502   \u251c\u2500\u2500 test_backend_integration.py\n\u2502   \u2514\u2500\u2500 test_e2e_workflows.py\n\u2514\u2500\u2500 e2e/                  # End-to-end tests\n    \u251c\u2500\u2500 test_basic_workflows.py\n    \u2514\u2500\u2500 test_advanced_workflows.py\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>import pytest\nfrom unittest.mock import Mock, patch\nfrom tracelet.core.experiment import Experiment\n\nclass TestExperiment:\n    def test_log_metric(self):\n        \"\"\"Test basic metric logging functionality\"\"\"\n        experiment = Experiment(\"test_exp\", \"test_project\")\n\n        # Test the functionality\n        experiment.log_metric(\"accuracy\", 0.95, step=100)\n\n        # Assert expected behavior\n        assert experiment.metrics[\"accuracy\"][-1] == (0.95, 100)\n\n    @patch('tracelet.backends.mlflow.MLflowBackend')\n    def test_backend_integration(self, mock_backend):\n        \"\"\"Test integration with backend\"\"\"\n        mock_backend.return_value.log_metric = Mock()\n\n        experiment = Experiment(\"test_exp\", \"test_project\")\n        experiment.add_backend(mock_backend.return_value)\n        experiment.log_metric(\"loss\", 0.5, step=1)\n\n        mock_backend.return_value.log_metric.assert_called_once_with(\"loss\", 0.5, 1)\n</code></pre>"},{"location":"development/contributing/#test-categories","title":"Test Categories","text":"<p>Unit Tests - Fast, isolated tests:</p> <pre><code>uv run pytest tests/unit -v\n</code></pre> <p>Integration Tests - Test component interactions:</p> <pre><code>uv run pytest tests/integration -v\n</code></pre> <p>E2E Tests - Full workflow tests (slow):</p> <pre><code>uv run pytest tests/e2e -v\n</code></pre>"},{"location":"development/contributing/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"development/contributing/#code-documentation","title":"Code Documentation","text":"<p>Use Google-style docstrings:</p> <pre><code>def log_metric(self, name: str, value: float, step: int = None) -&gt; None:\n    \"\"\"Log a scalar metric to the experiment.\n\n    Args:\n        name: The name of the metric (e.g., 'accuracy', 'loss').\n        value: The numeric value to log.\n        step: The step/iteration number. If None, auto-incremented.\n\n    Raises:\n        ValueError: If value is not a number.\n\n    Example:\n        &gt;&gt;&gt; experiment.log_metric(\"accuracy\", 0.95, step=100)\n    \"\"\"\n</code></pre>"},{"location":"development/contributing/#api-documentation","title":"API Documentation","text":"<p>Document all public APIs with:</p> <ul> <li>Clear description of purpose</li> <li>Parameter types and descriptions</li> <li>Return value information</li> <li>Usage examples</li> <li>Related functions/classes</li> </ul>"},{"location":"development/contributing/#adding-new-documentation","title":"Adding New Documentation","text":"<ol> <li>Create Markdown Files</li> </ol> <pre><code># Add new documentation\ntouch docs/guides/my-new-guide.md\n</code></pre> <ol> <li>Update Navigation    Edit <code>mkdocs.yml</code> to include your new documentation:</li> </ol> <pre><code>nav:\n  - Guides:\n      - My New Guide: guides/my-new-guide.md\n</code></pre> <ol> <li>Test Documentation</li> </ol> <pre><code># Build and serve docs locally\nuv run mkdocs serve\n</code></pre>"},{"location":"development/contributing/#adding-new-features","title":"Adding New Features","text":""},{"location":"development/contributing/#backend-integration","title":"Backend Integration","text":"<p>To add a new backend (e.g., Neptune):</p> <ol> <li>Create Backend Implementation</li> </ol> <pre><code># tracelet/backends/neptune.py\nfrom tracelet.core.interfaces import BackendInterface\n\nclass NeptuneBackend(BackendInterface):\n    def __init__(self, config: dict):\n        self.config = config\n        self._setup_neptune()\n\n    def log_metric(self, name: str, value: float, step: int):\n        # Implementation here\n        pass\n</code></pre> <ol> <li>Create Plugin</li> </ol> <pre><code># tracelet/plugins/neptune_backend.py\nfrom tracelet.core.plugins import BackendPlugin, PluginMetadata, PluginType\n\nclass NeptuneBackendPlugin(BackendPlugin):\n    @classmethod\n    def get_metadata(cls) -&gt; PluginMetadata:\n        return PluginMetadata(\n            name=\"neptune\",\n            version=\"1.0.0\",\n            type=PluginType.BACKEND,\n            description=\"Neptune.ai experiment tracking backend\"\n        )\n\n    def create_backend(self, config: dict):\n        from tracelet.backends.neptune import NeptuneBackend\n        return NeptuneBackend(config)\n</code></pre> <ol> <li>Add Tests</li> </ol> <pre><code># tests/unit/backends/test_neptune.py\n# tests/integration/test_neptune_integration.py\n</code></pre> <ol> <li>Update Documentation</li> </ol> <pre><code># docs/backends/neptune.md\n</code></pre>"},{"location":"development/contributing/#framework-integration","title":"Framework Integration","text":"<p>To add a new framework integration:</p> <ol> <li>Create Framework Module</li> </ol> <pre><code># tracelet/frameworks/jax.py\nfrom tracelet.core.interfaces import FrameworkInterface\n\nclass JAXFramework(FrameworkInterface):\n    def initialize(self, experiment):\n        # Patch JAX logging functions\n        pass\n</code></pre> <ol> <li>Add Plugin</li> <li>Write Tests</li> <li>Document Usage</li> </ol>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#version-management","title":"Version Management","text":"<p>We use semantic versioning (MAJOR.MINOR.PATCH):</p> <ul> <li>MAJOR: Breaking changes</li> <li>MINOR: New features (backward compatible)</li> <li>PATCH: Bug fixes (backward compatible)</li> </ul>"},{"location":"development/contributing/#creating-a-release","title":"Creating a Release","text":"<ol> <li>Update Version</li> </ol> <pre><code># Update version in pyproject.toml\n# Update CHANGELOG.md\n</code></pre> <ol> <li>Create Release PR</li> </ol> <pre><code>git checkout -b release/v1.2.0\ngit commit -m \"chore: prepare release v1.2.0\"\n</code></pre> <ol> <li>Tag Release</li> </ol> <pre><code>git tag v1.2.0\ngit push origin v1.2.0\n</code></pre>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<ul> <li>Be respectful and inclusive</li> <li>Focus on constructive feedback</li> <li>Help others learn and grow</li> <li>Follow our Code of Conduct</li> </ul>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Discord: Join our Discord server</li> <li>GitHub Issues: For bug reports and feature requests</li> <li>GitHub Discussions: For questions and community discussions</li> <li>Email: maintainers@tracelet.io</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in:</p> <ul> <li>CONTRIBUTORS.md file</li> <li>Release notes</li> <li>Documentation acknowledgments</li> <li>Social media shoutouts</li> </ul>"},{"location":"development/contributing/#questions","title":"Questions?","text":"<p>Don't hesitate to ask questions:</p> <ul> <li>Open a GitHub Discussion</li> <li>Join our Discord</li> <li>Email us at maintainers@tracelet.io</li> </ul> <p>Thank you for contributing to Tracelet! \ud83d\ude80</p>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>This page provides practical examples of using Tracelet for common experiment tracking scenarios.</p>"},{"location":"examples/basic/#quick-start-example","title":"Quick Start Example","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start experiment tracking\ntracelet.start_logging(\n    exp_name=\"basic_example\",\n    project=\"getting_started\",\n    backend=\"mlflow\"\n)\n\n# Create a simple model and training setup\nmodel = nn.Linear(10, 1)\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\n# Use TensorBoard as normal - metrics are automatically captured!\nwriter = SummaryWriter()\n\n# Training loop with automatic metric capture\nfor epoch in range(50):\n    # Synthetic training data\n    X = torch.randn(32, 10)\n    y = torch.randn(32, 1)\n\n    # Forward pass\n    optimizer.zero_grad()\n    output = model(X)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    # Log metrics - automatically sent to MLflow!\n    writer.add_scalar('Loss/Train', loss.item(), epoch)\n    writer.add_scalar('LearningRate', optimizer.param_groups[0]['lr'], epoch)\n\n# Log additional experiment metadata\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32,\n    \"epochs\": 50,\n    \"model_type\": \"linear\"\n})\n\n# Clean up\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Experiment completed! Check your MLflow UI at http://localhost:5000\")\n</code></pre>"},{"location":"examples/basic/#configuration-examples","title":"Configuration Examples","text":""},{"location":"examples/basic/#environment-variables","title":"Environment Variables","text":"<pre><code># Set default backend\nexport TRACELET_BACKEND=mlflow\n\n# Set project name\nexport TRACELET_PROJECT=my_ml_project\n\n# Backend-specific configuration\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre>"},{"location":"examples/basic/#programmatic-configuration","title":"Programmatic Configuration","text":"<pre><code>from tracelet.settings import TraceletSettings\n\n# Create custom settings\nsettings = TraceletSettings(\n    project=\"advanced_project\",\n    backend=[\"mlflow\"],\n    track_system=True,\n    metrics_interval=5.0\n)\n\n# Use settings\ntracelet.start_logging(\n    exp_name=\"configured_experiment\",\n    settings=settings\n)\n</code></pre>"},{"location":"examples/basic/#manual-metric-logging","title":"Manual Metric Logging","text":"<pre><code>import tracelet\n\n# Start experiment\nexp = tracelet.start_logging(\n    exp_name=\"manual_logging\",\n    project=\"examples\",\n    backend=\"mlflow\"\n)\n\n# Log metrics manually\nexp.log_metric(\"accuracy\", 0.95, iteration=100)\nexp.log_metric(\"loss\", 0.05, iteration=100)\n\n# Log parameters\nexp.log_params({\n    \"learning_rate\": 0.001,\n    \"batch_size\": 64,\n    \"optimizer\": \"adam\"\n})\n\n# Log artifacts\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3], [1, 4, 2])\nplt.savefig(\"training_plot.png\")\nexp.log_artifact(\"training_plot.png\", \"plots/training_curve.png\")\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"examples/basic/#error-handling","title":"Error Handling","text":"<pre><code>import tracelet\n\ntry:\n    tracelet.start_logging(\n        exp_name=\"robust_experiment\",\n        project=\"error_handling\",\n        backend=\"mlflow\"\n    )\n\n    # Your training code here\n    for epoch in range(10):\n        # Simulate potential error\n        if epoch == 5:\n            raise ValueError(\"Simulated training error\")\n\n        exp = tracelet.get_active_experiment()\n        exp.log_metric(\"epoch\", epoch, iteration=epoch)\n\nexcept Exception as e:\n    print(f\"Training failed: {e}\")\n\n    # Log error information\n    exp = tracelet.get_active_experiment()\n    if exp:\n        exp.log_params({\"error\": str(e), \"failed_at_epoch\": epoch})\n\nfinally:\n    # Always clean up\n    tracelet.stop_logging()\n</code></pre>"},{"location":"examples/basic/#context-manager-usage","title":"Context Manager Usage","text":"<pre><code>import tracelet\n\n# Automatic cleanup using context manager\nwith tracelet.start_logging(\n    exp_name=\"context_managed\",\n    project=\"examples\",\n    backend=\"mlflow\"\n) as exp:\n    # Training code here\n    exp.log_metric(\"start_time\", time.time())\n\n    for epoch in range(10):\n        exp.log_metric(\"epoch_loss\", 1.0 / (epoch + 1), iteration=epoch)\n\n    exp.log_params({\"total_epochs\": 10})\n\n# Automatic cleanup when exiting context\n</code></pre>"},{"location":"examples/basic/#multi-metric-logging","title":"Multi-Metric Logging","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\ntracelet.start_logging(\n    exp_name=\"multi_metric\",\n    project=\"examples\",\n    backend=\"mlflow\"\n)\n\nwriter = SummaryWriter()\n\nfor epoch in range(20):\n    # Log multiple related metrics\n    train_loss = 1.0 / (epoch + 1)\n    val_loss = train_loss * 1.1\n    accuracy = min(0.95, epoch * 0.05)\n\n    # Batch logging with TensorBoard\n    writer.add_scalars('Loss', {\n        'Train': train_loss,\n        'Validation': val_loss\n    }, epoch)\n\n    writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n    writer.add_scalar('Metrics/LearningRate', 0.01 * (0.9 ** epoch), epoch)\n\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"examples/basic/#reproducibility-example","title":"Reproducibility Example","text":"<pre><code>import tracelet\nimport torch\nimport numpy as np\nimport random\n\n# Set seeds for reproducibility\ndef set_seeds(seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n\nset_seeds(42)\n\n# Start experiment with reproducibility info\nexp = tracelet.start_logging(\n    exp_name=\"reproducible_experiment\",\n    project=\"reproducibility\",\n    backend=\"mlflow\"\n)\n\n# Log reproducibility parameters\nexp.log_params({\n    \"random_seed\": 42,\n    \"pytorch_version\": torch.__version__,\n    \"cuda_available\": torch.cuda.is_available(),\n    \"device\": str(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n})\n\n# Your training code here...\nmodel = torch.nn.Linear(5, 1)\ndata = torch.randn(100, 5)\ntarget = torch.randn(100, 1)\n\noutput = model(data)\nloss = torch.nn.functional.mse_loss(output, target)\n\nexp.log_metric(\"initial_loss\", loss.item())\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"examples/basic/#system-metrics-monitoring","title":"System Metrics Monitoring","text":"<pre><code>import tracelet\nimport time\n\n# Enable system metrics collection\nfrom tracelet.settings import TraceletSettings\n\nsettings = TraceletSettings(\n    project=\"system_monitoring\",\n    backend=[\"mlflow\"],\n    track_system=True,\n    metrics_interval=5.0  # Collect every 5 seconds\n)\n\ntracelet.start_logging(\n    exp_name=\"monitored_training\",\n    settings=settings\n)\n\n# Simulate training workload\nfor i in range(10):\n    # Simulate some work\n    time.sleep(2)\n\n    # Log training progress\n    exp = tracelet.get_active_experiment()\n    exp.log_metric(\"training_step\", i, iteration=i)\n\ntracelet.stop_logging()\nprint(\"Check your MLflow UI to see system metrics alongside training metrics!\")\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Multi-Backend Example to track to multiple platforms</li> <li>Explore Backend-Specific Guides for advanced features</li> <li>Check out the Interactive Notebooks for hands-on tutorials</li> <li>Review Best Practices for production usage</li> </ul>"},{"location":"examples/multi-backend/","title":"Multi-Backend Comparison","text":"<p>This page demonstrates how to use Tracelet with multiple backends simultaneously and provides comparison examples to help you choose the right backend for your needs.</p>"},{"location":"examples/multi-backend/#using-multiple-backends-simultaneously","title":"Using Multiple Backends Simultaneously","text":"<p>Tracelet can log to multiple experiment tracking platforms at once, allowing you to compare their features or migrate between systems.</p>"},{"location":"examples/multi-backend/#basic-multi-backend-setup","title":"Basic Multi-Backend Setup","text":"<pre><code>import tracelet\nimport torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start experiment tracking with multiple backends\ntracelet.start_logging(\n    exp_name=\"multi_backend_demo\",\n    project=\"backend_comparison\",\n    backend=[\"mlflow\", \"clearml\", \"wandb\"]  # List multiple backends\n)\n\n# Create a simple training setup\nmodel = nn.Linear(10, 1)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\n# Use TensorBoard - metrics automatically sent to ALL backends\nwriter = SummaryWriter()\n\nfor epoch in range(30):\n    # Generate synthetic data\n    X = torch.randn(64, 10)\n    y = torch.randn(64, 1)\n\n    # Training step\n    optimizer.zero_grad()\n    output = model(X)\n    loss = criterion(output, y)\n    loss.backward()\n    optimizer.step()\n\n    # Log metrics - sent to MLflow, ClearML, AND W&amp;B\n    writer.add_scalar('Loss/Train', loss.item(), epoch)\n    writer.add_scalar('Metrics/LearningRate', optimizer.param_groups[0]['lr'], epoch)\n\n# Log experiment parameters to all backends\nexp = tracelet.get_active_experiment()\nexp.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 64,\n    \"epochs\": 30,\n    \"optimizer\": \"adam\"\n})\n\nwriter.close()\ntracelet.stop_logging()\n\nprint(\"\u2705 Experiment logged to MLflow, ClearML, and W&amp;B simultaneously!\")\n</code></pre>"},{"location":"examples/multi-backend/#backend-specific-configuration","title":"Backend-Specific Configuration","text":"<pre><code>import tracelet\nfrom tracelet.settings import TraceletSettings\n\n# Configure settings for multiple backends\nsettings = TraceletSettings(\n    project=\"backend_specific_demo\",\n    backend=[\"mlflow\", \"wandb\"],\n    track_system=True,\n\n    # Backend-specific settings\n    mlflow_tracking_uri=\"http://localhost:5000\",\n    wandb_entity=\"your_team_name\",\n    wandb_project=\"comparison_study\"\n)\n\ntracelet.start_logging(\n    exp_name=\"configured_multi_backend\",\n    settings=settings\n)\n\n# Your training code here...\nexp = tracelet.get_active_experiment()\nexp.log_metric(\"test_metric\", 0.95)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"examples/multi-backend/#backend-feature-comparison","title":"Backend Feature Comparison","text":""},{"location":"examples/multi-backend/#quick-comparison-table","title":"Quick Comparison Table","text":"Feature MLflow ClearML W&amp;B AIM Local Deployment \u2705 \u2705 \u274c \u2705 Cloud Hosting \u2705 \u2705 \u2705 \u274c Real-time Metrics \u2705 \u2705 \u2705 \u2705 Model Registry \u2705 \u2705 \u2705 \u274c Artifact Storage \u2705 \u2705 \u2705 \u2705 Hyperparameter Optimization \u274c \u2705 \u2705 \u274c Dataset Versioning \u274c \u2705 \u2705 \u274c Collaboration Tools \u274c \u2705 \u2705 \u274c Open Source \u2705 \u2705 \u274c \u2705"},{"location":"examples/multi-backend/#mlflow-example","title":"MLflow Example","text":"<pre><code>import tracelet\n\n# MLflow - Great for model registry and local deployment\ntracelet.start_logging(\n    exp_name=\"mlflow_demo\",\n    project=\"backend_comparison\",\n    backend=\"mlflow\"\n)\n\nexp = tracelet.get_active_experiment()\n\n# Log comprehensive experiment data\nexp.log_params({\n    \"model_type\": \"ResNet50\",\n    \"dataset\": \"CIFAR-10\",\n    \"augmentation\": True\n})\n\nfor epoch in range(5):\n    exp.log_metric(\"train_loss\", 0.5 - epoch * 0.1, iteration=epoch)\n    exp.log_metric(\"val_accuracy\", 0.8 + epoch * 0.03, iteration=epoch)\n\n# MLflow excels at artifact logging\nimport matplotlib.pyplot as plt\nplt.plot([1, 2, 3, 4, 5], [0.5, 0.4, 0.3, 0.2, 0.1])\nplt.title(\"Training Loss\")\nplt.savefig(\"loss_curve.png\")\nexp.log_artifact(\"loss_curve.png\", \"plots/\")\n\ntracelet.stop_logging()\nprint(\"\u2705 Check MLflow UI at http://localhost:5000\")\n</code></pre>"},{"location":"examples/multi-backend/#clearml-example","title":"ClearML Example","text":"<pre><code>import tracelet\n\n# ClearML - Enterprise features and data management\ntracelet.start_logging(\n    exp_name=\"clearml_demo\",\n    project=\"backend_comparison\",\n    backend=\"clearml\"\n)\n\nexp = tracelet.get_active_experiment()\n\n# ClearML automatically captures environment info\nexp.log_params({\n    \"framework\": \"PyTorch\",\n    \"clearml_auto_capture\": True\n})\n\n# Simulate hyperparameter optimization\nfor lr in [0.001, 0.01, 0.1]:\n    for batch_size in [32, 64, 128]:\n        exp.log_metric(f\"accuracy_lr{lr}_bs{batch_size}\",\n                      0.8 + lr * 0.5 + batch_size * 0.001)\n\ntracelet.stop_logging()\nprint(\"\u2705 Check ClearML at https://app.clear.ml\")\n</code></pre>"},{"location":"examples/multi-backend/#weights-biases-example","title":"Weights &amp; Biases Example","text":"<pre><code>import tracelet\n\n# W&amp;B - Great for collaboration and sweeps\ntracelet.start_logging(\n    exp_name=\"wandb_demo\",\n    project=\"backend_comparison\",\n    backend=\"wandb\"\n)\n\nexp = tracelet.get_active_experiment()\n\n# W&amp;B excels at rich media logging\nexp.log_params({\n    \"architecture\": \"transformer\",\n    \"attention_heads\": 8,\n    \"hidden_size\": 512\n})\n\n# Log metrics with custom charts\nfor step in range(20):\n    exp.log_metric(\"training/loss\", 2.0 * (0.9 ** step), iteration=step)\n    exp.log_metric(\"training/perplexity\", 50 * (0.95 ** step), iteration=step)\n    exp.log_metric(\"validation/bleu_score\", min(0.9, step * 0.05), iteration=step)\n\ntracelet.stop_logging()\nprint(\"\u2705 Check W&amp;B at https://wandb.ai\")\n</code></pre>"},{"location":"examples/multi-backend/#aim-example","title":"AIM Example","text":"<pre><code>import tracelet\n\n# AIM - Lightweight and self-hosted\ntracelet.start_logging(\n    exp_name=\"aim_demo\",\n    project=\"backend_comparison\",\n    backend=\"aim\"\n)\n\nexp = tracelet.get_active_experiment()\n\n# AIM is great for fast iteration and visualization\nexp.log_params({\n    \"model\": \"lightweight_cnn\",\n    \"optimization\": \"fast_iteration\"\n})\n\n# Log multiple runs for comparison\nfor run_id in range(3):\n    for epoch in range(10):\n        # Simulate different random seeds\n        noise = run_id * 0.1\n        exp.log_metric(f\"run_{run_id}/accuracy\",\n                      0.7 + epoch * 0.02 + noise, iteration=epoch)\n\ntracelet.stop_logging()\nprint(\"\u2705 Check AIM UI at http://localhost:43800\")\n</code></pre>"},{"location":"examples/multi-backend/#migration-between-backends","title":"Migration Between Backends","text":""},{"location":"examples/multi-backend/#step-by-step-migration","title":"Step-by-Step Migration","text":"<pre><code>import tracelet\n\n# Step 1: Export from current backend (example with MLflow)\ndef export_mlflow_experiment(experiment_id):\n    \"\"\"Export experiment data for migration\"\"\"\n    import mlflow\n\n    client = mlflow.tracking.MlflowClient()\n    experiment = client.get_experiment(experiment_id)\n    runs = client.search_runs([experiment_id])\n\n    export_data = []\n    for run in runs:\n        run_data = {\n            \"name\": run.info.run_name,\n            \"params\": run.data.params,\n            \"metrics\": run.data.metrics,\n            \"tags\": run.data.tags\n        }\n        export_data.append(run_data)\n\n    return export_data\n\n# Step 2: Import to new backend\ndef migrate_to_new_backend(export_data, new_backend=\"wandb\"):\n    \"\"\"Migrate experiment data to new backend\"\"\"\n\n    for i, run_data in enumerate(export_data):\n        tracelet.start_logging(\n            exp_name=f\"migrated_{run_data['name']}\",\n            project=\"migration_project\",\n            backend=new_backend\n        )\n\n        exp = tracelet.get_active_experiment()\n\n        # Migrate parameters\n        exp.log_params(run_data[\"params\"])\n\n        # Migrate metrics (simplified - you may need to handle iterations)\n        for metric_name, metric_value in run_data[\"metrics\"].items():\n            exp.log_metric(metric_name, metric_value)\n\n        tracelet.stop_logging()\n        print(f\"\u2705 Migrated run {i+1}/{len(export_data)}\")\n\n# Example usage (uncomment to use):\n# exported = export_mlflow_experiment(\"your_experiment_id\")\n# migrate_to_new_backend(exported, \"wandb\")\n</code></pre>"},{"location":"examples/multi-backend/#parallel-logging-for-safe-migration","title":"Parallel Logging for Safe Migration","text":"<pre><code>import tracelet\n\n# Log to both old and new backends during transition\ntracelet.start_logging(\n    exp_name=\"safe_migration\",\n    project=\"migration_project\",\n    backend=[\"mlflow\", \"wandb\"]  # Old and new backend\n)\n\n# Your existing training code works unchanged\nexp = tracelet.get_active_experiment()\n\nfor epoch in range(10):\n    loss = 1.0 / (epoch + 1)\n    accuracy = min(0.95, epoch * 0.1)\n\n    # Metrics go to BOTH backends automatically\n    exp.log_metric(\"loss\", loss, iteration=epoch)\n    exp.log_metric(\"accuracy\", accuracy, iteration=epoch)\n\nexp.log_params({\"migration_phase\": \"parallel_logging\"})\n\ntracelet.stop_logging()\nprint(\"\u2705 Data safely logged to both MLflow and W&amp;B\")\n</code></pre>"},{"location":"examples/multi-backend/#choosing-the-right-backend","title":"Choosing the Right Backend","text":""},{"location":"examples/multi-backend/#use-mlflow-when","title":"Use MLflow When:","text":"<ul> <li>You need local deployment and control</li> <li>Model registry is important</li> <li>You're building ML platforms or tools</li> <li>Cost is a primary concern (open source)</li> </ul>"},{"location":"examples/multi-backend/#use-clearml-when","title":"Use ClearML When:","text":"<ul> <li>You need enterprise features</li> <li>Data versioning is critical</li> <li>You want comprehensive experiment tracking</li> <li>Team collaboration is important</li> </ul>"},{"location":"examples/multi-backend/#use-weights-biases-when","title":"Use Weights &amp; Biases When:","text":"<ul> <li>You're doing research or need rich visualizations</li> <li>Hyperparameter optimization is key</li> <li>You want the best user experience</li> <li>Collaboration and sharing are priorities</li> </ul>"},{"location":"examples/multi-backend/#use-aim-when","title":"Use AIM When:","text":"<ul> <li>You need lightweight, fast iteration</li> <li>You want self-hosted with minimal setup</li> <li>Visualization performance is critical</li> <li>You're doing metric-heavy experiments</li> </ul>"},{"location":"examples/multi-backend/#best-practices-for-multi-backend-usage","title":"Best Practices for Multi-Backend Usage","text":""},{"location":"examples/multi-backend/#1-start-simple","title":"1. Start Simple","text":"<pre><code># Begin with one backend\ntracelet.start_logging(backend=\"mlflow\")\n\n# Add more as needed\ntracelet.start_logging(backend=[\"mlflow\", \"wandb\"])\n</code></pre>"},{"location":"examples/multi-backend/#2-use-environment-variables-for-flexibility","title":"2. Use Environment Variables for Flexibility","text":"<pre><code># .env file\nTRACELET_BACKEND=mlflow,wandb\nTRACELET_PROJECT=my_project\n</code></pre> <pre><code>import os\nimport tracelet\n\n# Automatically uses backends from environment\nbackends = os.getenv(\"TRACELET_BACKEND\", \"mlflow\").split(\",\")\ntracelet.start_logging(\n    exp_name=\"flexible_backend\",\n    backend=backends\n)\n</code></pre>"},{"location":"examples/multi-backend/#3-handle-backend-specific-features","title":"3. Handle Backend-Specific Features","text":"<pre><code>import tracelet\n\nexp = tracelet.get_active_experiment()\n\n# Check which backends are active\nif \"wandb\" in exp.active_backends:\n    # W&amp;B-specific logging\n    exp.log_metric(\"wandb/special_metric\", 0.95)\n\nif \"clearml\" in exp.active_backends:\n    # ClearML-specific features\n    exp.log_params({\"clearml_task_id\": \"auto_captured\"})\n</code></pre>"},{"location":"examples/multi-backend/#next-steps","title":"Next Steps","text":"<ul> <li>Explore Backend-Specific Guides for detailed configuration</li> <li>Try the Interactive Notebooks for hands-on multi-backend experience</li> <li>Check Best Practices for production multi-backend usage</li> <li>Review Migration Guide for detailed migration strategies</li> </ul>"},{"location":"examples/notebooks/","title":"Jupyter Notebook Examples","text":"<p>This section provides interactive Jupyter notebook examples demonstrating Tracelet's capabilities in real-world scenarios.</p>"},{"location":"examples/notebooks/#available-notebooks","title":"Available Notebooks","text":""},{"location":"examples/notebooks/#getting-started","title":"Getting Started","text":""},{"location":"examples/notebooks/#1-quick-start-notebook","title":"1. Quick Start Notebook","text":"<ul> <li>File: <code>notebooks/01_quick_start.ipynb</code></li> <li>Description: Basic introduction to Tracelet with a simple PyTorch training loop</li> <li>Topics: Installation, basic logging, metric visualization</li> <li>Duration: 10-15 minutes</li> </ul>"},{"location":"examples/notebooks/#2-multi-backend-comparison","title":"2. Multi-Backend Comparison","text":"<ul> <li>File: <code>notebooks/02_multi_backend.ipynb</code></li> <li>Description: Compare different experiment tracking backends side-by-side</li> <li>Topics: MLflow, W&amp;B, ClearML configuration and output comparison</li> <li>Duration: 20-30 minutes</li> </ul>"},{"location":"examples/notebooks/#framework-integrations","title":"Framework Integrations","text":""},{"location":"examples/notebooks/#3-pytorch-integration-deep-dive","title":"3. PyTorch Integration Deep Dive","text":"<ul> <li>File: <code>notebooks/03_pytorch_integration.ipynb</code></li> <li>Description: Comprehensive PyTorch integration with TensorBoard</li> <li>Topics: Scalar metrics, histograms, images, model graphs</li> <li>Duration: 30-45 minutes</li> </ul>"},{"location":"examples/notebooks/#4-pytorch-lightning-integration","title":"4. PyTorch Lightning Integration","text":"<ul> <li>File: <code>notebooks/04_lightning_integration.ipynb</code></li> <li>Description: End-to-end Lightning training with automatic metric capture</li> <li>Topics: Lightning modules, callbacks, multi-GPU training</li> <li>Duration: 25-35 minutes</li> </ul>"},{"location":"examples/notebooks/#advanced-topics","title":"Advanced Topics","text":""},{"location":"examples/notebooks/#5-computer-vision-workflow","title":"5. Computer Vision Workflow","text":"<ul> <li>File: <code>notebooks/05_computer_vision.ipynb</code></li> <li>Description: Complete CV pipeline with image classification</li> <li>Topics: Data loading, augmentation, model training, prediction visualization</li> <li>Duration: 45-60 minutes</li> </ul>"},{"location":"examples/notebooks/#6-custom-plugin-development","title":"6. Custom Plugin Development","text":"<ul> <li>File: <code>notebooks/06_custom_plugins.ipynb</code></li> <li>Description: Building custom backends and metric collectors</li> <li>Topics: Plugin architecture, custom backends, metric routing</li> <li>Duration: 60+ minutes</li> </ul>"},{"location":"examples/notebooks/#running-the-notebooks","title":"Running the Notebooks","text":""},{"location":"examples/notebooks/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Tracelet with all extras\npip install tracelet[all]\n\n# Install Jupyter\npip install jupyter notebook\n\n# Or use JupyterLab\npip install jupyterlab\n</code></pre>"},{"location":"examples/notebooks/#local-setup","title":"Local Setup","text":"<pre><code># Clone the repository\ngit clone https://github.com/prassanna-ravishankar/tracelet.git\ncd tracelet\n\n# Install in development mode\npip install -e \".[all,dev]\"\n\n# Launch Jupyter\njupyter notebook notebooks/\n# or\njupyter lab notebooks/\n</code></pre>"},{"location":"examples/notebooks/#cloud-environments","title":"Cloud Environments","text":"<p>The notebooks are compatible with popular cloud platforms:</p>"},{"location":"examples/notebooks/#google-colab","title":"Google Colab","text":""},{"location":"examples/notebooks/#kaggle-kernels","title":"Kaggle Kernels","text":"<ul> <li>Upload notebooks to Kaggle and enable GPU/TPU as needed</li> <li>Install Tracelet in the first cell: <code>!pip install tracelet[all]</code></li> </ul>"},{"location":"examples/notebooks/#azure-notebooks-aws-sagemaker","title":"Azure Notebooks / AWS SageMaker","text":"<ul> <li>Compatible with both platforms</li> <li>Ensure you have appropriate credentials for your chosen backends</li> </ul>"},{"location":"examples/notebooks/#notebook-contents-overview","title":"Notebook Contents Overview","text":""},{"location":"examples/notebooks/#quick-start-example","title":"Quick Start Example","text":"<pre><code># Cell 1: Installation and imports\n!pip install tracelet[mlflow]\n\nimport tracelet\nimport torch\nimport torch.nn as nn\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Cell 2: Start experiment tracking\ntracelet.start_logging(\n    exp_name=\"notebook_demo\",\n    project=\"jupyter_examples\",\n    backend=\"mlflow\"\n)\n\n# Cell 3: Simple training loop with automatic metric capture\nwriter = SummaryWriter()\n\nfor epoch in range(10):\n    # Simulated training\n    loss = 1.0 / (epoch + 1)  # Decreasing loss\n    accuracy = min(0.95, epoch * 0.1)  # Increasing accuracy\n\n    # Metrics automatically captured by Tracelet\n    writer.add_scalar('Loss/Train', loss, epoch)\n    writer.add_scalar('Accuracy/Train', accuracy, epoch)\n\n    print(f\"Epoch {epoch}: Loss={loss:.3f}, Acc={accuracy:.3f}\")\n\n# Cell 4: View results\nprint(\"Check your MLflow UI to see the logged metrics!\")\nprint(\"Run: mlflow ui\")\n</code></pre>"},{"location":"examples/notebooks/#interactive-features","title":"Interactive Features","text":""},{"location":"examples/notebooks/#visualizations","title":"Visualizations","text":"<ul> <li>Metric plots: Real-time plotting with matplotlib/plotly</li> <li>Model visualizations: Architecture diagrams and parameter distributions</li> <li>Prediction samples: Image predictions, confusion matrices</li> </ul>"},{"location":"examples/notebooks/#backend-comparisons","title":"Backend Comparisons","text":"<ul> <li>Side-by-side comparison: Same experiment logged to multiple backends</li> <li>Feature comparison: Backend-specific capabilities</li> <li>Performance benchmarks: Logging speed and storage requirements</li> </ul>"},{"location":"examples/notebooks/#best-practices-for-notebook-development","title":"Best Practices for Notebook Development","text":""},{"location":"examples/notebooks/#1-environment-setup","title":"1. Environment Setup","text":"<pre><code># Always start with clear environment setup\n%load_ext tensorboard\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n</code></pre>"},{"location":"examples/notebooks/#2-experiment-organization","title":"2. Experiment Organization","text":"<pre><code># Use descriptive experiment names\nexp_name = f\"notebook_experiment_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n</code></pre>"},{"location":"examples/notebooks/#3-resource-management","title":"3. Resource Management","text":"<pre><code># Always clean up resources\ntry:\n    # Your training code here\n    pass\nfinally:\n    tracelet.stop_logging()\n    writer.close()\n</code></pre>"},{"location":"examples/notebooks/#4-interactive-widgets","title":"4. Interactive Widgets","text":"<pre><code># Use ipywidgets for interactive parameter tuning\nfrom ipywidgets import interact, IntSlider\n\n@interact(learning_rate=(0.001, 0.1, 0.001), batch_size=[16, 32, 64, 128])\ndef train_model(learning_rate=0.01, batch_size=32):\n    # Training code with interactive parameters\n    pass\n</code></pre>"},{"location":"examples/notebooks/#contributing-notebooks","title":"Contributing Notebooks","text":"<p>We welcome contributions of new notebook examples! Please see our Contributing Guidelines for details on:</p> <ul> <li>Notebook structure and formatting</li> <li>Documentation standards</li> <li>Testing procedures</li> <li>Submission process</li> </ul>"},{"location":"examples/notebooks/#notebook-template","title":"Notebook Template","text":"<p>Use our template for consistent formatting:</p> <pre><code># Standard header for all notebooks\n\"\"\"\nTracelet Example: [TITLE]\n\nDescription: [Brief description of what this notebook demonstrates]\nEstimated time: [X] minutes\nPrerequisites: [List any required knowledge or setup]\n\nAuthor: [Your name]\nDate: [Creation date]\n\"\"\"\n</code></pre>"},{"location":"guides/best-practices/","title":"Best Practices","text":"<p>This guide covers best practices for using Tracelet effectively in your machine learning projects.</p>"},{"location":"guides/best-practices/#experiment-organization","title":"Experiment Organization","text":""},{"location":"guides/best-practices/#naming-conventions","title":"Naming Conventions","text":"<p>Use consistent, descriptive naming for experiments and metrics:</p> <pre><code># Good: Descriptive experiment names\nexp_name = f\"resnet50_imagenet_{datetime.now().strftime('%Y%m%d_%H%M')}\"\nproject = \"image_classification\"\n\n# Good: Hierarchical metric names\nwriter.add_scalar('Loss/Train/CrossEntropy', ce_loss, step)\nwriter.add_scalar('Loss/Train/L2Regularization', l2_loss, step)\nwriter.add_scalar('Metrics/Accuracy/Train', train_acc, step)\nwriter.add_scalar('Metrics/Accuracy/Validation', val_acc, step)\n\n# Avoid: Vague or flat naming\nexp_name = \"experiment_1\"\nwriter.add_scalar('loss', loss, step)\nwriter.add_scalar('acc', acc, step)\n</code></pre>"},{"location":"guides/best-practices/#project-structure","title":"Project Structure","text":"<p>Organize your experiments by project and phase:</p> <pre><code># Development phase\ntracelet.start_logging(\n    exp_name=\"model_v1_dev\",\n    project=\"sentiment_analysis_dev\",\n    backend=\"mlflow\"\n)\n\n# Production experiments\ntracelet.start_logging(\n    exp_name=\"model_v2_prod\",\n    project=\"sentiment_analysis_prod\",\n    backend=[\"mlflow\", \"wandb\"]  # Multi-backend for production\n)\n</code></pre>"},{"location":"guides/best-practices/#metric-logging-strategy","title":"Metric Logging Strategy","text":""},{"location":"guides/best-practices/#what-to-log","title":"What to Log","text":"<p>Essential Metrics:</p> <ul> <li>Training and validation loss</li> <li>Key performance metrics (accuracy, F1, etc.)</li> <li>Learning rate and other hyperparameters</li> <li>System metrics (GPU/CPU usage, memory)</li> </ul> <p>Useful Metrics:</p> <ul> <li>Gradient norms and parameter distributions</li> <li>Intermediate layer activations</li> <li>Sample predictions and visualizations</li> <li>Timing and performance benchmarks</li> </ul> <p>Avoid Over-logging:</p> <ul> <li>Don't log every single parameter</li> <li>Limit high-resolution images and large tensors</li> <li>Reduce frequency for expensive operations</li> </ul>"},{"location":"guides/best-practices/#logging-frequency","title":"Logging Frequency","text":"<p>Balance detail with performance:</p> <pre><code># High-frequency: Essential metrics every step\nwriter.add_scalar('Loss/Train', loss, step)\n\n# Medium-frequency: Performance metrics every epoch\nif step % steps_per_epoch == 0:\n    writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n    writer.add_scalar('Learning_Rate', scheduler.get_lr()[0], epoch)\n\n# Low-frequency: Expensive operations occasionally\nif step % 1000 == 0:\n    # Histograms of model parameters\n    for name, param in model.named_parameters():\n        writer.add_histogram(f'Parameters/{name}', param, step)\n\nif step % 5000 == 0:\n    # Sample predictions and visualizations\n    writer.add_image('Predictions', prediction_grid, step)\n</code></pre>"},{"location":"guides/best-practices/#multi-backend-strategy","title":"Multi-Backend Strategy","text":""},{"location":"guides/best-practices/#when-to-use-multiple-backends","title":"When to Use Multiple Backends","text":"<p>Development: Single backend (usually MLflow for local development)</p> <pre><code>tracelet.start_logging(\"dev_experiment\", backend=\"mlflow\")\n</code></pre> <p>Team Collaboration: Shared backend (W&amp;B or ClearML)</p> <pre><code>tracelet.start_logging(\"team_experiment\", backend=\"wandb\")\n</code></pre> <p>Production: Multiple backends for redundancy</p> <pre><code>tracelet.start_logging(\n    \"prod_experiment\",\n    backend=[\"mlflow\", \"wandb\"]  # Local + cloud backup\n)\n</code></pre>"},{"location":"guides/best-practices/#backend-selection-guide","title":"Backend Selection Guide","text":"Use Case Recommended Backend Reason Local development MLflow Lightweight, no external dependencies Team collaboration Weights &amp; Biases Excellent sharing and visualization Enterprise deployment ClearML Enterprise features, self-hosted option Research projects AIM Open source, powerful analysis tools Production systems MLflow + W&amp;B Local reliability + cloud visualization"},{"location":"guides/best-practices/#performance-optimization","title":"Performance Optimization","text":""},{"location":"guides/best-practices/#memory-management","title":"Memory Management","text":"<pre><code># Configure memory-efficient logging\ntracelet.start_logging(\n    \"memory_efficient_exp\",\n    backend=\"mlflow\",\n    config={\n        \"track_system\": True,\n        \"metrics_interval\": 30.0,  # Reduce system monitoring frequency\n        \"max_image_size\": \"512KB\", # Limit image sizes\n        \"track_tensorboard\": True,\n        \"track_lightning\": True,\n    }\n)\n\n# In your training loop\nif step % 100 == 0:  # Reduce logging frequency\n    # Resize large images before logging\n    small_image = F.interpolate(large_image, size=(224, 224))\n    writer.add_image('Sample', small_image, step)\n</code></pre>"},{"location":"guides/best-practices/#network-efficiency","title":"Network Efficiency","text":"<p>For cloud backends, batch operations when possible:</p> <pre><code># Good: Batch related metrics\nmetrics = {\n    'Loss/Train': train_loss,\n    'Loss/Validation': val_loss,\n    'Accuracy/Train': train_acc,\n    'Accuracy/Validation': val_acc\n}\nfor name, value in metrics.items():\n    writer.add_scalar(name, value, step)\n</code></pre>"},{"location":"guides/best-practices/#reproducibility","title":"Reproducibility","text":""},{"location":"guides/best-practices/#environment-tracking","title":"Environment Tracking","text":"<p>Enable comprehensive environment tracking:</p> <pre><code>tracelet.start_logging(\n    \"reproducible_experiment\",\n    backend=\"mlflow\",\n    config={\n        \"track_git\": True,      # Git commit and status\n        \"track_env\": True,      # Python packages and versions\n        \"track_system\": True,   # Hardware information\n    }\n)\n\n# Log important hyperparameters\nexperiment = tracelet.get_active_experiment()\nexperiment.log_params({\n    'model_architecture': 'ResNet50',\n    'optimizer': 'AdamW',\n    'learning_rate': 0.001,\n    'batch_size': 32,\n    'epochs': 100,\n    'data_augmentation': True,\n    'random_seed': 42\n})\n</code></pre>"},{"location":"guides/best-practices/#random-seed-management","title":"Random Seed Management","text":"<pre><code>import random\nimport numpy as np\nimport torch\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\n# Set seed before training\nset_seed(42)\n\n# Log the seed\nexperiment.log_params({'random_seed': 42})\n</code></pre>"},{"location":"guides/best-practices/#error-handling-and-recovery","title":"Error Handling and Recovery","text":""},{"location":"guides/best-practices/#robust-experiment-setup","title":"Robust Experiment Setup","text":"<pre><code>import tracelet\nfrom contextlib import contextmanager\n\n@contextmanager\ndef experiment_context(exp_name, project, backend, config=None):\n    \"\"\"Context manager for safe experiment handling\"\"\"\n    try:\n        experiment = tracelet.start_logging(exp_name, project, backend, config)\n        yield experiment\n    except Exception as e:\n        print(f\"Experiment setup failed: {e}\")\n        # Fallback to basic logging\n        experiment = tracelet.start_logging(f\"{exp_name}_fallback\", project, \"mlflow\")\n        yield experiment\n    finally:\n        tracelet.stop_logging()\n\n# Usage\nwith experiment_context(\"my_experiment\", \"my_project\", \"wandb\") as exp:\n    # Your training code here\n    pass\n</code></pre>"},{"location":"guides/best-practices/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>def safe_log_metric(writer, name, value, step):\n    \"\"\"Safely log metrics with fallback\"\"\"\n    try:\n        writer.add_scalar(name, value, step)\n    except Exception as e:\n        print(f\"Failed to log {name}: {e}\")\n        # Continue training even if logging fails\n\ndef safe_log_image(writer, name, image, step):\n    \"\"\"Safely log images with size limits\"\"\"\n    try:\n        # Limit image size to prevent memory issues\n        if image.numel() &gt; 1000000:  # 1M pixels\n            image = F.interpolate(image, size=(512, 512))\n        writer.add_image(name, image, step)\n    except Exception as e:\n        print(f\"Failed to log image {name}: {e}\")\n</code></pre>"},{"location":"guides/best-practices/#team-collaboration","title":"Team Collaboration","text":""},{"location":"guides/best-practices/#shared-experiments","title":"Shared Experiments","text":"<p>Use consistent configuration across team members:</p> <pre><code># shared_config.py\nTRACELET_CONFIG = {\n    \"project\": \"team_project_2024\",\n    \"backend\": \"wandb\",\n    \"config\": {\n        \"track_git\": True,\n        \"track_env\": True,\n        \"track_system\": True,\n        \"metrics_interval\": 60.0,\n    }\n}\n\n# In individual scripts\nfrom shared_config import TRACELET_CONFIG\n\ntracelet.start_logging(\n    exp_name=f\"member_{os.getenv('USER')}_experiment\",\n    **TRACELET_CONFIG\n)\n</code></pre>"},{"location":"guides/best-practices/#experiment-handoffs","title":"Experiment Handoffs","text":"<p>Document experiments thoroughly:</p> <pre><code># Log experiment metadata\nexperiment.log_params({\n    'researcher': 'alice@company.com',\n    'experiment_purpose': 'hyperparameter_tuning',\n    'baseline_experiment': 'exp_v1_baseline',\n    'notes': 'Testing different learning rate schedules',\n    'next_steps': 'Try cosine annealing scheduler'\n})\n\n# Log important files and configurations\nexperiment.log_artifact('config.yaml')\nexperiment.log_artifact('model_architecture.py')\n</code></pre>"},{"location":"guides/best-practices/#security-and-privacy","title":"Security and Privacy","text":""},{"location":"guides/best-practices/#sensitive-data-handling","title":"Sensitive Data Handling","text":"<pre><code># Don't log sensitive information\nsafe_config = config.copy()\nsafe_config.pop('api_key', None)\nsafe_config.pop('password', None)\nexperiment.log_params(safe_config)\n\n# Use environment variables for credentials\nimport os\nwandb_key = os.getenv('WANDB_API_KEY')\n</code></pre>"},{"location":"guides/best-practices/#data-filtering","title":"Data Filtering","text":"<pre><code>def filter_sensitive_metrics(metrics_dict):\n    \"\"\"Remove sensitive information from metrics\"\"\"\n    filtered = {}\n    for key, value in metrics_dict.items():\n        if 'password' not in key.lower() and 'secret' not in key.lower():\n            filtered[key] = value\n    return filtered\n\n# Apply filtering before logging\nsafe_metrics = filter_sensitive_metrics(all_metrics)\nfor name, value in safe_metrics.items():\n    writer.add_scalar(name, value, step)\n</code></pre>"},{"location":"guides/migration/","title":"Migration Guide","text":"<p>This guide helps you migrate from other experiment tracking solutions to Tracelet.</p>"},{"location":"guides/migration/#migrating-from-pure-tensorboard","title":"Migrating from Pure TensorBoard","text":""},{"location":"guides/migration/#before-pure-tensorboard","title":"Before (Pure TensorBoard)","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\nwriter = SummaryWriter(log_dir='runs/experiment_1')\n\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n\n        # Manual TensorBoard logging\n        writer.add_scalar('Loss/Train', loss.item(),\n                         epoch * len(train_loader) + batch_idx)\n\n        if batch_idx % 100 == 0:\n            writer.add_histogram('Weights/Conv1', model.conv1.weight,\n                               epoch * len(train_loader) + batch_idx)\n\nwriter.close()\n</code></pre>"},{"location":"guides/migration/#after-with-tracelet","title":"After (With Tracelet)","text":"<pre><code>import tracelet  # Add this import\nfrom torch.utils.tensorboard import SummaryWriter\nimport torch.nn.functional as F\n\n# Add this line - everything else stays the same!\ntracelet.start_logging(\n    exp_name=\"experiment_1\",\n    project=\"my_project\",\n    backend=\"mlflow\"  # Choose your backend\n)\n\nwriter = SummaryWriter(log_dir='runs/experiment_1')\n\n# Same training loop - no changes needed!\nfor epoch in range(100):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        output = model(data)\n        loss = F.cross_entropy(output, target)\n\n        # Same TensorBoard calls - now automatically routed to MLflow!\n        writer.add_scalar('Loss/Train', loss.item(),\n                         epoch * len(train_loader) + batch_idx)\n\n        if batch_idx % 100 == 0:\n            writer.add_histogram('Weights/Conv1', model.conv1.weight,\n                               epoch * len(train_loader) + batch_idx)\n\nwriter.close()\ntracelet.stop_logging()  # Add this line\n</code></pre> <p>Migration effort: Add 2 lines, zero other changes! \u2728</p>"},{"location":"guides/migration/#migrating-from-mlflow","title":"Migrating from MLflow","text":""},{"location":"guides/migration/#before-direct-mlflow","title":"Before (Direct MLflow)","text":"<pre><code>import mlflow\nimport mlflow.pytorch\n\n# Manual MLflow setup\nmlflow.set_experiment(\"my_experiment\")\nwith mlflow.start_run():\n    # Log parameters\n    mlflow.log_params({\n        \"learning_rate\": 0.01,\n        \"batch_size\": 32,\n        \"epochs\": 100\n    })\n\n    for epoch in range(100):\n        loss = train_epoch()\n        accuracy = validate_epoch()\n\n        # Manual MLflow logging\n        mlflow.log_metric(\"train_loss\", loss, step=epoch)\n        mlflow.log_metric(\"val_accuracy\", accuracy, step=epoch)\n\n    # Log model\n    mlflow.pytorch.log_model(model, \"model\")\n</code></pre>"},{"location":"guides/migration/#after-with-tracelet_1","title":"After (With Tracelet)","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Simpler setup\ntracelet.start_logging(\n    exp_name=\"my_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Log parameters once\nexperiment = tracelet.get_active_experiment()\nexperiment.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32,\n    \"epochs\": 100\n})\n\n# Use TensorBoard for metrics (cleaner API)\nwriter = SummaryWriter()\nfor epoch in range(100):\n    loss = train_epoch()\n    accuracy = validate_epoch()\n\n    # Simpler metric logging\n    writer.add_scalar(\"train_loss\", loss, epoch)\n    writer.add_scalar(\"val_accuracy\", accuracy, epoch)\n\n# Still can log model directly if needed\nexperiment.log_artifact(\"model.pth\")\ntracelet.stop_logging()\n</code></pre> <p>Benefits: Simpler API, automatic TensorBoard integration, multi-backend support.</p>"},{"location":"guides/migration/#migrating-from-weights-biases","title":"Migrating from Weights &amp; Biases","text":""},{"location":"guides/migration/#before-direct-wb","title":"Before (Direct W&amp;B)","text":"<pre><code>import wandb\n\n# Manual W&amp;B setup\nwandb.init(\n    project=\"my_project\",\n    name=\"experiment_1\",\n    config={\n        \"learning_rate\": 0.01,\n        \"batch_size\": 32,\n        \"epochs\": 100\n    }\n)\n\nfor epoch in range(100):\n    loss = train_epoch()\n    accuracy = validate_epoch()\n\n    # Manual W&amp;B logging\n    wandb.log({\n        \"train/loss\": loss,\n        \"val/accuracy\": accuracy,\n        \"epoch\": epoch\n    })\n\n    # Log images\n    if epoch % 10 == 0:\n        wandb.log({\"predictions\": wandb.Image(prediction_image)})\n\nwandb.finish()\n</code></pre>"},{"location":"guides/migration/#after-with-tracelet_2","title":"After (With Tracelet)","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Simpler setup with same W&amp;B backend\ntracelet.start_logging(\n    exp_name=\"experiment_1\",\n    project=\"my_project\",\n    backend=\"wandb\",  # Still uses W&amp;B!\n    config={\n        \"learning_rate\": 0.01,\n        \"batch_size\": 32,\n        \"epochs\": 100\n    }\n)\n\n# Use standard TensorBoard API\nwriter = SummaryWriter()\nfor epoch in range(100):\n    loss = train_epoch()\n    accuracy = validate_epoch()\n\n    # Standard TensorBoard calls - automatically sent to W&amp;B\n    writer.add_scalar(\"train/loss\", loss, epoch)\n    writer.add_scalar(\"val/accuracy\", accuracy, epoch)\n\n    # Images work the same way\n    if epoch % 10 == 0:\n        writer.add_image(\"predictions\", prediction_image, epoch)\n\ntracelet.stop_logging()\n</code></pre> <p>Benefits: Standard TensorBoard API, easier to switch backends later, automatic metric capture.</p>"},{"location":"guides/migration/#migrating-from-clearml","title":"Migrating from ClearML","text":""},{"location":"guides/migration/#before-direct-clearml","title":"Before (Direct ClearML)","text":"<pre><code>from clearml import Task\n\n# Manual ClearML setup\ntask = Task.init(\n    project_name=\"my_project\",\n    task_name=\"experiment_1\"\n)\n\n# Get logger\nlogger = task.get_logger()\n\n# Connect hyperparameters\ntask.connect({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32,\n    \"epochs\": 100\n})\n\nfor epoch in range(100):\n    loss = train_epoch()\n    accuracy = validate_epoch()\n\n    # Manual ClearML logging\n    logger.report_scalar(\"train\", \"loss\", value=loss, iteration=epoch)\n    logger.report_scalar(\"validation\", \"accuracy\", value=accuracy, iteration=epoch)\n\ntask.close()\n</code></pre>"},{"location":"guides/migration/#after-with-tracelet_3","title":"After (With Tracelet)","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Simpler setup\ntracelet.start_logging(\n    exp_name=\"experiment_1\",\n    project=\"my_project\",\n    backend=\"clearml\"\n)\n\n# Log hyperparameters\nexperiment = tracelet.get_active_experiment()\nexperiment.log_params({\n    \"learning_rate\": 0.01,\n    \"batch_size\": 32,\n    \"epochs\": 100\n})\n\n# Standard TensorBoard API\nwriter = SummaryWriter()\nfor epoch in range(100):\n    loss = train_epoch()\n    accuracy = validate_epoch()\n\n    # Clean metric logging\n    writer.add_scalar(\"train/loss\", loss, epoch)\n    writer.add_scalar(\"validation/accuracy\", accuracy, epoch)\n\ntracelet.stop_logging()\n</code></pre>"},{"location":"guides/migration/#migrating-from-pytorch-lightning-loggers","title":"Migrating from PyTorch Lightning Loggers","text":""},{"location":"guides/migration/#before-lightning-multiple-loggers","title":"Before (Lightning + Multiple Loggers)","text":"<pre><code>import pytorch_lightning as pl\nfrom pytorch_lightning.loggers import MLFlowLogger, WandbLogger\n\n# Multiple logger setup\nmlf_logger = MLFlowLogger(\n    experiment_name=\"my_experiment\",\n    tracking_uri=\"http://localhost:5000\"\n)\nwandb_logger = WandbLogger(\n    project=\"my_project\",\n    name=\"experiment_1\"\n)\n\nclass MyModel(pl.LightningModule):\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_loss(batch)\n        # Automatic logging to both loggers\n        self.log('train/loss', loss)\n        return loss\n\ntrainer = pl.Trainer(\n    logger=[mlf_logger, wandb_logger],  # Multiple loggers\n    max_epochs=100\n)\ntrainer.fit(model)\n</code></pre>"},{"location":"guides/migration/#after-lightning-tracelet","title":"After (Lightning + Tracelet)","text":"<pre><code>import tracelet\nimport pytorch_lightning as pl\n\n# Single Tracelet setup for multiple backends\ntracelet.start_logging(\n    exp_name=\"experiment_1\",\n    project=\"my_project\",\n    backend=[\"mlflow\", \"wandb\"]  # Multi-backend with one call!\n)\n\nclass MyModel(pl.LightningModule):\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_loss(batch)\n        # Same Lightning logging - automatically captured\n        self.log('train/loss', loss)\n        return loss\n\n# No logger needed - Tracelet handles everything\ntrainer = pl.Trainer(max_epochs=100)\ntrainer.fit(model)\n\ntracelet.stop_logging()\n</code></pre> <p>Benefits: Simpler configuration, unified multi-backend logging, automatic capture.</p>"},{"location":"guides/migration/#migration-strategies","title":"Migration Strategies","text":""},{"location":"guides/migration/#gradual-migration","title":"Gradual Migration","text":"<ol> <li>Phase 1: Add Tracelet alongside existing logging</li> </ol> <pre><code># Keep your existing logging\nmlflow.log_metric(\"loss\", loss, step)\n\n# Add Tracelet in parallel\nimport tracelet\ntracelet.start_logging(\"parallel_test\", backend=\"mlflow\")\nwriter = SummaryWriter()\nwriter.add_scalar(\"loss_tracelet\", loss, step)\n</code></pre> <ol> <li>Phase 2: Switch to TensorBoard API with Tracelet</li> </ol> <pre><code># Remove direct backend calls\n# mlflow.log_metric(\"loss\", loss, step)  # Remove this\n\n# Use only TensorBoard + Tracelet\nwriter.add_scalar(\"loss\", loss, step)  # Automatically goes to MLflow\n</code></pre> <ol> <li>Phase 3: Leverage Tracelet's advanced features</li> </ol> <pre><code># Multi-backend logging\ntracelet.start_logging(\"production_exp\", backend=[\"mlflow\", \"wandb\"])\n\n# Automatic system monitoring\nconfig = {\"track_system\": True, \"track_git\": True}\n</code></pre>"},{"location":"guides/migration/#testing-migration","title":"Testing Migration","text":"<p>Create a validation script to ensure metrics match:</p> <pre><code>def validate_migration():\n    \"\"\"Test that Tracelet produces same metrics as direct backend calls\"\"\"\n\n    # Test data\n    test_metrics = {\"loss\": 0.5, \"accuracy\": 0.95}\n\n    # Method 1: Direct MLflow\n    import mlflow\n    with mlflow.start_run():\n        for name, value in test_metrics.items():\n            mlflow.log_metric(name, value, step=0)\n        direct_run_id = mlflow.active_run().info.run_id\n\n    # Method 2: Tracelet\n    import tracelet\n    from torch.utils.tensorboard import SummaryWriter\n\n    tracelet.start_logging(\"validation_test\", backend=\"mlflow\")\n    writer = SummaryWriter()\n    for name, value in test_metrics.items():\n        writer.add_scalar(name, value, 0)\n    tracelet.stop_logging()\n\n    print(\"\u2705 Migration validation complete - check MLflow UI for matching experiments\")\n\nvalidate_migration()\n</code></pre>"},{"location":"guides/migration/#best-practices-for-migration","title":"Best Practices for Migration","text":""},{"location":"guides/migration/#1-start-small","title":"1. Start Small","text":"<ul> <li>Begin with a single experiment</li> <li>Use your current backend initially</li> <li>Gradually add Tracelet features</li> </ul>"},{"location":"guides/migration/#2-maintain-compatibility","title":"2. Maintain Compatibility","text":"<pre><code># Keep existing parameter names and structure\n# Before: wandb.log({\"train_loss\": loss})\n# After:  writer.add_scalar(\"train_loss\", loss, step)\n</code></pre>"},{"location":"guides/migration/#3-test-thoroughly","title":"3. Test Thoroughly","text":"<ul> <li>Compare metric values between old and new systems</li> <li>Verify all important metrics are captured</li> <li>Test with different experiment configurations</li> </ul>"},{"location":"guides/migration/#4-document-changes","title":"4. Document Changes","text":"<pre><code># Document migration in your code\n\"\"\"\nMigration Notes:\n- Switched from direct MLflow to Tracelet on 2024-01-15\n- TensorBoard metrics now automatically routed to MLflow\n- Added multi-backend support for production experiments\n\"\"\"\n</code></pre>"},{"location":"guides/migration/#5-training-team","title":"5. Training Team","text":"<ul> <li>Update documentation and examples</li> <li>Provide migration scripts for common patterns</li> <li>Share best practices for new unified workflow</li> </ul>"},{"location":"guides/migration/#common-migration-issues","title":"Common Migration Issues","text":""},{"location":"guides/migration/#metric-name-differences","title":"Metric Name Differences","text":"<p>Different backends may expect different metric formats:</p> <pre><code># Solution: Use consistent hierarchical naming\nwriter.add_scalar(\"Loss/Train\", loss, step)      # Works everywhere\nwriter.add_scalar(\"Metrics/Accuracy\", acc, step) # Clear hierarchy\n</code></pre>"},{"location":"guides/migration/#configuration-migration","title":"Configuration Migration","text":"<p>Map your existing configuration to Tracelet:</p> <pre><code># Old W&amp;B config\nwandb_config = {\n    \"learning_rate\": 0.01,\n    \"architecture\": \"resnet50\"\n}\n\n# New Tracelet config (same data, better organization)\nexperiment = tracelet.get_active_experiment()\nexperiment.log_params(wandb_config)  # Same parameters\n</code></pre>"},{"location":"guides/migration/#timing-differences","title":"Timing Differences","text":"<p>Some backends may show slight timing differences:</p> <pre><code># Use consistent step counting\nglobal_step = epoch * len(dataloader) + batch_idx\nwriter.add_scalar(\"loss\", loss, global_step)\n</code></pre> <p>Need help with your specific migration? Contact us or open an issue.</p>"},{"location":"guides/troubleshooting/","title":"Troubleshooting","text":"<p>This guide helps you diagnose and resolve common issues when using Tracelet.</p>"},{"location":"guides/troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"guides/troubleshooting/#importerror-no-module-named-tracelet","title":"ImportError: No module named 'tracelet'","text":"<p>Problem: Tracelet is not installed or not in the Python path.</p> <p>Solution:</p> <pre><code># Install Tracelet\npip install tracelet\n\n# Or with specific backends\npip install tracelet[mlflow,wandb]\n\n# For development\npip install -e \".[dev]\"\n</code></pre>"},{"location":"guides/troubleshooting/#backend-import-errors","title":"Backend Import Errors","text":"<p>Problem: <code>ImportError: MLflow is not installed</code> or similar for other backends.</p> <p>Solution: Install the specific backend extras:</p> <pre><code>pip install tracelet[mlflow]     # For MLflow\npip install tracelet[clearml]    # For ClearML\npip install tracelet[aim]        # For AIM\npip install tracelet[all]        # For all backends\n</code></pre>"},{"location":"guides/troubleshooting/#connection-issues","title":"Connection Issues","text":""},{"location":"guides/troubleshooting/#mlflow-server-connection-failed","title":"MLflow Server Connection Failed","text":"<p>Problem: Cannot connect to MLflow tracking server.</p> <p>Diagnosis:</p> <pre><code>import mlflow\nprint(f\"MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n</code></pre> <p>Solutions:</p> <pre><code># Start local MLflow server\nmlflow server --host 127.0.0.1 --port 5000\n\n# Or set tracking URI\nexport MLFLOW_TRACKING_URI=http://localhost:5000\n</code></pre> <pre><code># In code\nimport mlflow\nmlflow.set_tracking_uri(\"http://localhost:5000\")\n</code></pre>"},{"location":"guides/troubleshooting/#wb-authentication-issues","title":"W&amp;B Authentication Issues","text":"<p>Problem: <code>wandb.errors.UsageError: api_key not configured</code></p> <p>Solution:</p> <pre><code># Login to W&amp;B\nwandb login\n\n# Or set API key\nexport WANDB_API_KEY=your_api_key_here\n</code></pre>"},{"location":"guides/troubleshooting/#clearml-offline-mode","title":"ClearML Offline Mode","text":"<p>Problem: ClearML requires internet connection.</p> <p>Solution (for testing/CI):</p> <pre><code>import os\nos.environ[\"CLEARML_WEB_HOST\"] = \"\"\nos.environ[\"CLEARML_API_HOST\"] = \"\"\nos.environ[\"CLEARML_FILES_HOST\"] = \"\"\n</code></pre>"},{"location":"guides/troubleshooting/#metric-logging-issues","title":"Metric Logging Issues","text":""},{"location":"guides/troubleshooting/#metrics-not-appearing-in-backend","title":"Metrics Not Appearing in Backend","text":"<p>Problem: TensorBoard metrics logged but not showing in MLflow/W&amp;B.</p> <p>Diagnosis:</p> <pre><code># Check if Tracelet is active\nimport tracelet\nprint(f\"Active experiment: {tracelet.get_active_experiment()}\")\nprint(f\"Active backends: {tracelet.get_active_backends()}\")\n</code></pre> <p>Solutions:</p> <ol> <li>Ensure <code>tracelet.start_logging()</code> is called before creating <code>SummaryWriter</code></li> <li>Check backend configuration:</li> </ol> <pre><code>tracelet.start_logging(\n    exp_name=\"test\",\n    backend=\"mlflow\",\n    config={\"track_tensorboard\": True}  # Ensure this is True\n)\n</code></pre>"},{"location":"guides/troubleshooting/#duplicate-metrics","title":"Duplicate Metrics","text":"<p>Problem: Same metrics appearing multiple times.</p> <p>Cause: Multiple experiment tracking tools running simultaneously.</p> <p>Solution: Use only Tracelet for experiment tracking:</p> <pre><code># Don't use multiple loggers simultaneously\n# writer = SummaryWriter()  # Tracelet handles this\n# mlflow.log_metric()       # Avoid direct backend calls\n# wandb.log()              # Let Tracelet route metrics\n</code></pre>"},{"location":"guides/troubleshooting/#missing-lightning-metrics","title":"Missing Lightning Metrics","text":"<p>Problem: PyTorch Lightning metrics not captured.</p> <p>Solutions:</p> <ol> <li>Start Tracelet before creating Trainer:</li> </ol> <pre><code>tracelet.start_logging(\"experiment\", backend=\"mlflow\")\ntrainer = pl.Trainer()  # Create after start_logging\n</code></pre> <ol> <li>Enable Lightning tracking:</li> </ol> <pre><code>tracelet.start_logging(\n    \"experiment\",\n    backend=\"mlflow\",\n    config={\"track_lightning\": True}\n)\n</code></pre>"},{"location":"guides/troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"guides/troubleshooting/#high-memory-usage","title":"High Memory Usage","text":"<p>Problem: Tracelet consuming too much memory.</p> <p>Solutions:</p> <pre><code># Reduce system monitoring frequency\ntracelet.start_logging(\n    \"experiment\",\n    backend=\"mlflow\",\n    config={\n        \"track_system\": False,          # Disable system monitoring\n        \"metrics_interval\": 60.0,       # Reduce frequency\n        \"max_image_size\": \"512KB\",      # Limit image sizes\n    }\n)\n\n# In training loop - reduce logging frequency\nif step % 100 == 0:  # Log less frequently\n    writer.add_histogram('weights', model.weights, step)\n</code></pre>"},{"location":"guides/troubleshooting/#slow-training","title":"Slow Training","text":"<p>Problem: Training significantly slower with Tracelet.</p> <p>Diagnosis:</p> <pre><code>import time\n\n# Test with and without Tracelet\nstart = time.time()\n# Your training step\nelapsed = time.time() - start\nprint(f\"Training step took {elapsed:.3f}s\")\n</code></pre> <p>Solutions:</p> <ol> <li>Reduce logging frequency:</li> </ol> <pre><code># Log metrics less frequently\nif step % 10 == 0:  # Instead of every step\n    writer.add_scalar('loss', loss, step)\n</code></pre> <ol> <li>Disable expensive operations:</li> </ol> <pre><code>config = {\n    \"track_system\": False,      # Disable system monitoring\n    \"track_git\": False,         # Disable git tracking\n    \"capture_histograms\": False # Disable histogram capture\n}\n</code></pre>"},{"location":"guides/troubleshooting/#network-timeouts","title":"Network Timeouts","text":"<p>Problem: Timeouts when logging to cloud backends.</p> <p>Solutions:</p> <ol> <li>Use local backend as fallback:</li> </ol> <pre><code>try:\n    tracelet.start_logging(\"exp\", backend=\"wandb\")\nexcept Exception:\n    print(\"W&amp;B failed, falling back to MLflow\")\n    tracelet.start_logging(\"exp\", backend=\"mlflow\")\n</code></pre> <ol> <li>Configure timeout settings:</li> </ol> <pre><code>import wandb\nwandb.Settings(base_url=\"https://api.wandb.ai\", timeout=60)\n</code></pre>"},{"location":"guides/troubleshooting/#platform-specific-issues","title":"Platform-Specific Issues","text":""},{"location":"guides/troubleshooting/#windows-path-issues","title":"Windows Path Issues","text":"<p>Problem: File path errors on Windows.</p> <p>Solution:</p> <pre><code>import os\nfrom pathlib import Path\n\n# Use pathlib for cross-platform paths\nlog_dir = Path(\"./runs\") / \"experiment_1\"\nwriter = SummaryWriter(log_dir=str(log_dir))\n</code></pre>"},{"location":"guides/troubleshooting/#m1-mac-compatibility","title":"M1 Mac Compatibility","text":"<p>Problem: Some backends not working on Apple Silicon.</p> <p>Solutions:</p> <ol> <li>Install x86_64 version:</li> </ol> <pre><code>arch -x86_64 pip install tracelet[all]\n</code></pre> <ol> <li>Use conda-forge:</li> </ol> <pre><code>conda install -c conda-forge tracelet\n</code></pre>"},{"location":"guides/troubleshooting/#docker-container-issues","title":"Docker Container Issues","text":"<p>Problem: Backends not accessible from Docker.</p> <p>Solution:</p> <pre><code># In Dockerfile\nENV MLFLOW_TRACKING_URI=http://host.docker.internal:5000\nENV WANDB_API_KEY=your_api_key\n\n# For MLflow server access\nEXPOSE 5000\n</code></pre>"},{"location":"guides/troubleshooting/#debugging-tips","title":"Debugging Tips","text":""},{"location":"guides/troubleshooting/#enable-debug-logging","title":"Enable Debug Logging","text":"<pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or Tracelet-specific logging\ntracelet_logger = logging.getLogger('tracelet')\ntracelet_logger.setLevel(logging.DEBUG)\n</code></pre>"},{"location":"guides/troubleshooting/#check-configuration","title":"Check Configuration","text":"<pre><code># Print current configuration\nexperiment = tracelet.get_active_experiment()\nprint(f\"Experiment: {experiment}\")\nprint(f\"Config: {experiment.config if experiment else 'No active experiment'}\")\n</code></pre>"},{"location":"guides/troubleshooting/#test-backend-connectivity","title":"Test Backend Connectivity","text":"<pre><code>def test_backend(backend_name):\n    \"\"\"Test if backend is accessible\"\"\"\n    try:\n        tracelet.start_logging(f\"test_{backend_name}\", backend=backend_name)\n        experiment = tracelet.get_active_experiment()\n        experiment.log_metric(\"test_metric\", 1.0, 0)\n        print(f\"\u2705 {backend_name} working\")\n        tracelet.stop_logging()\n        return True\n    except Exception as e:\n        print(f\"\u274c {backend_name} failed: {e}\")\n        return False\n\n# Test all backends\nfor backend in [\"mlflow\", \"wandb\", \"clearml\", \"aim\"]:\n    test_backend(backend)\n</code></pre>"},{"location":"guides/troubleshooting/#getting-help","title":"Getting Help","text":""},{"location":"guides/troubleshooting/#check-system-information","title":"Check System Information","text":"<pre><code>import tracelet\nprint(tracelet.__version__)\n\nimport sys\nprint(f\"Python: {sys.version}\")\n\nimport torch\nprint(f\"PyTorch: {torch.__version__}\")\n</code></pre>"},{"location":"guides/troubleshooting/#minimal-reproduction","title":"Minimal Reproduction","text":"<p>When reporting issues, provide a minimal example:</p> <pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Minimal failing example\ntracelet.start_logging(\"test_experiment\", backend=\"mlflow\")\nwriter = SummaryWriter()\nwriter.add_scalar(\"test\", 1.0, 0)\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"guides/troubleshooting/#common-error-messages","title":"Common Error Messages","text":"Error Message Likely Cause Solution <code>ModuleNotFoundError: No module named 'mlflow'</code> Backend not installed <code>pip install tracelet[mlflow]</code> <code>ConnectionError: HTTPConnectionPool</code> Backend server not running Start MLflow server <code>wandb.errors.UsageError: api_key not configured</code> W&amp;B not authenticated <code>wandb login</code> <code>AttributeError: 'NoneType' object has no attribute 'log_metric'</code> No active experiment Call <code>start_logging()</code> first <code>RuntimeError: CUDA out of memory</code> GPU memory exhausted Reduce batch size or disable system tracking <p>For additional help, please:</p> <ul> <li>Check our GitHub Issues</li> <li>Join our Discussions</li> <li>Email us at support@tracelet.io</li> </ul>"},{"location":"integrations/lightning/","title":"PyTorch Lightning Integration","text":"<p>Tracelet automatically captures PyTorch Lightning training metrics without any code modifications.</p>"},{"location":"integrations/lightning/#overview","title":"Overview","text":"<p>The Lightning integration hooks into the Lightning framework's logging system to capture all metrics logged via <code>self.log()</code> calls in your LightningModule.</p>"},{"location":"integrations/lightning/#supported-features","title":"Supported Features","text":"<ul> <li>Training Metrics - Loss, accuracy, custom metrics</li> <li>Validation Metrics - Validation loss, metrics from validation_step</li> <li>Test Metrics - Test phase metrics</li> <li>Hyperparameters - Model and trainer configuration</li> <li>System Metrics - GPU utilization, memory usage during training</li> </ul>"},{"location":"integrations/lightning/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nimport pytorch_lightning as pl\nfrom pytorch_lightning import Trainer\n\n# Start Tracelet before creating trainer\ntracelet.start_logging(\n    exp_name=\"lightning_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Define your LightningModule as usual\nclass MyModel(pl.LightningModule):\n    def training_step(self, batch, batch_idx):\n        loss = self.compute_loss(batch)\n\n        # All these metrics are automatically captured by Tracelet\n        self.log('train/loss', loss)\n        self.log('train/accuracy', self.compute_accuracy(batch))\n\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        val_loss = self.compute_loss(batch)\n        val_acc = self.compute_accuracy(batch)\n\n        # Validation metrics are also captured\n        self.log('val/loss', val_loss)\n        self.log('val/accuracy', val_acc)\n\n# Train your model - metrics automatically tracked\nmodel = MyModel()\ntrainer = Trainer(max_epochs=10)\ntrainer.fit(model, train_dataloader, val_dataloader)\n\n# Stop tracking\ntracelet.stop_logging()\n</code></pre>"},{"location":"integrations/lightning/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Configure Lightning-specific tracking\ntracelet.start_logging(\n    exp_name=\"advanced_lightning\",\n    backend=[\"mlflow\", \"wandb\"],  # Multi-backend logging\n    config={\n        \"track_lightning\": True,        # Enable Lightning integration\n        \"track_system\": True,           # Monitor system resources\n        \"track_git\": True,              # Track git information\n        \"metrics_interval\": 10.0,       # System metrics every 10 seconds\n    }\n)\n</code></pre>"},{"location":"integrations/lightning/#best-practices","title":"Best Practices","text":""},{"location":"integrations/lightning/#metric-naming","title":"Metric Naming","text":"<p>Use consistent, hierarchical naming:</p> <pre><code>def training_step(self, batch, batch_idx):\n    # Good: Hierarchical naming\n    self.log('train/loss', loss)\n    self.log('train/accuracy', accuracy)\n    self.log('train/f1_score', f1)\n\n    # Also good: Phase-specific metrics\n    self.log('metrics/train_loss', loss)\n    self.log('metrics/train_acc', accuracy)\n</code></pre>"},{"location":"integrations/lightning/#logging-frequency","title":"Logging Frequency","text":"<p>Control when metrics are logged:</p> <pre><code>def training_step(self, batch, batch_idx):\n    loss = self.compute_loss(batch)\n\n    # Log every step\n    self.log('train/loss', loss, on_step=True, on_epoch=False)\n\n    # Log epoch averages\n    self.log('train/epoch_loss', loss, on_step=False, on_epoch=True)\n\n    # Log both\n    self.log('train/loss_detailed', loss, on_step=True, on_epoch=True)\n</code></pre>"},{"location":"integrations/lightning/#custom-metrics","title":"Custom Metrics","text":"<p>Log custom metrics and hyperparameters:</p> <pre><code>def on_train_start(self):\n    # Log hyperparameters\n    self.logger.log_hyperparams({\n        'learning_rate': self.learning_rate,\n        'batch_size': self.batch_size,\n        'model_name': self.__class__.__name__\n    })\n\ndef training_step(self, batch, batch_idx):\n    # Custom metrics\n    predictions = self.forward(batch)\n    custom_metric = self.compute_custom_metric(predictions, batch)\n\n    self.log('custom/my_metric', custom_metric)\n</code></pre>"},{"location":"integrations/lightning/#multi-gpu-support","title":"Multi-GPU Support","text":"<p>Tracelet works seamlessly with Lightning's distributed training:</p> <pre><code># Works with DDP, DDP2, etc.\ntrainer = Trainer(\n    accelerator='gpu',\n    devices=4,\n    strategy='ddp'\n)\n\n# Metrics from all processes are automatically aggregated\ntrainer.fit(model)\n</code></pre>"},{"location":"integrations/lightning/#integration-with-callbacks","title":"Integration with Callbacks","text":"<p>Use with Lightning callbacks:</p> <pre><code>from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\ntracelet.start_logging(\"lightning_with_callbacks\", backend=\"clearml\")\n\ntrainer = Trainer(\n    callbacks=[\n        ModelCheckpoint(monitor='val/loss'),\n        EarlyStopping(monitor='val/loss', patience=3)\n    ],\n    max_epochs=100\n)\n\ntrainer.fit(model)\n</code></pre>"},{"location":"integrations/lightning/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/lightning/#common-issues","title":"Common Issues","text":"<p>Metrics not appearing: Ensure <code>tracelet.start_logging()</code> is called before creating the Trainer.</p> <p>Duplicate metrics: If using multiple loggers, you may see duplicate entries. Use Tracelet as the primary logger.</p> <p>Memory issues with large models: Enable gradient checkpointing and reduce logging frequency for memory-intensive operations.</p>"},{"location":"integrations/pytorch/","title":"PyTorch Integration","text":"<p>Tracelet provides seamless integration with PyTorch through automatic TensorBoard metric capture.</p>"},{"location":"integrations/pytorch/#overview","title":"Overview","text":"<p>The PyTorch integration works by automatically patching TensorBoard's <code>SummaryWriter</code> to capture all logged metrics and route them to your configured backends.</p>"},{"location":"integrations/pytorch/#supported-features","title":"Supported Features","text":"<ul> <li>Scalar Metrics - Training loss, validation accuracy, learning rates</li> <li>Histograms - Weight distributions, gradient histograms</li> <li>Images - Sample predictions, model visualizations</li> <li>Text - Training logs, model summaries</li> <li>Audio - Audio samples and generated content</li> </ul>"},{"location":"integrations/pytorch/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nimport torch\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start tracking\ntracelet.start_logging(\n    exp_name=\"pytorch_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Use TensorBoard normally\nwriter = SummaryWriter()\n\n# Your training loop\nfor epoch in range(num_epochs):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        # Training code...\n        loss = train_step(model, data, target)\n\n        # Log metrics - automatically captured by Tracelet\n        writer.add_scalar('Loss/Train', loss.item(), epoch * len(train_loader) + batch_idx)\n\n        if batch_idx % 100 == 0:\n            # Log histograms\n            for name, param in model.named_parameters():\n                writer.add_histogram(f'Parameters/{name}', param, epoch)\n\n# Stop tracking\ntracelet.stop_logging()\n</code></pre>"},{"location":"integrations/pytorch/#advanced-configuration","title":"Advanced Configuration","text":"<pre><code># Configure specific PyTorch tracking options\ntracelet.start_logging(\n    exp_name=\"advanced_pytorch\",\n    backend=\"wandb\",\n    config={\n        \"track_tensorboard\": True,      # Enable TensorBoard capture\n        \"track_system\": True,           # Monitor system metrics\n        \"metrics_interval\": 5.0,        # System metrics every 5 seconds\n    }\n)\n</code></pre>"},{"location":"integrations/pytorch/#best-practices","title":"Best Practices","text":""},{"location":"integrations/pytorch/#metric-organization","title":"Metric Organization","text":"<p>Organize your metrics with clear hierarchies:</p> <pre><code># Good: Hierarchical naming\nwriter.add_scalar('Loss/Train', train_loss, step)\nwriter.add_scalar('Loss/Validation', val_loss, step)\nwriter.add_scalar('Accuracy/Train', train_acc, step)\nwriter.add_scalar('Accuracy/Validation', val_acc, step)\n\n# Avoid: Flat naming\nwriter.add_scalar('train_loss', train_loss, step)\nwriter.add_scalar('val_loss', val_loss, step)\n</code></pre>"},{"location":"integrations/pytorch/#performance-optimization","title":"Performance Optimization","text":"<p>For high-frequency logging:</p> <pre><code># Log less frequently for expensive operations\nif step % 100 == 0:\n    writer.add_histogram('gradients', model.parameters(), step)\n\nif step % 1000 == 0:\n    writer.add_image('predictions', sample_predictions, step)\n</code></pre>"},{"location":"integrations/pytorch/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/pytorch/#common-issues","title":"Common Issues","text":"<p>TensorBoard metrics not appearing: Ensure you're using <code>SummaryWriter</code> after calling <code>tracelet.start_logging()</code>.</p> <p>Memory issues: Reduce logging frequency for large tensors like histograms and images.</p> <p>Performance impact: Use <code>metrics_interval</code> to control system metrics collection frequency.</p>"},{"location":"integrations/tensorboard/","title":"TensorBoard Integration","text":"<p>Tracelet's TensorBoard integration automatically captures all metrics logged to TensorBoard and routes them to your configured experiment tracking backends.</p>"},{"location":"integrations/tensorboard/#overview","title":"Overview","text":"<p>The integration works by transparently patching TensorBoard's <code>SummaryWriter</code> class to intercept all logging calls. Your existing TensorBoard code works unchanged while metrics are automatically sent to backends like MLflow, W&amp;B, or ClearML.</p>"},{"location":"integrations/tensorboard/#supported-operations","title":"Supported Operations","text":"<p>Tracelet captures all TensorBoard logging operations:</p> <ul> <li><code>add_scalar()</code> - Scalar metrics (loss, accuracy, etc.)</li> <li><code>add_histogram()</code> - Weight distributions, gradients</li> <li><code>add_image()</code> - Images, plots, visualizations</li> <li><code>add_text()</code> - Text logs, summaries</li> <li><code>add_audio()</code> - Audio samples</li> <li><code>add_figure()</code> - Matplotlib figures</li> <li><code>add_graph()</code> - Model computational graphs</li> </ul>"},{"location":"integrations/tensorboard/#basic-usage","title":"Basic Usage","text":"<pre><code>import tracelet\nfrom torch.utils.tensorboard import SummaryWriter\n\n# Start Tracelet\ntracelet.start_logging(\n    exp_name=\"tensorboard_experiment\",\n    project=\"my_project\",\n    backend=\"mlflow\"\n)\n\n# Use TensorBoard exactly as before\nwriter = SummaryWriter(log_dir='./runs/experiment_1')\n\n# All these operations are automatically captured\nfor step in range(100):\n    # Scalars\n    writer.add_scalar('Loss/Train', loss_value, step)\n    writer.add_scalar('Accuracy/Train', acc_value, step)\n\n    # Histograms\n    writer.add_histogram('Weights/Layer1', model.layer1.weight, step)\n\n    # Images (every 10 steps)\n    if step % 10 == 0:\n        writer.add_image('Predictions', pred_image, step)\n\nwriter.close()\ntracelet.stop_logging()\n</code></pre>"},{"location":"integrations/tensorboard/#advanced-features","title":"Advanced Features","text":""},{"location":"integrations/tensorboard/#multiple-writers","title":"Multiple Writers","text":"<p>Tracelet supports multiple SummaryWriter instances:</p> <pre><code># Multiple writers for different aspects\ntrain_writer = SummaryWriter('runs/train')\nval_writer = SummaryWriter('runs/validation')\n\n# Both are automatically captured\ntrain_writer.add_scalar('loss', train_loss, step)\nval_writer.add_scalar('loss', val_loss, step)\n</code></pre>"},{"location":"integrations/tensorboard/#hierarchical-metrics","title":"Hierarchical Metrics","text":"<p>Organize metrics with forward slashes:</p> <pre><code># Creates nested structure in backends\nwriter.add_scalar('Loss/Train/CrossEntropy', ce_loss, step)\nwriter.add_scalar('Loss/Train/Regularization', reg_loss, step)\nwriter.add_scalar('Loss/Validation/Total', val_loss, step)\n\nwriter.add_scalar('Metrics/Accuracy/Train', train_acc, step)\nwriter.add_scalar('Metrics/Accuracy/Validation', val_acc, step)\nwriter.add_scalar('Metrics/F1/Macro', f1_macro, step)\n</code></pre>"},{"location":"integrations/tensorboard/#custom-tags-and-metadata","title":"Custom Tags and Metadata","text":"<p>Add additional context to your metrics:</p> <pre><code># Scalars with custom metadata\nwriter.add_scalar('learning_rate', lr, step)\nwriter.add_scalar('batch_size', batch_size, step)\n\n# Text logs for additional context\nwriter.add_text('Config', f\"Model: {model_name}, LR: {lr}\", step)\nwriter.add_text('Notes', 'Changed optimizer to AdamW', step)\n</code></pre>"},{"location":"integrations/tensorboard/#configuration-options","title":"Configuration Options","text":"<p>Control TensorBoard integration behavior:</p> <pre><code>tracelet.start_logging(\n    exp_name=\"custom_tensorboard\",\n    backend=\"wandb\",\n    config={\n        \"track_tensorboard\": True,       # Enable TensorBoard capture (default: True)\n        \"tensorboard_log_dir\": \"./runs\", # TensorBoard log directory\n        \"capture_images\": True,          # Capture add_image() calls\n        \"capture_histograms\": True,      # Capture add_histogram() calls\n        \"capture_audio\": False,          # Skip audio (can be large)\n        \"max_image_size\": \"1MB\",         # Limit image sizes\n    }\n)\n</code></pre>"},{"location":"integrations/tensorboard/#performance-considerations","title":"Performance Considerations","text":""},{"location":"integrations/tensorboard/#high-frequency-logging","title":"High-Frequency Logging","text":"<p>For high-frequency metrics, consider batching:</p> <pre><code># Good: Batch similar metrics\nif step % 10 == 0:  # Log every 10 steps\n    writer.add_scalar('Loss/Train', loss, step)\n\nif step % 100 == 0:  # Log expensive operations less frequently\n    writer.add_histogram('Gradients', model.gradients, step)\n\nif step % 1000 == 0:  # Log very expensive operations rarely\n    writer.add_image('Samples', sample_images, step)\n</code></pre>"},{"location":"integrations/tensorboard/#memory-management","title":"Memory Management","text":"<p>For large tensors and images:</p> <pre><code># Limit image resolution\nresized_image = F.interpolate(large_image, size=(224, 224))\nwriter.add_image('Prediction', resized_image, step)\n\n# Log histograms selectively\nif step % 500 == 0:  # Reduce frequency for memory-intensive ops\n    for name, param in model.named_parameters():\n        if 'weight' in name:  # Only log weights, not biases\n            writer.add_histogram(f'Weights/{name}', param, step)\n</code></pre>"},{"location":"integrations/tensorboard/#migration-from-pure-tensorboard","title":"Migration from Pure TensorBoard","text":"<p>Migrating existing TensorBoard code is trivial:</p>"},{"location":"integrations/tensorboard/#before-pure-tensorboard","title":"Before (Pure TensorBoard)","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter()\n# ... training loop with writer.add_scalar() calls ...\nwriter.close()\n</code></pre>"},{"location":"integrations/tensorboard/#after-with-tracelet","title":"After (With Tracelet)","text":"<pre><code>import tracelet  # Add this import\nfrom torch.utils.tensorboard import SummaryWriter\n\ntracelet.start_logging(\"my_experiment\", backend=\"mlflow\")  # Add this line\nwriter = SummaryWriter()\n# ... same training loop, no changes needed ...\nwriter.close()\ntracelet.stop_logging()  # Add this line\n</code></pre>"},{"location":"integrations/tensorboard/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/tensorboard/#common-issues","title":"Common Issues","text":"<p>Metrics appear in TensorBoard but not backend: Ensure <code>tracelet.start_logging()</code> is called before creating <code>SummaryWriter</code>.</p> <p>Some metrics missing: Check if you're using multiple writers - all are captured automatically.</p> <p>Performance degradation: Reduce logging frequency for expensive operations like histograms and images.</p> <p>Large file sizes: Configure limits for images and audio, or reduce logging frequency.</p>"},{"location":"integrations/tensorboard/#debugging","title":"Debugging","text":"<p>Enable debug logging to see what's being captured:</p> <pre><code>import logging\nlogging.basicConfig(level=logging.DEBUG)\n\ntracelet.start_logging(\"debug_experiment\", backend=\"mlflow\")\n# ... your code ...\n</code></pre> <p>This will show all TensorBoard operations being intercepted and routed to backends.</p>"}]}