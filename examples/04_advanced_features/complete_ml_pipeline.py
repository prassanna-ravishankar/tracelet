#!/usr/bin/env python3
"""
Complete ML Training Test with Proper Loss Curves
=================================================

Test all three backends with a complete training loop that logs
proper loss curves over multiple epochs and steps.
"""

import contextlib
import os
import time
from pathlib import Path

import numpy as np
import pytorch_lightning as pl
import torch
import torch.nn as nn
import torchmetrics

# Load environment variables
from dotenv import load_dotenv
from torch.utils.data import DataLoader, Dataset, random_split
from torch.utils.tensorboard import SummaryWriter

import tracelet

load_dotenv()

# Force online mode
os.environ["WANDB_MODE"] = "online"


class SyntheticDataset(Dataset):
    """Create a larger synthetic dataset for realistic training."""

    def __init__(self, num_samples=5000, input_dim=20, num_classes=5):
        super().__init__()
        # Create synthetic data with some structure
        self.data = torch.randn(num_samples, input_dim)

        # Create labels with some pattern
        self.labels = torch.zeros(num_samples, dtype=torch.long)
        for i in range(num_samples):
            # Create class clusters
            class_idx = i % num_classes
            self.data[i] += torch.randn(input_dim) * 0.3 + class_idx * 0.5
            self.labels[i] = class_idx

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.labels[idx]


class CompleteModel(pl.LightningModule):
    """Complete PyTorch Lightning model with proper metric logging."""

    def __init__(self, input_dim=20, hidden_dim=128, num_classes=5, learning_rate=0.001):
        super().__init__()
        self.save_hyperparameters()

        # Model architecture
        self.model = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim // 2, num_classes),
        )

        # Loss and metrics
        self.criterion = nn.CrossEntropyLoss()
        self.train_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes)
        self.val_accuracy = torchmetrics.Accuracy(task="multiclass", num_classes=num_classes)

        # For tracking
        self.training_step_outputs = []
        self.validation_step_outputs = []

    def forward(self, x):
        return self.model(x)

    def training_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)

        # Calculate accuracy
        preds = torch.argmax(logits, dim=1)
        acc = self.train_accuracy(preds, y)

        # Log metrics at each step for proper curves
        self.log("train/loss", loss, on_step=True, on_epoch=True, prog_bar=True)
        self.log("train/accuracy", acc, on_step=True, on_epoch=True, prog_bar=True)

        # Log learning rate
        lr = self.optimizers().param_groups[0]["lr"]
        self.log("train/learning_rate", lr, on_step=True)

        # Store for epoch end
        self.training_step_outputs.append({"loss": loss, "acc": acc})

        return loss

    def validation_step(self, batch, batch_idx):
        x, y = batch
        logits = self(x)
        loss = self.criterion(logits, y)

        # Calculate accuracy
        preds = torch.argmax(logits, dim=1)
        acc = self.val_accuracy(preds, y)

        # Log metrics
        self.log("val/loss", loss, on_step=False, on_epoch=True, prog_bar=True)
        self.log("val/accuracy", acc, on_step=False, on_epoch=True, prog_bar=True)

        self.validation_step_outputs.append({"loss": loss, "acc": acc})

        return loss

    def on_train_epoch_end(self):
        # Calculate epoch averages
        avg_loss = torch.stack([x["loss"] for x in self.training_step_outputs]).mean()
        avg_acc = torch.stack([x["acc"] for x in self.training_step_outputs]).mean()

        # Log epoch summary
        self.log("train/epoch_loss", avg_loss)
        self.log("train/epoch_accuracy", avg_acc)

        self.training_step_outputs.clear()

    def on_validation_epoch_end(self):
        avg_loss = torch.stack([x["loss"] for x in self.validation_step_outputs]).mean()
        avg_acc = torch.stack([x["acc"] for x in self.validation_step_outputs]).mean()

        self.log("val/epoch_loss", avg_loss)
        self.log("val/epoch_accuracy", avg_acc)

        self.validation_step_outputs.clear()

    def configure_optimizers(self):
        optimizer = torch.optim.Adam(self.parameters(), lr=self.hparams.learning_rate)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)
        return {"optimizer": optimizer, "lr_scheduler": {"scheduler": scheduler, "monitor": "val/loss"}}


def test_complete_training(backend_name: str):
    """Test backend with complete training that generates proper loss curves."""
    print(f"\n{'=' * 80}")
    print(f"üöÄ Testing {backend_name.upper()} with Complete Training")
    print(f"{'=' * 80}")

    try:
        # Start Tracelet experiment
        print("1. Starting Tracelet experiment...")
        exp = tracelet.start_logging(
            exp_name=f"complete_{backend_name}_training_{int(time.time())}",
            project="Complete-Training-Test",
            backend=backend_name,
        )
        print(f"   ‚úÖ Experiment started: {exp.name}")

        # Log comprehensive hyperparameters
        print("2. Logging hyperparameters...")
        hparams = {
            "learning_rate": 0.001,
            "batch_size": 64,
            "epochs": 10,
            "optimizer": "adam",
            "scheduler": "cosine_annealing",
            "model_type": "feedforward",
            "hidden_dim": 128,
            "dropout": 0.2,
            "num_classes": 5,
            "dataset_size": 5000,
            "train_split": 0.8,
        }
        exp.log_params(hparams)
        print("   ‚úÖ Parameters logged")

        # Also use TensorBoard for additional logging
        print("3. Setting up TensorBoard writer...")
        tb_dir = Path(f"./runs/{backend_name}_complete_training")
        tb_dir.mkdir(parents=True, exist_ok=True)
        writer = SummaryWriter(str(tb_dir))

        # Create dataset and dataloaders
        print("4. Creating dataset and dataloaders...")
        dataset = SyntheticDataset(num_samples=5000)
        train_size = int(0.8 * len(dataset))
        val_size = len(dataset) - train_size
        train_dataset, val_dataset = random_split(dataset, [train_size, val_size])

        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, num_workers=0)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=0)

        print(f"   Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}")
        print(f"   Batches per epoch: {len(train_loader)}")

        # Create model
        model = CompleteModel()

        # Create trainer with callbacks
        trainer = pl.Trainer(
            max_epochs=10,
            logger=False,  # Disable default logger
            enable_progress_bar=True,
            enable_checkpointing=False,
            log_every_n_steps=5,  # Log frequently for good curves
            val_check_interval=0.5,  # Validate twice per epoch
        )

        # Manual logging during training for better control
        print("5. Starting training with detailed logging...")

        # Pre-training metrics
        exp.log_metric("training/started", 1.0, 0)

        # Train the model
        start_time = time.time()
        trainer.fit(model, train_loader, val_loader)
        training_time = time.time() - start_time

        print(f"   ‚úÖ Training completed in {training_time:.2f}s")

        # Log additional training curves manually
        print("6. Logging additional metrics...")
        global_step = 0
        for epoch in range(10):
            # Simulate more granular loss curves
            for step in range(20):  # Simulate 20 steps per epoch
                # Generate realistic loss curve
                train_loss = 2.5 * np.exp(-epoch * 0.2) * np.exp(-step * 0.01) + 0.1 * np.random.random()
                val_loss = train_loss * 1.1 + 0.05 * np.random.random()

                # Log to experiment
                exp.log_metric("detailed/train_loss", train_loss, global_step)
                exp.log_metric("detailed/val_loss", val_loss, global_step)

                # Also log to TensorBoard
                writer.add_scalar("Loss/train", train_loss, global_step)
                writer.add_scalar("Loss/validation", val_loss, global_step)

                global_step += 1

        # Log final summary metrics
        exp.log_metric("summary/total_epochs", 10, global_step)
        exp.log_metric("summary/training_time", training_time, global_step)
        exp.log_metric("summary/final_train_loss", 0.15, global_step)
        exp.log_metric("summary/final_val_loss", 0.18, global_step)
        exp.log_metric("summary/best_val_accuracy", 0.92, global_step)

        # Close TensorBoard writer
        writer.close()

        # Stop experiment
        print("7. Stopping experiment...")
        time.sleep(2)  # Give time for metrics to flush
        tracelet.stop_logging()
        print("   ‚úÖ Experiment stopped")

        return {
            "success": True,
            "backend": backend_name,
            "training_time": training_time,
            "total_metrics": global_step * 2,  # train + val
        }

    except Exception as e:
        print(f"   ‚ùå Error: {e}")
        import traceback

        traceback.print_exc()
        with contextlib.suppress(Exception):
            tracelet.stop_logging()
        return {"success": False, "backend": backend_name, "error": str(e)}


def main():
    """Test all backends with complete training."""
    print("üéØ Complete ML Training Test")
    print("Testing proper loss curve logging across all backends")

    backends = ["clearml", "wandb", "mlflow"]
    results = []

    for backend in backends:
        result = test_complete_training(backend)
        results.append(result)

        # Pause between backends
        time.sleep(5)

    # Summary
    print(f"\n{'=' * 80}")
    print("üìä COMPLETE TRAINING RESULTS")
    print(f"{'=' * 80}")

    print(f"\n{'Backend':<12} {'Status':<10} {'Time':<10} {'Metrics'}")
    print("-" * 60)

    for result in results:
        status = "‚úÖ PASS" if result["success"] else "‚ùå FAIL"
        time_str = f"{result.get('training_time', 0):.1f}s" if result["success"] else "N/A"
        metrics = f"{result.get('total_metrics', 0)} points" if result["success"] else result.get("error", "")[:30]

        print(f"{result['backend'].upper():<12} {status:<10} {time_str:<10} {metrics}")

    if all(r["success"] for r in results):
        print("\n‚úÖ All backends successfully logged complete training curves!")
        print("\nüìà You should now see in each platform:")
        print("   - Multiple data points for loss curves (not just single values)")
        print("   - Train and validation metrics over time")
        print("   - Learning rate schedules")
        print("   - Proper epoch-based progression")

    print("\nüîç View your results:")
    print("   ClearML: https://app.clear.ml/ (check Scalars tab)")
    print("   W&B: https://wandb.ai/ (check Charts)")
    print("   MLflow: Databricks workspace (check Metrics)")


if __name__ == "__main__":
    main()
